W&B connected
======================================================================
LGBM TRANSITION MODEL (COLOMBIA DATASET - 5-YEAR LOOKAHEAD)
======================================================================
Train: /Users/gianluca/Desktop/Master's Thesis/code/data/ml/train_colombia_win5.parquet
Val: /Users/gianluca/Desktop/Master's Thesis/code/data/ml/val_colombia_win5.parquet
Test: /Users/gianluca/Desktop/Master's Thesis/code/data/ml/test_colombia_win5.parquet
Params: /Users/gianluca/Desktop/Master's Thesis/code/scripts/colombia/ML/training/lgbm_best_params.json
Output: /Users/gianluca/Desktop/Master's Thesis/code/outputs/Results/ml_models

Loaded metadata from /Users/gianluca/Desktop/Master's Thesis/code/data/ml/train_colombia_win5_metadata.json
  scale_pos_weight: 8.01316886455917

Loaded params from /Users/gianluca/Desktop/Master's Thesis/code/scripts/colombia/ML/training/lgbm_best_params.json
Best CV score: 0.0911
Using scale_pos_weight from metadata: 8.013169
Selected 60 features, loading 66 cols from train, 66 from test
Target column: transition_01_win5

======================================================================
PHASE 1: TRAINING
======================================================================

STEP A: LOAD TRAIN (2000-2014)

Loading train_colombia_win5.parquet (two-pass NumPy loader)...
  Pass 1: Counting rows...
  Found 15,206,524 samples
  Pass 2: Pre-allocating arrays (15,206,524 samples, 60 features)...
  10 batches, 500,000/15,206,524 samples loaded
  20 batches, 1,000,000/15,206,524 samples loaded
  30 batches, 1,500,000/15,206,524 samples loaded
  40 batches, 2,000,000/15,206,524 samples loaded
  50 batches, 2,500,000/15,206,524 samples loaded
  60 batches, 3,000,000/15,206,524 samples loaded
  70 batches, 3,500,000/15,206,524 samples loaded
  80 batches, 4,000,000/15,206,524 samples loaded
  90 batches, 4,500,000/15,206,524 samples loaded
  100 batches, 5,000,000/15,206,524 samples loaded
  110 batches, 5,500,000/15,206,524 samples loaded
  120 batches, 6,000,000/15,206,524 samples loaded
  130 batches, 6,500,000/15,206,524 samples loaded
  140 batches, 7,000,000/15,206,524 samples loaded
  150 batches, 7,500,000/15,206,524 samples loaded
  160 batches, 8,000,000/15,206,524 samples loaded
  170 batches, 8,500,000/15,206,524 samples loaded
  180 batches, 9,000,000/15,206,524 samples loaded
  190 batches, 9,500,000/15,206,524 samples loaded
  200 batches, 10,000,000/15,206,524 samples loaded
  210 batches, 10,500,000/15,206,524 samples loaded
  220 batches, 11,000,000/15,206,524 samples loaded
  230 batches, 11,500,000/15,206,524 samples loaded
  240 batches, 12,000,000/15,206,524 samples loaded
  250 batches, 12,500,000/15,206,524 samples loaded
  260 batches, 13,000,000/15,206,524 samples loaded
  270 batches, 13,500,000/15,206,524 samples loaded
  280 batches, 14,000,000/15,206,524 samples loaded
  290 batches, 14,500,000/15,206,524 samples loaded
  300 batches, 15,000,000/15,206,524 samples loaded
  Loaded 15,206,524 rows in 13.4s (3.41 GB)
  14,973,334 neg, 233,190 pos, ratio 1:64.2, years 2000-2014

STEP B: LOAD VAL (2015-2017)

Loading val_colombia_win5.parquet (two-pass NumPy loader)...
  Pass 1: Counting rows...
  Found 2,940,252 samples
  Pass 2: Pre-allocating arrays (2,940,252 samples, 60 features)...
  10 batches, 500,000/2,940,252 samples loaded
  20 batches, 1,000,000/2,940,252 samples loaded
  30 batches, 1,500,000/2,940,252 samples loaded
  40 batches, 2,000,000/2,940,252 samples loaded
  50 batches, 2,500,000/2,940,252 samples loaded
  Loaded 2,940,252 rows in 2.5s (0.66 GB)
  2,894,006 neg, 46,246 pos, ratio 1:62.6, years 2015-2017

STEP C: TRAIN & EVALUATE
Temporal split: train (2000-2014): 15,206,524 rows, val (2015-2017): 2,940,252 rows

Imbalance: ratio 64.211, scale_pos_weight 8.013169 (from JSON params)

Training on train (15,206,524 samples)...
  Creating train dataset...
  Creating validation dataset...
  Starting training...
Training until validation scores don't improve for 500 rounds
[100]	val's average_precision: 0.550879
[200]	val's average_precision: 0.620426
[300]	val's average_precision: 0.640456
[400]	val's average_precision: 0.652901
[500]	val's average_precision: 0.661529
[600]	val's average_precision: 0.666101
[700]	val's average_precision: 0.672846
[800]	val's average_precision: 0.677086
[900]	val's average_precision: 0.679441
[1000]	val's average_precision: 0.681722
[1100]	val's average_precision: 0.686642
[1200]	val's average_precision: 0.691285
[1300]	val's average_precision: 0.693458
[1400]	val's average_precision: 0.694445
[1500]	val's average_precision: 0.696224
[1600]	val's average_precision: 0.697617
[1700]	val's average_precision: 0.699466
[1800]	val's average_precision: 0.700641
[1900]	val's average_precision: 0.702772
[2000]	val's average_precision: 0.703423
[2100]	val's average_precision: 0.705398
[2200]	val's average_precision: 0.706171
[2300]	val's average_precision: 0.70704
[2400]	val's average_precision: 0.707854
[2500]	val's average_precision: 0.709007
[2600]	val's average_precision: 0.709281
[2700]	val's average_precision: 0.710842
[2800]	val's average_precision: 0.711611
[2900]	val's average_precision: 0.711966
[3000]	val's average_precision: 0.712219
Did not meet early stopping. Best iteration is:
[2968]	val's average_precision: 0.712305

Training done in 1393.0s. Best iteration: 2968

Validation: ROC-AUC 0.9630, PR-AUC 0.7123, P@1% 0.8240, P@5% 0.2713, P@10% 0.1399
Model saved: /Users/gianluca/Desktop/Master's Thesis/code/data/ml/models/modelC_lgbm_win5_20251226_154601.pkl
Phase 1 completed.

======================================================================
PHASE 2: TESTING
======================================================================

Loading model from /Users/gianluca/Desktop/Master's Thesis/code/data/ml/models/modelC_lgbm_win5_20251226_154601.pkl
Processing test in batches of 50,000...
  Pass 1: Counting test rows...
  Found 1,944,570 test samples
  Pre-allocating arrays for 1,944,570 samples...
Writing to /Users/gianluca/Desktop/Master's Thesis/code/outputs/Results/ml_models/modelC_lgbm_win5_scored_20251226_154601.parquet
  10 batches, 500,000/1,944,570 rows
  20 batches, 1,000,000/1,944,570 rows
  30 batches, 1,500,000/1,944,570 rows
Completed 39 batches in 70.8s, 1,944,570 rows

Test set: 1,931,466 neg, 13,104 pos, ratio 1:147.4, years 2018-2019

Test metrics: ROC-AUC 0.7915, PR-AUC 0.0553
  P@1%: 0.1520 (2,955/19,445), lift 22.55x
  P@5%: 0.0483 (4,697/97,228), lift 7.17x
  P@10%: 0.0294 (5,726/194,457), lift 4.37x

Top 20 features:
                  feature  importance
deforestation_b1_smooth64       25957
         NDVI_b1_smooth64       16206
                dist_wdpa       15088
                   GPW_b1       13043
deforestation_b1_smooth16       12757
          GSN_b2_smooth64       12597
             dist_oil_gas       12541
         NDVI_b1_smooth16       12311
          GSN_b4_smooth64       11681
          GSN_b2_smooth16       11019
         HNTL_b1_smooth64       10817
          GSN_b5_smooth64       10807
          GPW_b1_smooth64       10405
     wildfire_b1_smooth64       10319
          GPW_b1_smooth16        9974
             elevation_b2        9744
          dist_powerplant        8979
          GSN_b5_smooth16        7564
            WorldClim_b14        7402
                  NDVI_b1        6783

Metrics saved: /Users/gianluca/Desktop/Master's Thesis/code/outputs/Results/ml_models/modelC_lgbm_win5_metrics_20251226_154601.json

======================================================================
SUMMARY
======================================================================
Model: LightGBM, 60 features
Validation (2000-2014â†’2015-2017): PR-AUC 0.7123, ROC-AUC 0.9630
Test (2018-2019, 1,944,570 samples, 0.674% pos): ROC-AUC 0.7915, PR-AUC 0.0553
  P@1%: 0.1520 (22.6x), P@5%: 0.0483 (7.2x), P@10%: 0.0294 (4.4x)
Timings: train 1393.0s, val 86.0s, pred 70.8s, total 1570.2s (26.2m)
======================================================================
Done.
