
Configuration:
  WINDOW: 2 years
  Target column: transition_01_win2

Loaded metadata from /Users/gianluca/Desktop/Master's Thesis/code/data/ml/train_embeddings_win2_metadata.json
  Train years: 2018-2020
  Val years: 2021-2021
  Test years: 2022-2022
W&B connected
======================================================================
LGBM TRANSITION MODEL (EMBEDDINGS DATASET - 2-YEAR LOOKAHEAD)
======================================================================
Train: /Users/gianluca/Desktop/Master's Thesis/code/data/ml/train_embeddings_win2.parquet
Val: /Users/gianluca/Desktop/Master's Thesis/code/data/ml/val_embeddings_win2.parquet
Test: /Users/gianluca/Desktop/Master's Thesis/code/data/ml/test_embeddings_win2.parquet
Params: /Users/gianluca/Desktop/Master's Thesis/code/scripts/embeddings/ML/training/lgbm_best_params.json
Output: /Users/gianluca/Desktop/Master's Thesis/code/outputs/Results/ml_models

Loaded params from /Users/gianluca/Desktop/Master's Thesis/code/scripts/embeddings/ML/training/lgbm_best_params.json
Selected 64 features, loading 68 cols from train, 68 from test
Target column: transition_01_win2

======================================================================
PHASE 1: TRAINING
======================================================================

STEP A: LOAD TRAIN (2018-2020)

Loading train_embeddings_win2.parquet (two-pass NumPy loader)...
  Pass 1: Counting rows...
  Found 47,568,698 samples
  Pass 2: Pre-allocating arrays (47,568,698 samples, 64 features)...
  10 batches, 500,000/47,568,698 samples loaded
  20 batches, 1,000,000/47,568,698 samples loaded
  30 batches, 1,500,000/47,568,698 samples loaded
  40 batches, 2,000,000/47,568,698 samples loaded
  50 batches, 2,500,000/47,568,698 samples loaded
  60 batches, 3,000,000/47,568,698 samples loaded
  70 batches, 3,500,000/47,568,698 samples loaded
  80 batches, 4,000,000/47,568,698 samples loaded
  90 batches, 4,500,000/47,568,698 samples loaded
  100 batches, 5,000,000/47,568,698 samples loaded
  110 batches, 5,500,000/47,568,698 samples loaded
  120 batches, 6,000,000/47,568,698 samples loaded
  130 batches, 6,500,000/47,568,698 samples loaded
  140 batches, 7,000,000/47,568,698 samples loaded
  150 batches, 7,500,000/47,568,698 samples loaded
  160 batches, 8,000,000/47,568,698 samples loaded
  170 batches, 8,500,000/47,568,698 samples loaded
  180 batches, 9,000,000/47,568,698 samples loaded
  190 batches, 9,500,000/47,568,698 samples loaded
  200 batches, 10,000,000/47,568,698 samples loaded
  210 batches, 10,500,000/47,568,698 samples loaded
  220 batches, 11,000,000/47,568,698 samples loaded
  230 batches, 11,500,000/47,568,698 samples loaded
  240 batches, 12,000,000/47,568,698 samples loaded
  250 batches, 12,500,000/47,568,698 samples loaded
  260 batches, 13,000,000/47,568,698 samples loaded
  270 batches, 13,500,000/47,568,698 samples loaded
  280 batches, 14,000,000/47,568,698 samples loaded
  290 batches, 14,500,000/47,568,698 samples loaded
  300 batches, 15,000,000/47,568,698 samples loaded
  310 batches, 15,500,000/47,568,698 samples loaded
  320 batches, 16,000,000/47,568,698 samples loaded
  330 batches, 16,500,000/47,568,698 samples loaded
  340 batches, 17,000,000/47,568,698 samples loaded
  350 batches, 17,500,000/47,568,698 samples loaded
  360 batches, 18,000,000/47,568,698 samples loaded
  370 batches, 18,500,000/47,568,698 samples loaded
  380 batches, 19,000,000/47,568,698 samples loaded
  390 batches, 19,500,000/47,568,698 samples loaded
  400 batches, 20,000,000/47,568,698 samples loaded
  410 batches, 20,500,000/47,568,698 samples loaded
  420 batches, 21,000,000/47,568,698 samples loaded
  430 batches, 21,500,000/47,568,698 samples loaded
  440 batches, 22,000,000/47,568,698 samples loaded
  450 batches, 22,500,000/47,568,698 samples loaded
  460 batches, 23,000,000/47,568,698 samples loaded
  470 batches, 23,500,000/47,568,698 samples loaded
  480 batches, 24,000,000/47,568,698 samples loaded
  490 batches, 24,500,000/47,568,698 samples loaded
  500 batches, 25,000,000/47,568,698 samples loaded
  510 batches, 25,500,000/47,568,698 samples loaded
  520 batches, 26,000,000/47,568,698 samples loaded
  530 batches, 26,500,000/47,568,698 samples loaded
  540 batches, 27,000,000/47,568,698 samples loaded
  550 batches, 27,500,000/47,568,698 samples loaded
  560 batches, 28,000,000/47,568,698 samples loaded
  570 batches, 28,500,000/47,568,698 samples loaded
  580 batches, 29,000,000/47,568,698 samples loaded
  590 batches, 29,500,000/47,568,698 samples loaded
  600 batches, 30,000,000/47,568,698 samples loaded
  610 batches, 30,500,000/47,568,698 samples loaded
  620 batches, 31,000,000/47,568,698 samples loaded
  630 batches, 31,500,000/47,568,698 samples loaded
  640 batches, 32,000,000/47,568,698 samples loaded
  650 batches, 32,500,000/47,568,698 samples loaded
  660 batches, 33,000,000/47,568,698 samples loaded
  670 batches, 33,500,000/47,568,698 samples loaded
  680 batches, 34,000,000/47,568,698 samples loaded
  690 batches, 34,500,000/47,568,698 samples loaded
  700 batches, 35,000,000/47,568,698 samples loaded
  710 batches, 35,500,000/47,568,698 samples loaded
  720 batches, 36,000,000/47,568,698 samples loaded
  730 batches, 36,500,000/47,568,698 samples loaded
  740 batches, 37,000,000/47,568,698 samples loaded
  750 batches, 37,500,000/47,568,698 samples loaded
  760 batches, 38,000,000/47,568,698 samples loaded
  770 batches, 38,500,000/47,568,698 samples loaded
  780 batches, 39,000,000/47,568,698 samples loaded
  790 batches, 39,500,000/47,568,698 samples loaded
  800 batches, 40,000,000/47,568,698 samples loaded
  810 batches, 40,500,000/47,568,698 samples loaded
  820 batches, 41,000,000/47,568,698 samples loaded
  830 batches, 41,500,000/47,568,698 samples loaded
  840 batches, 42,000,000/47,568,698 samples loaded
  850 batches, 42,500,000/47,568,698 samples loaded
  860 batches, 43,000,000/47,568,698 samples loaded
  870 batches, 43,500,000/47,568,698 samples loaded
  880 batches, 44,000,000/47,568,698 samples loaded
  890 batches, 44,500,000/47,568,698 samples loaded
  900 batches, 45,000,000/47,568,698 samples loaded
  910 batches, 45,500,000/47,568,698 samples loaded
  920 batches, 46,000,000/47,568,698 samples loaded
  930 batches, 46,500,000/47,568,698 samples loaded
  940 batches, 47,000,000/47,568,698 samples loaded
  950 batches, 47,500,000/47,568,698 samples loaded
  Loaded 47,568,698 rows in 46.5s (11.39 GB)
  47,427,353 neg, 141,345 pos, ratio 1:335.5, years 2018-2020

STEP B: LOAD VAL (2021-2021)

Loading val_embeddings_win2.parquet (two-pass NumPy loader)...
  Pass 1: Counting rows...
  Found 15,782,255 samples
  Pass 2: Pre-allocating arrays (15,782,255 samples, 64 features)...
  10 batches, 500,000/15,782,255 samples loaded
  20 batches, 1,000,000/15,782,255 samples loaded
  30 batches, 1,500,000/15,782,255 samples loaded
  40 batches, 2,000,000/15,782,255 samples loaded
  50 batches, 2,500,000/15,782,255 samples loaded
  60 batches, 3,000,000/15,782,255 samples loaded
  70 batches, 3,500,000/15,782,255 samples loaded
  80 batches, 4,000,000/15,782,255 samples loaded
  90 batches, 4,500,000/15,782,255 samples loaded
  100 batches, 5,000,000/15,782,255 samples loaded
  110 batches, 5,500,000/15,782,255 samples loaded
  120 batches, 6,000,000/15,782,255 samples loaded
  130 batches, 6,500,000/15,782,255 samples loaded
  140 batches, 7,000,000/15,782,255 samples loaded
  150 batches, 7,500,000/15,782,255 samples loaded
  160 batches, 8,000,000/15,782,255 samples loaded
  170 batches, 8,500,000/15,782,255 samples loaded
  180 batches, 9,000,000/15,782,255 samples loaded
  190 batches, 9,500,000/15,782,255 samples loaded
  200 batches, 10,000,000/15,782,255 samples loaded
  210 batches, 10,500,000/15,782,255 samples loaded
  220 batches, 11,000,000/15,782,255 samples loaded
  230 batches, 11,500,000/15,782,255 samples loaded
  240 batches, 12,000,000/15,782,255 samples loaded
  250 batches, 12,500,000/15,782,255 samples loaded
  260 batches, 13,000,000/15,782,255 samples loaded
  270 batches, 13,500,000/15,782,255 samples loaded
  280 batches, 14,000,000/15,782,255 samples loaded
  290 batches, 14,500,000/15,782,255 samples loaded
  300 batches, 15,000,000/15,782,255 samples loaded
  310 batches, 15,500,000/15,782,255 samples loaded
  Loaded 15,782,255 rows in 17.1s (3.78 GB)
  15,742,542 neg, 39,713 pos, ratio 1:396.4, years 2021-2021

STEP C: TRAIN & EVALUATE
Temporal split: train (2018-2020): 47,568,698 rows, val (2021-2021): 15,782,255 rows

Imbalance: ratio 335.543, scale_pos_weight 335.543196 (n_neg/n_pos from TRAIN)

Training on train (47,568,698 samples)...
  Creating train dataset...
  Creating validation dataset...
  Starting training...
Training until validation scores don't improve for 500 rounds
[100]	val's average_precision: 0.00462576
[200]	val's average_precision: 0.00455025
[300]	val's average_precision: 0.00442722
[400]	val's average_precision: 0.00454779
[500]	val's average_precision: 0.0044271
Early stopping, best iteration is:
[1]	val's average_precision: 0.0520479

Training done in 1495.0s. Best iteration: 1

Validation: ROC-AUC 0.8273, PR-AUC 0.0520, P@1% 0.0762, P@5% 0.0236, P@10% 0.0130
Model saved: /Users/gianluca/Desktop/Master's Thesis/code/data/ml/models/modelE_lgbm_win2_20251226_185534.pkl
Phase 1 completed.

======================================================================
PHASE 2: TESTING
======================================================================

Loading model from /Users/gianluca/Desktop/Master's Thesis/code/data/ml/models/modelE_lgbm_win2_20251226_185534.pkl
Processing test in batches of 50,000...
  Pass 1: Counting test rows...
  Found 15,766,956 test samples
  Pre-allocating arrays for 15,766,956 samples...
Writing to /Users/gianluca/Desktop/Master's Thesis/code/outputs/Results/ml_models/modelE_lgbm_win2_scored_20251226_185534.parquet
  10 batches, 500,000/15,766,956 rows
  20 batches, 1,000,000/15,766,956 rows
  30 batches, 1,500,000/15,766,956 rows
  40 batches, 2,000,000/15,766,956 rows
  50 batches, 2,500,000/15,766,956 rows
  60 batches, 3,000,000/15,766,956 rows
  70 batches, 3,500,000/15,766,956 rows
  80 batches, 4,000,000/15,766,956 rows
  90 batches, 4,500,000/15,766,956 rows
  100 batches, 5,000,000/15,766,956 rows
  110 batches, 5,500,000/15,766,956 rows
  120 batches, 6,000,000/15,766,956 rows
  130 batches, 6,500,000/15,766,956 rows
  140 batches, 7,000,000/15,766,956 rows
  150 batches, 7,500,000/15,766,956 rows
  160 batches, 8,000,000/15,766,956 rows
  170 batches, 8,500,000/15,766,956 rows
  180 batches, 9,000,000/15,766,956 rows
  190 batches, 9,500,000/15,766,956 rows
  200 batches, 10,000,000/15,766,956 rows
  210 batches, 10,500,000/15,766,956 rows
  220 batches, 11,000,000/15,766,956 rows
  230 batches, 11,500,000/15,766,956 rows
  240 batches, 12,000,000/15,766,956 rows
  250 batches, 12,500,000/15,766,956 rows
  260 batches, 13,000,000/15,766,956 rows
  270 batches, 13,500,000/15,766,956 rows
  280 batches, 14,000,000/15,766,956 rows
  290 batches, 14,500,000/15,766,956 rows
  300 batches, 15,000,000/15,766,956 rows
  310 batches, 15,500,000/15,766,956 rows
Completed 316 batches in 16.0s, 15,766,956 rows

Test set: 15,757,206 neg, 9,750 pos, ratio 1:1616.1, years 2022-2022

Test metrics: ROC-AUC 0.7510, PR-AUC 0.0019
  P@1%: 0.0025 (390/157,669), lift 4.00x
  P@5%: 0.0025 (1,951/788,347), lift 4.00x
  P@10%: 0.0028 (4,452/1,576,695), lift 4.57x

Top 20 features:
feature  importance
 emb_28           5
 emb_47           4
 emb_05           3
 emb_31           3
 emb_11           3
 emb_23           2
 emb_57           2
 emb_43           2
 emb_48           2
 emb_29           2
 emb_54           2
 emb_56           2
 emb_24           2
 emb_45           2
 emb_22           2
 emb_44           2
 emb_62           2
 emb_08           2
 emb_14           2
 emb_13           2

Metrics saved: /Users/gianluca/Desktop/Master's Thesis/code/outputs/Results/ml_models/modelE_lgbm_win2_metrics_20251226_185534.json

======================================================================
SUMMARY
======================================================================
Model: LightGBM, 64 features
Validation: PR-AUC 0.0520, ROC-AUC 0.8273
Test (2022-2022, 15,766,956 samples, 0.062% pos): ROC-AUC 0.7510, PR-AUC 0.0019
  P@1%: 0.0025 (4.0x), P@5%: 0.0025 (4.0x), P@10%: 0.0028 (4.6x)
Timings: train 1495.0s, val 4.6s, pred 16.0s, total 1588.9s (26.5m)
======================================================================
Done.
