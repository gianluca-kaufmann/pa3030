======================================================================
IMBALANCED SAMPLE TRAINING WDPA TRANSITION PREDICTION (LightGBM)
======================================================================

Loading data from /Users/gianluca/Desktop/Master's Thesis/code/data/ml/sample_training_imbalanced.parquet …
Loaded 10,000,000 rows with 48 columns.

Using 'transition_01' as target variable.
Remaining: 10,000,000 rows.

Using full dataset: 10,000,000 rows.

Using 40 numeric features.
Excluded columns: ['WDPA_b1', 'WDPA_prev', 'col', 'row', 'transition_01', 'x', 'y', 'year']

======================================================================
TARGET DISTRIBUTION (IMBALANCED)
======================================================================
  No transition (0):    9,957,155  (99.572%)
  New WDPA (0→1):          42,845  (0.428%)
  Class ratio:       1 : 232.4

Using scale_pos_weight: 50.00
(Automatic value based on class ratio: 232.40)

======================================================================
TRAIN/VAL/TEST SPLIT
======================================================================
Splitting into train/val/test (70/10/20) with stratification …

Training set: 7,000,000 samples (70.0%)
  No transition (0): 6,970,008
  New WDPA (0→1):    29,992

Validation set: 1,000,000 samples (10.0%)
  No transition (0): 995,716
  New WDPA (0→1):    4,284

Test set: 2,000,000 samples (20.0%)
  No transition (0): 1,991,431
  New WDPA (0→1):    8,569

======================================================================
MODEL TRAINING
======================================================================
Training LightGBM classifier …
Using scale_pos_weight=50.00 to handle imbalanced data …

Model parameters:
  objective: binary
  metric: auc
  boosting_type: gbdt
  num_leaves: 127
  learning_rate: 0.05
  feature_fraction: 0.8
  bagging_fraction: 0.8
  bagging_freq: 5
  max_depth: 14
  min_child_samples: 300
  scale_pos_weight: 50.0
  random_state: 42
  n_jobs: -1

Training with early stopping on validation set (patience=50) …
[100]	valid_0's auc: 0.947289
[200]	valid_0's auc: 0.957153
[300]	valid_0's auc: 0.962264
[400]	valid_0's auc: 0.965547
[500]	valid_0's auc: 0.96762

Training complete. Best iteration: 500
Best validation AUC: 0.9676

======================================================================
EVALUATION ON TEST SET
======================================================================
Computing predictions …

======================================================================
CLASSIFICATION REPORT
======================================================================
              precision    recall  f1-score   support

           0     0.9991    0.9617    0.9800   1991431
           1     0.0814    0.7897    0.1477      8569

    accuracy                         0.9609   2000000
   macro avg     0.5403    0.8757    0.5638   2000000
weighted avg     0.9951    0.9609    0.9764   2000000


======================================================================
CONFUSION MATRIX
======================================================================
                 Predicted Negative    Predicted Positive
Actual Negative           1,915,112                76,319
Actual Positive               1,802                 6,767

True Negatives:  1,915,112
False Positives: 76,319
False Negatives: 1,802
True Positives:  6,767

======================================================================
AREA UNDER CURVE METRICS
======================================================================
ROC-AUC:  0.9667
PR-AUC:   0.1493

======================================================================
PRECISION @ TOP-K PREDICTIONS
======================================================================
(Precision among highest-confidence predictions)

Precision @ top  1%:   0.1600  ( 3,201 positives in top  20,000 predictions)
Precision @ top  5%:   0.0713  ( 7,127 positives in top 100,000 predictions)
Precision @ top 10%:   0.0399  ( 7,989 positives in top 200,000 predictions)

Baseline (random):  0.0043  (overall positive rate in test set)

======================================================================
TOP 20 MOST IMPORTANT FEATURES
======================================================================
      feature  importance
       GPW_b1        6552
       gdp_b1        5250
     VIIRS_b1        4051
      NDVI_b1        2955
WorldClim_b15        2635
 WorldClim_b4        2555
 WorldClim_b2        2472
WorldClim_b18        2428
 WorldClim_b3        2340
 WorldClim_b7        2236
WorldClim_b19        2216
 elevation_b2        2103
 elevation_b1        2066
WorldClim_b13        2040
WorldClim_b12        1983
WorldClim_b17        1846
WorldClim_b16        1839
WorldClim_b14        1813
 WorldClim_b5        1779
 WorldClim_b8        1532

======================================================================
SUMMARY
======================================================================
Model:                LightGBM (500 iterations, scale_pos_weight=50.00)
Test samples:         2,000,000
Positive rate:        0.428%
ROC-AUC:              0.9667
PR-AUC:               0.1493
Precision @ top 1%:   0.1600
Precision @ top 5%:   0.0713
Precision @ top 10%:  0.0399
======================================================================

All done.
