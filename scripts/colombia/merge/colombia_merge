#!/usr/bin/python3
"""
Merge Colombia 1 km rasters into a yearly panel with streaming I/O and low memory use.

Summary:
- Reads per-dataset rasters from ``data`` (or ``$SCRATCH/data/ml/ready`` on cluster for storage-optimized inputs).
- Processes one year at a time, reusing earlier data when a dataset is missing for a given year.
- Filters pixels using combined mask: (SA backbone == 1) & (Colombia mask == 1).
- Streams window batches to a single Parquet panel while cleaning up memory eagerly.

Inputs:
- Year-specific raster folders under ``data/<dataset>`` with filenames containing the year.
- Static rasters (no year in filename) read once and injected into every year.
- backbone.tif (SA backbone, defines the reference grid, CRS, transform, and extent).
- colombia_backbone.tif (Colombia mask, must match SA grid CRS, transform, and shape).

Int16 Auto-Unscaling:
- Automatically detects Int16-scaled data (WorldClim, GSN, powerplants, oil_gas) and unscales (รท100).
- Works seamlessly with both original and storage-optimized inputs.
- No configuration needed - detection is automatic based on dtype.

Static vs. Yearly Datasets:
Static datasets (no year in filename, constant across all years):
  - elevation: Digital elevation model (terrain height)
  - slope: Terrain slope derived from elevation
  - oil_gas: Oil and gas infrastructure locations (fixed reference year)
  - road_infrastructure: Road network (fixed reference year)
  - GSN: Grid system network (fixed reference dataset)
  - powerplants: Power plant locations and capacity (fixed reference year)
  - WorldClim: Climate variables (static bioclimatic averages)
  
  Handling: Static files are read ONCE per window and reused for ALL years without 
  forward-filling. They appear as identical values in every output year, ensuring
  no artificial NA inflation. Static datasets are loaded first in each window,
  then combined with yearly data.

Yearly datasets (year in filename, time-varying):
  - WDPA: Protected areas (yearly updates)
  - HNTL: Nighttime lights (yearly, 2000-2020, forward-filled to 2024)
  - NDVI: Vegetation index (yearly)
  - GPW: Population density (periodic updates, e.g., 2015, 2020)
  - deforestation: Forest loss (yearly)
  - landcover: Land cover classification (yearly)
  - wildfire: Wildfire occurrence and burned area (yearly)
  
  Handling: If a dataset has no file for a target year, the script performs FORWARD-FILL,
  reusing the most recent prior year to maintain panel completeness. If a year precedes
  all available data, BACKWARD-FILL is used (reusing the earliest available year).
  
  Special case: HNTL data covers 2000-2020. For years 2021-2024, the script automatically
  forward-fills with HNTL 2020 values.

Outputs:
- Parquet panel: ``outputs/Results/merged_panel_colombia_<start>_<end>.parquet`` storing coordinates plus all bands (filtered by SA backbone & Colombia mask).

Key choices:
- Portable path resolution from ``Path(__file__)`` with automatic ``$SCRATCH`` override for HPC runs.
- Combined masking: only pixels where (SA backbone == 1) & (Colombia mask == 1) are included in the Parquet output.
- Dynamic year detection: automatically discovers all available years from the data (no hardcoded range).
- Automatic dataset classification: separates static (time-invariant) from yearly (time-varying) datasets.
- Static datasets are loaded once per window and reused across all years, eliminating artificial NA inflation.
- Forward-fill for yearly datasets to maintain complete panel and avoid gaps in derived indicators.
- Vectorized coordinate generation and Arrow streaming to avoid pandas and large in-memory tables.
- Explicit cleanup (`del`, `gc.collect`) after each window and year to keep RAM usage bounded.
- Automatic Int16 unscaling for storage-optimized inputs (no speed penalty, dtype-based detection).

Short overview of what the script does:
1) Initialize Weights & Biases and resolve paths for data and outputs.
2) Open backbone.tif (SA backbone) to define the reference grid (shape, CRS, transform). Load SA backbone directly and Colombia mask (asserts it matches the SA grid).
3) Discover all candidate rasters in data and group them by dataset and year.
4) Automatically determine the year range from available data (all years with at least one dataset).
5) Classify datasets into static (time-invariant) and yearly (time-varying) categories.
6) Determine the final band list by probing static and yearly datasets separately.
7) For each year in the detected range:
   - For each window:
     a) Load static datasets ONCE (these remain constant across all years).
     b) Load yearly datasets for the current year (with forward/backward-fill fallback).
     c) Auto-detect and unscale Int16 data if present (รท100 for WorldClim, GSN, etc.).
     d) Combine static + yearly arrays in consistent order.
   - Reproject on-the-fly to match the reference grid if needed.
   - Concatenate dataset arrays into a single band stack for the year.
   - Convert the stack to a long Arrow table (vectorized coords).
   - Apply combined mask (SA backbone & Colombia mask) to filter pixels before writing to Parquet.
   - Append the year's rows to a single Parquet file using a persistent writer.
   - Immediately free memory for arrays and Arrow tables.
"""

import re
import sys
import time
import os
import gc
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import numpy as np
import rasterio
from rasterio.enums import Resampling
from rasterio.warp import reproject
from rasterio.windows import Window
from rasterio.crs import CRS
import pyarrow as pa
import pyarrow.parquet as pq
import wandb

# Determine base path - use current script location for portability
SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent.parent

# Use $SCRATCH for data if on cluster, otherwise use project root
if "SCRATCH" in os.environ:
    DATA_ROOT = Path(os.environ["SCRATCH"]) / "data"
    # On cluster, read from storage-optimized rasters
    READY_ROOT = DATA_ROOT / "ready"
else:
    DATA_ROOT = PROJECT_ROOT / "data"
    # Locally, read from original data directory
    READY_ROOT = DATA_ROOT

# Datasets that may be Int16-scaled (ร100) if using storage-optimized inputs
# These need to be unscaled (รท100) when reading
# The script will auto-detect Int16 dtype and unscale accordingly
INT16_SCALED_DATASETS = {"WorldClim", "GSN", "powerplants", "oil_gas"}

# Use $SCRATCH if on cluster, otherwise use local outputs
if "SCRATCH" in os.environ:
    OUTPUT_ROOT = Path(os.environ["SCRATCH"]) / "outputs" / "Results"
else:
    OUTPUT_ROOT = PROJECT_ROOT / "outputs" / "Results"

OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)

# YEARS will be determined dynamically from available data
TILE_SIZE = 512


def log_progress(message: str, start_time: Optional[float] = None):
    """Log progress with optional timing."""
    if start_time:
        elapsed = time.time() - start_time
        print(f"[time] {message} ({elapsed:.1f}s)")
    else:
        print(f"[info] {message}")


def coerce_pseudo_mercator(crs, log_preflight: bool = False, filename: Optional[str] = None) -> CRS:
    """
    Coerce Pseudo-Mercator (LOCAL_CS["WGS 84 / Pseudo-Mercator"...]) to EPSG:3857.
    
    Returns CRS.from_epsg(3857) if crs is None or if WKT contains "LOCAL_CS" and "Pseudo-Mercator",
    otherwise returns the original crs unchanged.
    
    This avoids CRS operations (warping/transforming) on problematic EngineeringCRS/PROJJSON formats
    by treating them as EPSG:3857 directly.
    
    Args:
        crs: CRS object or representation to coerce
        log_preflight: If True, log original CRS WKT snippet and coerced EPSG code
        filename: Optional filename for logging
    
    Returns:
        Coerced CRS object (EPSG:3857 for Pseudo-Mercator, original otherwise)
    """
    if crs is None:
        coerced = CRS.from_epsg(3857)
        if log_preflight:
            log_progress(f"  Preflight CRS: None -> EPSG:3857{f' ({filename})' if filename else ''}")
        return coerced
    
    original_wkt_snippet = None
    was_pseudo_mercator = False
    
    try:
        # Get WKT representation to check for LOCAL_CS and Pseudo-Mercator
        crs_wkt = None
        if isinstance(crs, CRS):
            try:
                crs_wkt = crs.to_wkt()
            except:
                try:
                    crs_wkt = str(crs)
                except:
                    pass
        else:
            crs_wkt = str(crs)
        
        if crs_wkt:
            original_wkt_snippet = crs_wkt[:200] if len(crs_wkt) > 200 else crs_wkt
            if "LOCAL_CS" in crs_wkt and "Pseudo-Mercator" in crs_wkt:
                was_pseudo_mercator = True
                coerced = CRS.from_epsg(3857)
                if log_preflight:
                    log_progress(f"  Preflight CRS (coerced): {original_wkt_snippet}... -> EPSG:3857{f' ({filename})' if filename else ''}")
                return coerced
    except:
        pass
    
    # Return original CRS if it's already a CRS object, otherwise try to convert
    if isinstance(crs, CRS):
        coerced = crs
    else:
        try:
            coerced = CRS(crs)
        except:
            coerced = CRS.from_epsg(3857)  # Fallback to EPSG:3857 if conversion fails
    
    if log_preflight:
        try:
            coerced_epsg = coerced.to_epsg() if coerced else None
            epsg_str = f"EPSG:{coerced_epsg}" if coerced_epsg else str(coerced)
        except:
            epsg_str = str(coerced)
        wkt_info = f" (WKT: {original_wkt_snippet}...)" if original_wkt_snippet else ""
        log_progress(f"  Preflight CRS: {wkt_info} -> {epsg_str}{f' ({filename})' if filename else ''}")
    
    return coerced


def normalize_crs(crs, debug_filename: Optional[str] = None, _debug_log: Optional[dict] = None) -> Optional[CRS]:
    """
    Normalize CRS to a standard format, handling PROJ JSON and EngineeringCRS.
    
    This function handles cases where CRS is in PROJ JSON format, EngineeringCRS, or other formats
    that might not be directly compatible with rasterio reprojection operations.
    Only converts Pseudo-Mercator CRS to EPSG:3857 when clearly detected.
    Does not default to EPSG:3857 otherwise.
    
    Args:
        crs: CRS object or string representation
        debug_filename: Optional filename for debug logging (one-time per session)
        _debug_log: Optional dict for tracking debug state (internal use)
        
    Returns:
        Normalized CRS object, or None if CRS cannot be determined
    """
    if _debug_log is None:
        _debug_log = {}
    if crs is None:
        return None
    
    try:
        # Get string representation and type information
        crs_str = ""
        crs_name = None
        crs_type = None
        
        if isinstance(crs, CRS):
            try:
                # Get CRS type (e.g., 'EngineeringCRS', 'GeographicCRS', etc.)
                if hasattr(crs, '__class__'):
                    crs_type = crs.__class__.__name__
                
                # Try to get the name attribute if available
                if hasattr(crs, 'name'):
                    crs_name = str(crs.name).lower() if crs.name else None
                
                # Try to_string() but catch errors (fails for some PROJJSON/EngineeringCRS)
                try:
                    crs_str = crs.to_string()
                except:
                    # If to_string() fails, try WKT or other representation
                    try:
                        crs_str = str(crs)
                    except:
                        crs_str = ""
            except:
                crs_str = str(crs)
        else:
            crs_str = str(crs)
        
        # One-time debug logging for first file
        if debug_filename and "first_crs_debug" not in _debug_log:
            log_progress(f"\n=== CRS Debug Info (first file: {debug_filename}) ===")
            log_progress(f"  Original CRS type: {crs_type}")
            log_progress(f"  Original CRS name: {crs_name}")
            log_progress(f"  Original CRS string (first 200 chars): {crs_str[:200] if crs_str else 'N/A'}")
            _debug_log["first_crs_debug"] = True
        
        # PSEUDO-MERCATOR DETECTION (only when clearly detected)
        # Check multiple indicators for Pseudo-Mercator / EPSG:3857
        is_pseudo_mercator = False
        
        # Method 1: Check CRS name
        if crs_name and ("pseudo" in crs_name or "popular visualisation crs" in crs_name):
            if "mercator" in crs_name or "3857" in crs_name:
                is_pseudo_mercator = True
        
        # Method 2: Check string representation
        crs_str_lower = crs_str.lower()
        if "pseudo-mercator" in crs_str_lower or "pseudo mercator" in crs_str_lower:
            is_pseudo_mercator = True
        if "3857" in crs_str and ("epsg" in crs_str_lower or "epsg:3857" in crs_str_lower):
            is_pseudo_mercator = True
        if "popular visualisation crs" in crs_str_lower:
            is_pseudo_mercator = True
        
        # Method 3: Check for EngineeringCRS with WGS 84 in name
        if crs_type == "EngineeringCRS" or "engineeringcrs" in crs_str_lower:
            if crs_name and "wgs 84" in crs_name and ("pseudo" in crs_name or "mercator" in crs_name):
                is_pseudo_mercator = True
        
        # Method 4: Check for PROJJSON with Pseudo-Mercator indicators
        is_proj_json = "$schema" in crs_str_lower or "projjson" in crs_str_lower or crs_str.strip().startswith("{")
        if is_proj_json:
            # Try to parse JSON and look for indicators
            import json
            try:
                crs_dict = json.loads(crs_str)
                # Look for EPSG code
                if "id" in crs_dict:
                    id_str = str(crs_dict["id"])
                    if "3857" in id_str and "EPSG" in id_str.upper():
                        is_pseudo_mercator = True
                # Check name field
                if "name" in crs_dict:
                    name = str(crs_dict["name"]).lower()
                    if ("pseudo" in name or "3857" in name) and "mercator" in name:
                        is_pseudo_mercator = True
            except:
                pass
        
        # If clearly detected as Pseudo-Mercator, convert to EPSG:3857
        if is_pseudo_mercator:
            if debug_filename and "first_crs_debug" in _debug_log and _debug_log["first_crs_debug"]:
                log_progress(f"  Detected Pseudo-Mercator, normalizing to EPSG:3857")
                log_progress("=== End CRS Debug ===\n")
                _debug_log["first_crs_debug"] = False  # Prevent duplicate logging
            return CRS.from_epsg(3857)
        
        # Try to extract explicit EPSG code
        if isinstance(crs_str, str) and crs_str.upper().startswith("EPSG:"):
            epsg_code = crs_str.split(":")[-1]
            if epsg_code.isdigit():
                return CRS.from_epsg(int(epsg_code))
        
        # Try to use CRS object directly if valid
        if isinstance(crs, CRS) and not is_proj_json:
            try:
                # Test if CRS is usable by checking for to_wkt
                if hasattr(crs, 'to_wkt'):
                    crs.to_wkt()
                    return crs
            except:
                pass
        
        # Robust fallback: try to use original CRS object if it's a valid CRS instance
        # This handles cases where CRS is valid but we couldn't extract EPSG code
        if isinstance(crs, CRS):
            try:
                # Test if CRS is usable by trying to access its properties
                _ = crs.to_dict() if hasattr(crs, 'to_dict') else None
                return crs
            except:
                # If to_dict fails, try to_wkt as a test
                try:
                    _ = crs.to_wkt() if hasattr(crs, 'to_wkt') else None
                    return crs
                except:
                    pass
        
        # Last resort: try to create CRS from string representation using constructor
        if crs_str:
            try:
                # Try CRS constructor which can handle various formats
                return CRS(crs_str)
            except:
                try:
                    # Try WKT format explicitly
                    if "PROJCS" in crs_str or "GEOGCS" in crs_str or "COMPD_CS" in crs_str:
                        return CRS.from_wkt(crs_str)
                except:
                    pass
        
        # If all else fails, return None (should be rare for normal CRSs)
        if debug_filename and "first_crs_debug" in _debug_log and _debug_log["first_crs_debug"]:
            log_progress(f"  Could not definitively identify CRS")
            log_progress("=== End CRS Debug ===\n")
            _debug_log["first_crs_debug"] = False
        return None
        
    except Exception as e:
        # Robust fallback: try to return original CRS if it's a valid CRS object
        if isinstance(crs, CRS):
            try:
                return crs
            except:
                pass
        
        # If normalization fails completely, return None
        if debug_filename and "first_crs_debug" in _debug_log and _debug_log["first_crs_debug"]:
            log_progress(f"  CRS normalization error: {e}")
            log_progress("=== End CRS Debug ===\n")
            _debug_log["first_crs_debug"] = False
        return None


def open_reference_backbone() -> Tuple[Path, rasterio.DatasetReader, np.ndarray, np.ndarray]:
    """Open backbone.tif (SA backbone) as reference grid and load Colombia mask for filtering.
    
    Returns:
        Tuple of (ref_path, ref_src, sa_backbone_mask, colombia_mask) where:
        - ref_path: Path to reference grid (backbone.tif)
        - ref_src: Opened rasterio dataset for reference grid
        - sa_backbone_mask: SA backbone loaded directly from reference grid (for filtering)
        - colombia_mask: Colombia mask loaded directly (must match SA grid)
    """
    log_progress("Loading SA backbone as reference grid...")
    
    # Search for backbone.tif (SA backbone, used as reference grid)
    ref_paths = [
        READY_ROOT / "backbone" / "backbone.tif",
        DATA_ROOT / "backbone" / "backbone.tif",
    ]
    
    ref_path = None
    for p in ref_paths:
        if p.exists():
            ref_path = p
            break
    
    if ref_path is None:
        print("ERROR: backbone.tif (SA backbone) not found.")
        print("\nChecked locations:")
        for p in ref_paths:
            print(f"   - {p} (exists: {p.exists()})")
        print("\nPlease ensure backbone.tif exists in data/ready/backbone/")
        sys.exit(1)
    
    ref_src = rasterio.open(ref_path)
    ref_profile = ref_src.profile
    
    log_progress(f"Using {ref_path.name} as reference grid")
    log_progress(f"Reference grid: {ref_src.shape} pixels, CRS: {ref_src.crs}")
    
    # Load SA backbone directly from reference grid (no reprojection needed)
    log_progress("Loading SA backbone mask directly from reference grid...")
    sa_backbone_data = ref_src.read(1, masked=False).astype(np.uint8)
    # Binarize SA backbone
    sa_backbone_mask = (sa_backbone_data > 0.5).astype(np.uint8)
    
    log_progress(f"  SA backbone pixels: {(sa_backbone_mask == 1).sum():,}")
    
    # Load Colombia mask and assert it matches the SA grid
    log_progress("Loading Colombia backbone mask (must match SA grid)...")
    colombia_mask_paths = [
        READY_ROOT / "backbone" / "colombia_backbone.tif",
        DATA_ROOT / "backbone" / "colombia_backbone.tif",
    ]
    
    colombia_mask_path = None
    for p in colombia_mask_paths:
        if p.exists():
            colombia_mask_path = p
            break
    
    if colombia_mask_path is None:
        print("ERROR: colombia_backbone.tif not found.")
        print("\nChecked locations:")
        for p in colombia_mask_paths:
            print(f"   - {p} (exists: {p.exists()})")
        print("\nPlease ensure colombia_backbone.tif exists. Run backbone_colombia script first.")
        ref_src.close()
        sys.exit(1)
    
    # Load Colombia mask directly and assert it matches SA grid
    with rasterio.open(colombia_mask_path) as colombia_src:
        # Assert shape matches
        if colombia_src.shape != ref_src.shape:
            ref_src.close()
            raise ValueError(
                f"colombia_backbone.tif shape {colombia_src.shape} does not match "
                f"SA backbone shape {ref_src.shape}"
            )
        
        # Coerce Pseudo-Mercator to EPSG:3857 for comparison (avoids CRS operations)
        colombia_crs = coerce_pseudo_mercator(colombia_src.crs)
        ref_crs = coerce_pseudo_mercator(ref_src.crs)
        
        try:
            crs_match = colombia_crs.to_epsg() == ref_crs.to_epsg()
        except:
            try:
                crs_match = colombia_crs.to_string() == ref_crs.to_string()
            except:
                crs_match = False
        
        if not crs_match:
            ref_src.close()
            raise ValueError(
                f"colombia_backbone.tif CRS {colombia_crs} does not match "
                f"SA backbone CRS {ref_crs}"
            )
        
        # Assert transform matches
        if not np.allclose(colombia_src.transform, ref_src.transform, atol=1e-6):
            ref_src.close()
            raise ValueError(
                f"colombia_backbone.tif transform does not match SA backbone transform"
            )
        
        # Read Colombia mask directly (no reprojection needed)
        colombia_mask_data = colombia_src.read(1, masked=False).astype(np.uint8)
        # Binarize Colombia mask
        colombia_mask = (colombia_mask_data > 0.5).astype(np.uint8)
    
    log_progress(f"  Colombia mask pixels: {(colombia_mask == 1).sum():,}")
    log_progress(f"  Colombia mask matches SA grid (CRS, transform, shape)")
    
    # Calculate combined mask statistics
    combined_mask = (sa_backbone_mask == 1) & (colombia_mask == 1)
    total_pixels = combined_mask.size
    keep_pixels = int(combined_mask.sum())
    
    log_progress(f"Combined filter: SA backbone & Colombia mask")
    log_progress(f"  Keep pixels (SA & Colombia): {keep_pixels:,} ({100*keep_pixels/total_pixels:.1f}%)")
    
    return ref_path, ref_src, sa_backbone_mask, colombia_mask


def discover_raster_catalog() -> Dict[str, Dict[Optional[int], List[Path]]]:
    """
    Scan READY_ROOT subfolders and build a catalog:
    {dataset_name: {year(or None for static): [paths]}}
    """
    log_progress("Building raster catalog...")
    catalog: Dict[str, Dict[Optional[int], List[Path]]] = {}
    year_re = re.compile(r"(19|20)\d{2}")
    if not READY_ROOT.exists():
        print(f"ERROR: Data directory not found: {READY_ROOT}")
        print(f"  Data root: {DATA_ROOT}")
        print("\nPlease ensure data directory exists.")
        sys.exit(1)
    
    total_files = 0
    for subdir in sorted([p for p in READY_ROOT.iterdir() if p.is_dir()]):
        dataset = subdir.name
        catalog.setdefault(dataset, {})
        files_in_dataset = 0
        for tif in sorted(subdir.rglob("*.tif")):
            m = year_re.search(tif.name)
            year = int(m.group(0)) if m else None
            catalog[dataset].setdefault(year, []).append(tif)
            files_in_dataset += 1
            total_files += 1
        if files_in_dataset > 0:
            log_progress(f"  {dataset}: {files_in_dataset} files")
    
    log_progress(f"Total files catalogued: {total_files}")
    return catalog


def detect_year_range(catalog: Dict[str, Dict[Optional[int], List[Path]]]) -> List[int]:
    """
    Automatically detect the year range from available data.
    Returns a sorted list of all years that have at least one dataset.
    """
    all_years = set()
    for dataset, entries in catalog.items():
        for year in entries.keys():
            if year is not None:  # Exclude None (static files)
                all_years.add(year)
    
    if not all_years:
        print("ERROR: No yearly data found in catalog")
        sys.exit(1)
    
    years = sorted(all_years)
    log_progress(f"Detected year range: {min(years)} - {max(years)} ({len(years)} years)")
    log_progress(f"Years: {years}")
    return years


def check_alignment(src_path: Path, ref_profile: dict) -> bool:
    """Check if a raster is already aligned with the reference grid."""
    try:
        with rasterio.open(src_path) as src:
            # Coerce Pseudo-Mercator to EPSG:3857 for comparison (skip CRS operations)
            src_crs = coerce_pseudo_mercator(src.crs)
            ref_crs = coerce_pseudo_mercator(ref_profile["crs"])
            
            # Compare using EPSG codes when possible
            try:
                crs_match = src_crs.to_epsg() == ref_crs.to_epsg()
            except:
                # Fallback to string comparison
                try:
                    crs_match = src_crs.to_string() == ref_crs.to_string()
                except:
                    crs_match = False
            
            return (
                src.shape == (ref_profile["height"], ref_profile["width"]) and
                crs_match and
                np.allclose(src.transform, ref_profile["transform"], atol=1e-6)
            )
    except Exception as e:
        log_progress(f"  Warning: Alignment check failed for {src_path.name}: {e}")
        return False


def read_reproject_to_ref(path: Path, ref_profile: dict) -> np.ndarray:
    """
    Read a raster and reproject/resample to the reference profile, returning a float32 array.
    
    CRITICAL: Never performs CRS operations on Pseudo-Mercator files.
    If CRS is LOCAL_CS["WGS 84 / Pseudo-Mercator"], coerces to EPSG:3857 and reads directly if aligned.
    """
    log_progress(f"  Processing {path.name}...")
    start_time = time.time()
    
    # Check if already aligned
    if check_alignment(path, ref_profile):
        log_progress(f"    Already aligned, reading directly...")
        with rasterio.open(path) as src:
            data = src.read(masked=False).astype(np.float32)
            # Handle nodata values
            if src.nodata is not None:
                data[data == src.nodata] = np.nan
            log_progress(f"    Read {path.name} ({time.time() - start_time:.1f}s)")
            return data
    
    # Before reprojecting, check if this is a Pseudo-Mercator file
    with rasterio.open(path) as src:
        is_pseudo_mercator = False
        if src.crs is not None:
            try:
                crs_wkt = str(src.crs) if not isinstance(src.crs, CRS) else src.crs.to_wkt()
                if "LOCAL_CS" in crs_wkt and "Pseudo-Mercator" in crs_wkt:
                    is_pseudo_mercator = True
            except:
                pass
        
        # If Pseudo-Mercator and not aligned, raise error (cannot reproject)
        if is_pseudo_mercator:
            raise ValueError(
                f"Pseudo-Mercator file {path.name} is not aligned with reference grid. "
                f"Shape: {src.shape} vs {ref_profile['height']}, {ref_profile['width']}, "
                f"Transform mismatch. Cannot reproject Pseudo-Mercator files."
            )
    
    log_progress(f"    Reprojecting {path.name}...")
    with rasterio.open(path) as src:
        dst_height = ref_profile["height"]
        dst_width = ref_profile["width"]
        dst_transform = ref_profile["transform"]
        # Coerce Pseudo-Mercator to EPSG:3857 (avoids CRS operations on EngineeringCRS/PROJJSON)
        src_crs = coerce_pseudo_mercator(src.crs)
        dst_crs = coerce_pseudo_mercator(ref_profile["crs"])
        
        count = src.count
        
        # Standard reprojection path
        dst = np.full((count, dst_height, dst_width), np.nan, dtype=np.float32)
        
        for b in range(1, count + 1):
            reproject(
                source=rasterio.band(src, b),
                destination=dst[b - 1],
                src_transform=src.transform,
                src_crs=src_crs,
                src_nodata=src.nodata,
                dst_transform=dst_transform,
                dst_crs=dst_crs,
                resampling=Resampling.nearest,
                dst_nodata=np.nan,
            )
        
        log_progress(f"    Reprojected {path.name} ({time.time() - start_time:.1f}s)")
        # Always return (count, height, width) shape for consistency with aligned path
        return dst


def read_window_direct(src_path: Path, window: Window, dataset: Optional[str] = None) -> np.ndarray:
    """
    Read a window directly from a raster file without any CRS operations or reprojection.
    Used for probing band structure.
    
    Args:
        src_path: Path to source raster file
        window: Window to read
        dataset: Optional dataset name (for Int16 unscaling detection)
    
    Returns:
        Array of shape (bands, height, width) as float32
    """
    with rasterio.open(src_path) as src:
        count = src.count
        dst_height = int(window.height)
        dst_width = int(window.width)
        
        # Read window directly
        data = src.read(window=window, masked=False).astype(np.float32)
        
        # Handle nodata values
        if src.nodata is not None:
            data[data == src.nodata] = np.nan
        
        # Auto-unscale Int16 data if needed
        needs_unscaling = (
            dataset is not None and 
            dataset in INT16_SCALED_DATASETS and
            src.dtypes[0] in ('int16', 'Int16')
        )
        if needs_unscaling:
            data = data / 100.0
        
        return data


def iter_reference_windows(ref_profile: dict, tile_size: int = TILE_SIZE):
    """Yield rasterio Window objects covering the reference grid in tiles."""
    height = ref_profile["height"]
    width = ref_profile["width"]
    for row_off in range(0, height, tile_size):
        for col_off in range(0, width, tile_size):
            h = min(tile_size, height - row_off)
            w = min(tile_size, width - col_off)
            yield Window(col_off=col_off, row_off=row_off, width=w, height=h)


def read_reproject_window(src_path: Path, ref_profile: dict, window: Window, 
                          dataset: Optional[str] = None, _debug_state: Optional[dict] = None) -> np.ndarray:
    """
    Read a source raster and reproject only the given reference window to a float32 array (B, h, w).
    
    CRITICAL: Never performs CRS operations (reproject, WarpedVRT, etc.) on Pseudo-Mercator files.
    If CRS is LOCAL_CS["WGS 84 / Pseudo-Mercator"], coerces to EPSG:3857 and reads directly if aligned.
    
    Optimization: If already aligned, read window directly without reprojection.
    Pseudo-Mercator CRS is coerced to EPSG:3857 and treated as aligned if transform/shape match.
    
    Int16 auto-unscaling: If the dataset is in INT16_SCALED_DATASETS and the file dtype is Int16,
    automatically divides by 100 to restore original values. This allows seamless use of storage-optimized
    inputs without manual intervention.
    
    Args:
        src_path: Path to source raster file
        ref_profile: Reference grid profile
        window: Window to read
        dataset: Optional dataset name (for Int16 unscaling detection)
        _debug_state: Optional dict for tracking debug state (internal use)
    """
    if _debug_state is None:
        _debug_state = {}
    with rasterio.open(src_path) as src:
        dst_height = int(window.height)
        dst_width = int(window.width)
        count = src.count
        
        # Auto-detect if this is Int16 scaled data that needs unscaling
        needs_unscaling = (
            dataset is not None and 
            dataset in INT16_SCALED_DATASETS and
            src.dtypes[0] in ('int16', 'Int16')
        )
        
        # Coerce Pseudo-Mercator to EPSG:3857 (avoids CRS operations on EngineeringCRS/PROJJSON)
        src_crs = coerce_pseudo_mercator(src.crs)
        dst_crs = coerce_pseudo_mercator(ref_profile["crs"])
        
        # Check if source CRS was Pseudo-Mercator (coerced to EPSG:3857)
        # If so, we MUST read directly without any reprojection operations
        is_pseudo_mercator = False
        if src.crs is not None:
            try:
                crs_wkt = str(src.crs) if not isinstance(src.crs, CRS) else src.crs.to_wkt()
                if "LOCAL_CS" in crs_wkt and "Pseudo-Mercator" in crs_wkt:
                    is_pseudo_mercator = True
            except:
                pass
        
        # Fast path: if already aligned, just read the window directly (MUCH faster)
        # This handles Pseudo-Mercator files that are already aligned
        try:
            crs_match = src_crs.to_epsg() == dst_crs.to_epsg()
        except:
            # Fallback to string comparison
            try:
                crs_match = src_crs.to_string() == dst_crs.to_string()
            except:
                crs_match = False
        
        shape_match = src.shape == (ref_profile["height"], ref_profile["width"])
        transform_match = np.allclose(src.transform, ref_profile["transform"], atol=1e-6)
        
        # If Pseudo-Mercator and aligned, ALWAYS read directly (never reproject)
        if is_pseudo_mercator and shape_match and transform_match:
            data = src.read(window=window, masked=False).astype(np.float32)
            # Handle nodata values
            if src.nodata is not None:
                data[data == src.nodata] = np.nan
            
            # Auto-unscale Int16 data (รท100) if detected
            if needs_unscaling:
                data = data / 100.0
            
            return data
        
        # For non-Pseudo-Mercator files, check alignment normally
        if shape_match and crs_match and transform_match:
            data = src.read(window=window, masked=False).astype(np.float32)
            # Handle nodata values
            if src.nodata is not None:
                data[data == src.nodata] = np.nan
            
            # Auto-unscale Int16 data (รท100) if detected
            if needs_unscaling:
                data = data / 100.0
            
            return data
        
        # Slow path: reprojection needed (only for non-Pseudo-Mercator files)
        # If we reach here with Pseudo-Mercator, it means alignment failed - this is an error
        if is_pseudo_mercator:
            raise ValueError(
                f"Pseudo-Mercator file {src_path.name} is not aligned with reference grid. "
                f"Shape: {src.shape} vs {ref_profile['height']}, {ref_profile['width']}, "
                f"Transform mismatch. Cannot reproject Pseudo-Mercator files."
            )
        
        dst_transform = rasterio.windows.transform(window, ref_profile["transform"])
        
        # Standard reprojection path (using coerced CRS)
        dst = np.full((count, dst_height, dst_width), np.nan, dtype=np.float32)
        for b in range(1, count + 1):
            reproject(
                source=rasterio.band(src, b),
                destination=dst[b - 1],
                src_transform=src.transform,
                src_crs=src_crs,
                src_nodata=src.nodata,
                dst_transform=dst_transform,
                dst_crs=dst_crs,
                resampling=Resampling.nearest,
                dst_nodata=np.nan,
            )
        
        # Auto-unscale Int16 data (รท100) if detected
        if needs_unscaling:
            dst = dst / 100.0
        
        return dst if count > 1 else dst[0][np.newaxis, ...]


def window_to_arrow(year: int, window: Window, band_stack: np.ndarray, band_names: List[str], 
                    ref_profile: dict, sa_backbone_window: np.ndarray, colombia_mask_window: np.ndarray) -> pa.Table:
    """Create an Arrow table for a window, filtering by combined mask (SA backbone & Colombia mask)."""
    h = int(window.height)
    w = int(window.width)
    rows = (np.arange(h, dtype=np.int32) + int(window.row_off)).repeat(w)
    cols = np.tile(np.arange(w, dtype=np.int32) + int(window.col_off), h)
    transform = ref_profile["transform"]
    xs = (transform.a * cols + transform.b * rows + transform.c).astype(np.float64)
    ys = (transform.d * cols + transform.e * rows + transform.f).astype(np.float64)

    # Flatten bands (band_stack is already float32, no need to convert)
    flat_bands = [band_stack[i].reshape(-1) for i in range(band_stack.shape[0])]

    # Apply combined filter: keep pixels where (sa_backbone == 1) & (colombia_mask == 1)
    sa_backbone_flat = sa_backbone_window.reshape(-1)
    colombia_mask_flat = colombia_mask_window.reshape(-1)
    keep = (sa_backbone_flat == 1) & (colombia_mask_flat == 1)

    # Skip empty windows (no pixels to keep)
    if keep.sum() == 0:
        return pa.table({
            "year": pa.array([], type=pa.int16()),
            "x": pa.array([], type=pa.float64()),
            "y": pa.array([], type=pa.float64()),
            "row": pa.array([], type=pa.int32()),
            "col": pa.array([], type=pa.int32()),
            **{name: pa.array([], type=pa.float32()) for name in band_names}
        })

    cols_dict = {
        "year": pa.array(np.full(keep.sum(), year, dtype=np.int16)),
        "x": pa.array(xs[keep]),
        "y": pa.array(ys[keep]),
        "row": pa.array(rows[keep]),
        "col": pa.array(cols[keep]),
    }
    for name, fb in zip(band_names, flat_bands):
        cols_dict[name] = pa.array(fb[keep])
    return pa.table(cols_dict)


def build_band_names(dataset: str, arrays: List[np.ndarray]) -> List[str]:
    """Build band names for a dataset given contributing arrays."""
    names: List[str] = []
    idx = 1
    for arr in arrays:
        if arr.ndim == 2:
            names.append(f"{dataset}")
        else:
            for i in range(arr.shape[0]):
                names.append(f"{dataset}_b{idx}")
                idx += 1
    
    # If all were single-band and duplicates resulted, ensure unique suffixes
    if len(names) != len(set(names)):
        names = [f"{dataset}_{i+1}" for i in range(len(names))]
    return names


def apply_temporal_coverage_rules(band_stack: np.ndarray, band_names: List[str], year: int, 
                                  log_once: Optional[dict] = None) -> None:
    """
    Apply temporal coverage rules to enforce dataset-specific valid time ranges.
    Modifies band_stack in-place by setting values to NaN for years outside valid coverage.
    
    Note: HNTL now covers 2000-2020 and is forward-filled to 2024 automatically.
    GDP has been excluded from the feature set.
    
    Args:
        band_stack: (bands, height, width) array to modify in-place
        band_names: List of band names corresponding to band_stack bands
        year: The year being processed
        log_once: Optional dict for tracking which rules have been logged
    """
    if log_once is None:
        log_once = {}
    # Define temporal coverage rules: {band_name: first_valid_year}
    # Currently no temporal coverage rules active (GDP excluded, HNTL handles 2000-2020 natively)
    TEMPORAL_RULES = {}
    
    for band_name, first_valid_year in TEMPORAL_RULES.items():
        if band_name in band_names and year < first_valid_year:
            band_idx = band_names.index(band_name)
            band_stack[band_idx, :, :] = np.nan
            
            # Log once per band when rule is first applied
            if band_name not in log_once:
                log_progress(f"  Temporal coverage: Setting {band_name} to NaN for year {year} < {first_valid_year}")
                log_once[band_name] = True


def classify_datasets(catalog: Dict[str, Dict[Optional[int], List[Path]]]) -> Tuple[List[str], List[str]]:
    """
    Classify datasets as static (time-invariant) or yearly (time-varying).
    
    Static datasets have no year in filename (key = None in catalog).
    Yearly datasets have years in filenames (key = int in catalog).
    
    Returns: (static_datasets, yearly_datasets)
    """
    static = []
    yearly = []
    
    for dataset, entries in catalog.items():
        has_static = None in entries
        has_yearly = any(y is not None for y in entries.keys())
        
        if has_static and not has_yearly:
            # Only static files, no yearly variation
            static.append(dataset)
        elif has_yearly:
            # Has yearly files (may also have some static files, but treat as yearly)
            yearly.append(dataset)
        # Skip datasets with neither (shouldn't happen)
    
    return sorted(static), sorted(yearly)


def load_dataset_window(
    paths: List[Path],
    ref_profile: dict,
    window: Window,
    expected_bands: int,
    window_height: int,
    window_width: int,
    dataset: Optional[str] = None
) -> List[np.ndarray]:
    """
    Load and reproject dataset files for a given window.
    Returns NaN placeholder if no paths provided.
    
    Args:
        paths: List of file paths to load
        ref_profile: Reference grid profile
        window: Window to read
        expected_bands: Expected number of bands for placeholder
        window_height: Window height for placeholder
        window_width: Window width for placeholder
        dataset: Dataset name (for Int16 unscaling detection)
    """
    if not paths:
        # No data available - create NaN placeholder
        placeholder = np.full((expected_bands, window_height, window_width), np.nan, dtype=np.float32)
        return [placeholder]
    
    arrays = []
    for p in paths:
        try:
            arr = read_reproject_window(p, ref_profile, window, dataset=dataset)
            arrays.append(arr)
        except Exception as e:
            # Individual file failure: log warning and skip this file (continue with others)
            log_progress(f"    Warning: Failed to read {p.name} in window, skipping: {e}")
            continue
    
    # If all files failed, return NaN placeholder
    if not arrays:
        placeholder = np.full((expected_bands, window_height, window_width), np.nan, dtype=np.float32)
        return [placeholder]
    
    # Validate total number of bands produced
    total_bands = 0
    for arr in arrays:
        if arr.ndim == 2:
            total_bands += 1
        else:
            total_bands += arr.shape[0]
    
    # If band count doesn't match expected (e.g., due to failed files), return NaN placeholder
    if total_bands != expected_bands:
        log_progress(f"    Warning: Band count mismatch (expected {expected_bands}, got {total_bands}), using NaN placeholder")
        placeholder = np.full((expected_bands, window_height, window_width), np.nan, dtype=np.float32)
        return [placeholder]
    
    return arrays


# Removed pandas DataFrame conversions in favor of direct Arrow streaming per window
# Removed global DataFrame creation; coordinates and bands are emitted per window
# Removed process_dataset_for_year and process_dataset functions (unused legacy code)


def main():
    """Main processing function with progress monitoring."""
    # Initialize wandb FIRST before any processing
    print("๐ Initializing Weights & Biases connection...")
    
    # Get credentials from environment variables
    wandb_api_key = os.environ.get("WANDB_API_KEY")
    wandb_entity = os.environ.get("WANDB_ENTITY")
    
    if not wandb_api_key:
        print("Warning: WANDB_API_KEY not found in environment variables")
        print("   Set it with: export WANDB_API_KEY='your-key-here'")
    
    if not wandb_entity:
        print("Warning: WANDB_ENTITY not found in environment variables")
        print("   Set it with: export WANDB_ENTITY='your-entity-here'")
    
    total_start_time = time.time()
    log_progress("Starting optimized merge process...")
    
    # Log data source
    log_progress(f"Using data from: {READY_ROOT}")
    log_progress(f"Int16 auto-unscaling enabled for: {', '.join(INT16_SCALED_DATASETS)}")
    
    # Setup reference using SA backbone (backbone.tif) and load filtering masks
    ref_path, ref_src, sa_backbone_mask, colombia_mask = open_reference_backbone()
    ref_profile = ref_src.profile.copy()
    ref_profile.update({
        "count": 1,
        "dtype": np.float32,
        "nodata": np.nan,
        "compress": "lzw",
    })
    
    # Calculate combined mask statistics
    combined_mask = (sa_backbone_mask == 1) & (colombia_mask == 1)
    keep_pixels = int(combined_mask.sum())
    total_pixels = combined_mask.size
    
    ref_src.close()  # Close the reference file
    
    # Build catalog and detect year range
    catalog = discover_raster_catalog()
    YEARS = detect_year_range(catalog)
    
    # Dynamic Parquet path based on detected year range
    PARQUET_PATH = OUTPUT_ROOT / f"merged_panel_colombia_{min(YEARS)}_{max(YEARS)}.parquet"
    
    # Now initialize wandb with detected years
    wandb.init(
        project="merge",
        entity=wandb_entity,
        save_code=True,
        config={
            "years": YEARS,
            "year_range": f"{min(YEARS)}-{max(YEARS)}",
            "num_years": len(YEARS),
            "data_root": str(DATA_ROOT),
            "ready_root": str(READY_ROOT),
            "output_root": str(OUTPUT_ROOT),
            "scratch_mode": "SCRATCH" in os.environ,
            "int16_auto_unscaling_enabled": True,
            "int16_scaled_datasets": list(INT16_SCALED_DATASETS),
        }
    )
    print("Weights & Biases connected successfully!")
    
    wandb.log({"status": "started", "start_time": total_start_time})
    
    # Log reference grid info
    wandb.log({
        "reference_grid/height": ref_profile["height"],
        "reference_grid/width": ref_profile["width"],
        "reference_grid/total_pixels": total_pixels,
        "reference_grid/keep_pixels": keep_pixels,
        "reference_grid/keep_percentage": 100 * keep_pixels / total_pixels,
        "reference_grid/crs": str(ref_profile["crs"]),
        "reference_grid/filter": "SA_backbone_and_Colombia_mask",
    })

    # Identify datasets to include
    # Note: backbone, gdp, gdp_preprocessing, gdp_processed, DynamicWorld, DW, and assetlevel are excluded
    include_priority = [
        "WDPA", "HNTL", "NDVI", "GPW",
        "deforestation", "landcover", "WorldClim",
        "elevation", "slope", "oil_gas", "road_infrastructure",
        "GSN", "powerplants", "wildfire",
    ]

    datasets_present = [d for d in catalog.keys() 
                       if d in include_priority or d.lower() in [x.lower() for x in include_priority]]
    datasets_present = sorted(set(datasets_present), 
                             key=lambda d: (0 if d.upper() == "WDPA" else 1, d.lower()))
    
    if not datasets_present:
        print(f"ERROR: No datasets found under {READY_ROOT}")
        print(f"  Expected datasets: {', '.join(include_priority)}")
        sys.exit(1)
    
    log_progress(f"Datasets to process: {', '.join(datasets_present)}")
    
    # Classify datasets into static (time-invariant) and yearly (time-varying)
    # Hard override: these datasets are ALWAYS treated as static, even if year-stamped files exist
    STATIC_OVERRIDE = {
        "elevation", "slope", "oil_gas", "road_infrastructure",
        "GSN", "powerplants", "WorldClim"
    }
    # Normalize for robust matching
    static_override_norm = {s.lower() for s in STATIC_OVERRIDE}
    static_datasets = [d for d in datasets_present if d.lower() in static_override_norm]
    yearly_datasets = [d for d in datasets_present if d.lower() not in static_override_norm]
    
    log_progress(f"\nDataset classification:")
    log_progress(f"  Static datasets (constant across years): {', '.join(static_datasets) if static_datasets else 'none'}")
    log_progress(f"  Yearly datasets (time-varying): {', '.join(yearly_datasets) if yearly_datasets else 'none'}")
    
    log_progress(f"\nDataset handling notes:")
    log_progress(f"  HNTL: Covers 2000-2020, automatically forward-filled with 2020 values for 2021-2024")
    log_progress(f"  GDP: Excluded from feature set")
    
    wandb.config.update({
        "datasets": datasets_present, 
        "num_datasets": len(datasets_present),
        "static_datasets": static_datasets,
        "yearly_datasets": yearly_datasets,
        "num_static": len(static_datasets),
        "num_yearly": len(yearly_datasets)
    })

    # Determine band names using a small probe window to avoid full reads
    # Probe reads directly from source files without any CRS operations or normalization
    log_progress("\nDetermining band structure with direct probe reads (no CRS operations)...")
    
    # Probe static datasets first
    static_band_names: List[str] = []
    static_band_counts: Dict[str, int] = {}
    
    log_progress("  Probing static datasets...")
    for dataset in static_datasets:
        try:
            entries = catalog[dataset]
            # Prefer truly static files (no year in filename); otherwise fall back to earliest available year
            if None in entries:
                candidate_paths = entries[None]
            else:
                available_years = sorted([y for y in entries.keys() if y is not None])
                candidate_paths = entries[available_years[0]] if available_years else []
                if not candidate_paths:
                    log_progress(f"    Warning: {dataset} static override but no files found at all")
                    continue
            arrays = []
            for p in candidate_paths:
                try:
                    # Read directly without any CRS operations or normalization
                    with rasterio.open(p) as src:
                        # Use a small window (8x8 or smaller if file is smaller)
                        probe_h = min(8, src.height)
                        probe_w = min(8, src.width)
                        direct_window = Window(col_off=0, row_off=0, width=probe_w, height=probe_h)
                        
                        # Read window directly (no CRS operations, no normalization)
                        data = src.read(window=direct_window, masked=False).astype(np.float32)
                        
                        # Handle nodata values
                        if src.nodata is not None:
                            data[data == src.nodata] = np.nan
                        
                        # Auto-unscale Int16 data if needed
                        needs_unscaling = (
                            dataset is not None and 
                            dataset in INT16_SCALED_DATASETS and
                            src.dtypes[0] in ('int16', 'Int16')
                        )
                        if needs_unscaling:
                            data = data / 100.0
                        
                        arr = data
                    arrays.append(arr)
                except Exception as e:
                    log_progress(f"    Warning: Failed to read {p.name}: {e}")
                    continue
            
            if not arrays:
                log_progress(f"    Warning: {dataset} - all files failed to read, skipping")
                continue
            
            band_names = build_band_names(dataset, arrays)
            if band_names:
                static_band_names.extend(band_names)
                static_band_counts[dataset] = len(band_names)
                log_progress(f"    {dataset}: {len(band_names)} bands (static)")
        except Exception as e:
            log_progress(f"    Warning: Could not probe static dataset {dataset}: {e}")
            continue
    
    # Probe yearly datasets
    yearly_band_names: List[str] = []
    yearly_band_counts: Dict[str, int] = {}
    
    log_progress("  Probing yearly datasets...")
    for dataset in yearly_datasets:
        try:
            entries = catalog[dataset]
            candidate_paths: List[Path] = []
            
            # Find ANY available year for this dataset
            available_years = sorted([y for y in entries.keys() if y is not None])
            if available_years:
                candidate_paths = entries[available_years[0]]
            
            if not candidate_paths:
                log_progress(f"    Skipping {dataset}: no data files found")
                continue
            
            arrays = []
            for p in candidate_paths:
                try:
                    # Read directly without any CRS operations or normalization
                    with rasterio.open(p) as src:
                        # Use a small window (8x8 or smaller if file is smaller)
                        probe_h = min(8, src.height)
                        probe_w = min(8, src.width)
                        direct_window = Window(col_off=0, row_off=0, width=probe_w, height=probe_h)
                        
                        # Read window directly (no CRS operations, no normalization)
                        data = src.read(window=direct_window, masked=False).astype(np.float32)
                        
                        # Handle nodata values
                        if src.nodata is not None:
                            data[data == src.nodata] = np.nan
                        
                        # Auto-unscale Int16 data if needed
                        needs_unscaling = (
                            dataset is not None and 
                            dataset in INT16_SCALED_DATASETS and
                            src.dtypes[0] in ('int16', 'Int16')
                        )
                        if needs_unscaling:
                            data = data / 100.0
                        
                        arr = data
                    arrays.append(arr)
                except Exception as e:
                    log_progress(f"    Warning: Failed to read {p.name}: {e}")
                    continue
            
            if not arrays:
                log_progress(f"    Warning: {dataset} - all files failed to read, skipping")
                continue
            
            band_names = build_band_names(dataset, arrays)
            if band_names:
                yearly_band_names.extend(band_names)
                yearly_band_counts[dataset] = len(band_names)
                log_progress(f"    {dataset}: {len(band_names)} bands (yearly)")
        except Exception as e:
            log_progress(f"    Warning: Could not probe yearly dataset {dataset}: {e}")
            continue
    
    # Combine band names: static first, then yearly (ensures consistent column order)
    all_band_names = static_band_names + yearly_band_names
    dataset_band_counts = {**static_band_counts, **yearly_band_counts}
    
    if not all_band_names:
        print("\n" + "="*80)
        print("ERROR: Failed to determine band structure - no datasets could be read.")
        print("="*80)
        print("\nDebugging information:")
        print(f"  Static datasets attempted: {static_datasets}")
        print(f"  Yearly datasets attempted: {yearly_datasets}")
        print(f"  Data root: {READY_ROOT}")
        print("\nPossible causes:")
        print("  1. CRS normalization issues with PROJJSON/EngineeringCRS")
        print("  2. All raster files are corrupted or unreadable")
        print("  3. Incompatible rasterio version")
        print("\nPlease check the error messages above for specific file failures.")
        print("="*80)
        sys.exit(1)
    
    log_progress(f"\nBand structure summary:")
    log_progress(f"  Static bands: {len(static_band_names)}")
    log_progress(f"  Yearly bands: {len(yearly_band_names)}")
    log_progress(f"  Total bands: {len(all_band_names)}")
    log_progress(f"  All band names: {all_band_names}")
    
    wandb.log({
        "processing/total_bands": len(all_band_names),
        "processing/static_bands": len(static_band_names),
        "processing/yearly_bands": len(yearly_band_names)
    })

    # Setup incremental Parquet writer
    log_progress("Setting up incremental Parquet writer...")
    parquet_writer = None
    total_rows = 0
    
    # Process window-by-window, with all years for each window
    # Key optimization: Load static datasets ONCE per window and reuse across ALL years
    log_progress("\nProcessing windows with per-window static data caching (major speedup)...")
    
    # Track progress
    total_windows = sum(1 for _ in iter_reference_windows(ref_profile, TILE_SIZE))
    window_idx = 0
    year_rows_dict = {year: 0 for year in YEARS}
    
    current_band_names = all_band_names
    
    # Iterate windows FIRST, then years
    for w in iter_reference_windows(ref_profile, TILE_SIZE):
        window_idx += 1
        if window_idx % 100 == 0:
            log_progress(f"Processing window {window_idx}/{total_windows}...")
        
        window_height = int(w.height)
        window_width = int(w.width)
        
        # Extract mask windows (same for all years)
        sa_backbone_window = sa_backbone_mask[
            int(w.row_off):int(w.row_off + w.height),
            int(w.col_off):int(w.col_off + w.width)
        ]
        colombia_mask_window = colombia_mask[
            int(w.row_off):int(w.row_off + w.height),
            int(w.col_off):int(w.col_off + w.width)
        ]
        
        # Skip windows with no pixels to keep (SA backbone & Colombia mask)
        combined_window = (sa_backbone_window == 1) & (colombia_mask_window == 1)
        if combined_window.sum() == 0:
            continue
        
        # LOAD STATIC DATA ONCE for this window (reused across all years)
        static_window_arrays: List[np.ndarray] = []
        for dataset in static_datasets:
            entries = catalog[dataset]
            # Prefer truly static files; fallback to earliest available year
            if None in entries:
                paths = entries[None]
            else:
                years_avail = sorted([y for y in entries.keys() if y is not None])
                paths = entries[years_avail[0]] if years_avail else []
            expected_bands = static_band_counts.get(dataset, 1)
            arrays = load_dataset_window(paths, ref_profile, w, expected_bands, window_height, window_width, dataset=dataset)
            static_window_arrays.extend(arrays)
        
        # Now process ALL years for this window
        for year in YEARS:
            try:
                # LOAD YEARLY DATA (varies by year, with forward-fill)
                yearly_window_arrays: List[np.ndarray] = []
                for dataset in yearly_datasets:
                    entries = catalog[dataset]
                    
                    # Find paths: current year, forward-fill, or backward-fill
                    if year in entries:
                        paths = entries[year]
                    else:
                        # Forward-fill: use most recent prior year
                        prevs = [y for y in entries.keys() if y is not None and y < year]
                        if prevs:
                            paths = entries[max(prevs)]
                        else:
                            # Backward-fill: use earliest available year
                            nexts = [y for y in entries.keys() if y is not None and y > year]
                            paths = entries[min(nexts)] if nexts else []
                    
                    expected_bands = yearly_band_counts.get(dataset, 1)
                    arrays = load_dataset_window(paths, ref_profile, w, expected_bands, window_height, window_width, dataset=dataset)
                    yearly_window_arrays.extend(arrays)
                
                # COMBINE: Static arrays first, then yearly arrays
                all_window_arrays = static_window_arrays + yearly_window_arrays
                
                if not all_window_arrays:
                    continue
                
                # Stack into single band array
                band_stack = np.concatenate([a if a.ndim == 3 else a[np.newaxis, ...] for a in all_window_arrays], axis=0)
                
                # Safety check: ensure band count matches expected
                if band_stack.shape[0] != len(current_band_names):
                    error_msg = (f"Band count mismatch for year {year}, window {w}: "
                               f"expected {len(current_band_names)} bands, got {band_stack.shape[0]}")
                    log_progress(f"ERROR: {error_msg}")
                    raise ValueError(error_msg)
                
                # Apply temporal coverage rules (in-place modification)
                apply_temporal_coverage_rules(band_stack, current_band_names, year)
                
                # Parquet streaming with combined mask (SA backbone & Colombia mask)
                table = window_to_arrow(year, w, band_stack, current_band_names, ref_profile, sa_backbone_window, colombia_mask_window)
                if parquet_writer is None:
                    log_progress("  Initializing Parquet writer...")
                    parquet_writer = pq.ParquetWriter(PARQUET_PATH, table.schema, compression='zstd')
                if table.num_rows > 0:
                    parquet_writer.write_table(table)
                    year_rows_dict[year] += table.num_rows
                
                # Clean up yearly data only (keep static data for next year)
                del band_stack, yearly_window_arrays, all_window_arrays, table
                
            except Exception as e:
                log_progress(f"ERROR processing year {year}, window {window_idx}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # Clean up static data and mask windows after processing all years
        del static_window_arrays, sa_backbone_window, colombia_mask_window
        # Only gc.collect() periodically to reduce overhead
        if window_idx % 50 == 0:
            gc.collect()
    
    # Log results per year
    for year in YEARS:
        year_rows = year_rows_dict[year]
        total_rows += year_rows
        wandb.log({
            f"year/{year}/rows": year_rows,
            f"year/{year}/bands": len(current_band_names),
            f"year/{year}/status": "success"
        })
        log_progress(f"Year {year}: {year_rows:,} rows")

    # Close Parquet writer
    if parquet_writer is not None:
        log_progress("Closing Parquet writer...")
        parquet_writer.close()
        
        if PARQUET_PATH.exists():
            parquet_size_mb = PARQUET_PATH.stat().st_size / (1024 * 1024)
            log_progress(f"Wrote Parquet panel: {PARQUET_PATH}")
            log_progress(f"Panel size: {parquet_size_mb:.1f} MB")
            log_progress(f"Total rows: {total_rows:,}")
            
            wandb.log({
                "output/parquet_rows": total_rows,
                "output/parquet_columns": len(all_band_names) + 5,  # bands + year + x + y + row + col
                "output/parquet_size_mb": parquet_size_mb,
                "output/years_processed": len(YEARS)
            })
        else:
            log_progress("ERROR: Parquet file was not created")
            wandb.log({"output/status": "failed", "output/error": "file_not_created"})
    else:
        log_progress("ERROR: No data to write to Parquet")
        wandb.log({"output/status": "failed", "output/error": "no_data"})

    total_time = time.time() - total_start_time
    log_progress(f"Merge process completed in {total_time/60:.1f} minutes")
    
    wandb.log({
        "summary/total_time_minutes": total_time / 60,
        "summary/total_time_hours": total_time / 3600,
        "summary/status": "completed"
    })
    
    wandb.finish()


if __name__ == "__main__":
    main()
