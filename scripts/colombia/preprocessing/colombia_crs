#!/usr/bin/env python3
"""
Fix CRS for all Colombia rasters by warping to backbone grid (EPSG:3857).

This preprocessing step rewrites all input GeoTIFFs to match the backbone exactly:
- CRS: EPSG:3857 (true EPSG, not EngineeringCRS)
- Transform: Exact match to backbone transform
- Shape: Exact match to backbone shape
- Nodata: Preserved from source

This eliminates the need for complex CRS handling in the merge script.

Inputs:
- Source rasters in data/ready/<dataset>/ (or $SCRATCH/data/ready/<dataset>/ on cluster)
- Backbone: data/ready/backbone/colombia_GEE.tif

Outputs:
- Fixed rasters in data/ready/fixed_crs/<dataset>/ (mirrors source structure)
- Fixed backbone: data/ready/fixed_crs/backbone/colombia_GEE.tif

Usage:
    ./fix_crs_to_backbone

On Euler, ensure conda-forge GDAL/PROJ is available:
    conda install -c conda-forge gdal proj
"""

import sys
import os
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import re

import numpy as np
import rasterio
from rasterio.enums import Resampling
from rasterio.warp import reproject, calculate_default_transform
from rasterio.crs import CRS

# Determine base path
SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent.parent.parent.parent

# Use $SCRATCH for data if on cluster, otherwise use project root
if "SCRATCH" in os.environ:
    DATA_ROOT = Path(os.environ["SCRATCH"]) / "data"
    READY_ROOT = DATA_ROOT / "ready"
else:
    DATA_ROOT = PROJECT_ROOT / "data"
    READY_ROOT = DATA_ROOT / "ready"

# Output directory for fixed CRS rasters
FIXED_CRS_ROOT = READY_ROOT / "fixed_crs"

# Datasets to process (exclude backbone, gdp, assetlevel, DynamicWorld, DW)
INCLUDE_DATASETS = [
    "WDPA", "HNTL", "NDVI", "GPW",
    "deforestation", "landcover", "WorldClim",
    "elevation", "slope", "oil_gas", "road_infrastructure",
    "GSN", "powerplants", "wildfire",
]


def log_progress(message: str, start_time: Optional[float] = None):
    """Log progress with optional timing."""
    if start_time:
        elapsed = time.time() - start_time
        print(f"[time] {message} ({elapsed:.1f}s)")
    else:
        print(f"[info] {message}")


def load_backbone_template() -> Tuple[rasterio.DatasetReader, dict]:
    """
    Load backbone raster to get reference grid (CRS, transform, shape).
    
    Returns:
        (backbone_dataset, reference_profile)
    """
    log_progress("Loading backbone template...")
    
    backbone_paths = [
        READY_ROOT / "backbone" / "colombia_GEE.tif",
        DATA_ROOT / "backbone" / "colombia_GEE.tif",
    ]
    
    backbone_path = None
    for p in backbone_paths:
        if p.exists():
            backbone_path = p
            break
    
    if backbone_path is None:
        print("ERROR: colombia_GEE.tif not found.")
        print("\nChecked locations:")
        for p in backbone_paths:
            print(f"   - {p} (exists: {p.exists()})")
        sys.exit(1)
    
    backbone_src = rasterio.open(backbone_path)
    
    # Create reference profile
    ref_profile = {
        "crs": CRS.from_epsg(3857),  # Force true EPSG:3857
        "transform": backbone_src.transform,
        "width": backbone_src.width,
        "height": backbone_src.height,
        "dtype": rasterio.float32,
        "nodata": np.nan,
        "compress": "lzw",
    }
    
    log_progress(f"Backbone template: {backbone_src.width}Ã—{backbone_src.height}, CRS: EPSG:3857")
    log_progress(f"  Transform: {backbone_src.transform}")
    
    return backbone_src, ref_profile


def discover_raster_files() -> Dict[str, Dict[Optional[int], List[Path]]]:
    """
    Scan READY_ROOT and build catalog: {dataset: {year(or None): [paths]}}
    """
    log_progress("Scanning for raster files...")
    catalog: Dict[str, Dict[Optional[int], List[Path]]] = {}
    year_re = re.compile(r"(19|20)\d{2}")
    
    if not READY_ROOT.exists():
        print(f"ERROR: Data directory not found: {READY_ROOT}")
        sys.exit(1)
    
    total_files = 0
    for subdir in sorted([p for p in READY_ROOT.iterdir() if p.is_dir()]):
        dataset = subdir.name
        
        # Skip if not in include list
        if dataset not in INCLUDE_DATASETS and dataset != "backbone":
            continue
        
        catalog.setdefault(dataset, {})
        files_in_dataset = 0
        
        for tif in sorted(subdir.rglob("*.tif")):
            m = year_re.search(tif.name)
            year = int(m.group(0)) if m else None
            catalog[dataset].setdefault(year, []).append(tif)
            files_in_dataset += 1
            total_files += 1
        
        if files_in_dataset > 0:
            log_progress(f"  {dataset}: {files_in_dataset} files")
    
    log_progress(f"Total files found: {total_files}")
    return catalog


def warp_raster_to_backbone(
    src_path: Path,
    dst_path: Path,
    ref_profile: dict,
    dataset: str
) -> bool:
    """
    Warp a single raster to match backbone exactly (EPSG:3857, same transform/shape).
    
    Returns:
        True if successful, False otherwise
    """
    try:
        with rasterio.open(src_path) as src:
            # Check if already matches (fast path)
            if (src.crs == ref_profile["crs"] and
                src.width == ref_profile["width"] and
                src.height == ref_profile["height"] and
                np.allclose(src.transform, ref_profile["transform"], atol=1e-6)):
                log_progress(f"    Already aligned: {src_path.name}")
                # Copy file directly if identical
                import shutil
                dst_path.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(src_path, dst_path)
                return True
            
            # Need to warp
            count = src.count
            src_nodata = src.nodata
            
            # Create destination array
            dst_data = np.full(
                (count, ref_profile["height"], ref_profile["width"]),
                ref_profile["nodata"],
                dtype=ref_profile["dtype"]
            )
            
            # Determine source CRS (force to EPSG:3857 if it's Pseudo-Mercator)
            src_crs = src.crs
            if src_crs is not None:
                try:
                    # Check if it's Pseudo-Mercator variant
                    crs_str = str(src_crs).lower()
                    crs_name = str(src_crs.name).lower() if hasattr(src_crs, 'name') and src_crs.name else ""
                    
                    if ("pseudo" in crs_name or "pseudo" in crs_str or
                        "3857" in str(src_crs) or
                        (hasattr(src_crs, '__class__') and 
                         src_crs.__class__.__name__ == "EngineeringCRS" and
                         "wgs 84" in crs_name)):
                        src_crs = CRS.from_epsg(3857)
                except:
                    pass
            
            # If source CRS is None or invalid, assume EPSG:3857
            if src_crs is None:
                src_crs = CRS.from_epsg(3857)
            
            # Reproject each band
            for b in range(1, count + 1):
                reproject(
                    source=rasterio.band(src, b),
                    destination=dst_data[b - 1],
                    src_transform=src.transform,
                    src_crs=src_crs,
                    src_nodata=src_nodata,
                    dst_transform=ref_profile["transform"],
                    dst_crs=ref_profile["crs"],
                    resampling=Resampling.nearest,
                    dst_nodata=ref_profile["nodata"],
                )
            
            # Write output
            dst_path.parent.mkdir(parents=True, exist_ok=True)
            with rasterio.open(
                dst_path,
                'w',
                driver='GTiff',
                count=count,
                width=ref_profile["width"],
                height=ref_profile["height"],
                dtype=ref_profile["dtype"],
                crs=ref_profile["crs"],
                transform=ref_profile["transform"],
                nodata=ref_profile["nodata"],
                compress=ref_profile["compress"],
            ) as dst:
                dst.write(dst_data)
            
            return True
            
    except Exception as e:
        log_progress(f"    ERROR warping {src_path.name}: {e}")
        return False


def process_backbone(backbone_src: rasterio.DatasetReader, ref_profile: dict) -> bool:
    """Process backbone file itself (ensure it's true EPSG:3857)."""
    log_progress("\nProcessing backbone...")
    
    backbone_path = Path(backbone_src.name)
    dst_path = FIXED_CRS_ROOT / "backbone" / backbone_path.name
    
    # Check if already correct CRS
    if (backbone_src.crs == ref_profile["crs"] and
        backbone_src.width == ref_profile["width"] and
        backbone_src.height == ref_profile["height"] and
        np.allclose(backbone_src.transform, ref_profile["transform"], atol=1e-6)):
        log_progress("  Backbone already in correct format, copying...")
        import shutil
        dst_path.parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(backbone_path, dst_path)
        return True
    
    # Warp backbone
    return warp_raster_to_backbone(backbone_path, dst_path, ref_profile, "backbone")


def main():
    """Main processing function."""
    total_start = time.time()
    log_progress("="*60)
    log_progress("Colombia CRS Fix Preprocessing")
    log_progress("="*60)
    log_progress(f"Source: {READY_ROOT}")
    log_progress(f"Output: {FIXED_CRS_ROOT}")
    
    # Load backbone template
    backbone_src, ref_profile = load_backbone_template()
    
    # Process backbone first
    process_backbone(backbone_src, ref_profile)
    backbone_src.close()
    
    # Discover all raster files
    catalog = discover_raster_files()
    
    # Process each dataset
    log_progress("\nProcessing datasets...")
    total_processed = 0
    total_failed = 0
    
    for dataset in sorted(catalog.keys()):
        if dataset == "backbone":
            continue  # Already processed
        
        if dataset not in INCLUDE_DATASETS:
            continue
        
        log_progress(f"\n{dataset}:")
        entries = catalog[dataset]
        
        # Process static files (year=None)
        if None in entries:
            for src_path in entries[None]:
                dst_path = FIXED_CRS_ROOT / dataset / src_path.name
                if dst_path.exists():
                    log_progress(f"  Skipping (exists): {src_path.name}")
                    total_processed += 1
                    continue
                
                if warp_raster_to_backbone(src_path, dst_path, ref_profile, dataset):
                    total_processed += 1
                else:
                    total_failed += 1
        
        # Process yearly files
        for year in sorted([y for y in entries.keys() if y is not None]):
            for src_path in entries[year]:
                dst_path = FIXED_CRS_ROOT / dataset / src_path.name
                if dst_path.exists():
                    log_progress(f"  Skipping (exists): {src_path.name}")
                    total_processed += 1
                    continue
                
                if warp_raster_to_backbone(src_path, dst_path, ref_profile, dataset):
                    total_processed += 1
                else:
                    total_failed += 1
    
    # Summary
    total_time = time.time() - total_start
    log_progress("\n" + "="*60)
    log_progress("PREPROCESSING COMPLETE")
    log_progress("="*60)
    log_progress(f"Processed: {total_processed} files")
    if total_failed > 0:
        log_progress(f"Failed: {total_failed} files")
    log_progress(f"Time: {total_time/60:.1f} minutes")
    log_progress(f"\nFixed rasters available at: {FIXED_CRS_ROOT}")
    log_progress("\nNext step: Run colombia_merge (it will automatically use fixed_crs/)")


if __name__ == "__main__":
    main()

