#!/usr/bin/python3
"""
Merge South America 1 km rasters into a yearly panel with streaming I/O and low memory use.

Summary:
- Reads per-dataset rasters from ``data/ready`` (or ``$SCRATCH/data``) and aligns them to the backbone grid.
- Processes one year at a time, reusing earlier data when a dataset is missing for a given year.
- Masks out all pixels where backbone = 0 (outside South America) to reduce output size.
- Streams window batches to yearly GeoTIFFs and a single Parquet panel while cleaning up memory eagerly.

Inputs:
- Year-specific raster folders under ``data/ready/<dataset>`` with filenames containing the year.
- Static rasters (no year in filename) read once and injected into every year.
- backbone.tif (binary mask) that defines the reference grid and South America extent.

Static vs. Yearly Datasets:
Static datasets (no year in filename, constant across all years):
  - elevation: Digital elevation model (terrain height)
  - slope: Terrain slope derived from elevation
  - oil_gas: Oil and gas infrastructure locations (fixed reference year)
  - road_infrastructure: Road network (fixed reference year)
  - GSN: Grid system network (fixed reference dataset)
  - powerplants: Power plant locations and capacity (fixed reference year)
  - WorldClim: Climate variables (static bioclimatic averages)
  
  Handling: Static files are read ONCE per window and reused for ALL years without 
  forward-filling. They appear as identical values in every output year, ensuring
  no artificial NA inflation. Static datasets are loaded first in each window,
  then combined with yearly data.

Yearly datasets (year in filename, time-varying):
  - WDPA: Protected areas (yearly updates)
  - VIIRS: Nighttime lights (yearly)
  - NDVI: Vegetation index (yearly)
  - GPW: Population density (periodic updates, e.g., 2015, 2020)
  - gdp: GDP estimates (yearly or periodic)
  - deforestation: Forest loss (yearly)
  - landcover: Land cover classification (yearly)
  - wildfire: Wildfire occurrence and burned area (yearly)
  
  Handling: If a dataset has no file for a target year, the script performs FORWARD-FILL,
  reusing the most recent prior year to maintain panel completeness. If a year precedes
  all available data, BACKWARD-FILL is used (reusing the earliest available year).

Outputs:
- GeoTIFF per year: ``outputs/Results/merged_tifs/merged_SA_1km_<year>.tif`` (or ``$SCRATCH/outputs/...``).
- Parquet panel: ``outputs/Results/merged_panel_<start>_<end>.parquet`` storing coordinates plus all bands (backbone=1 only).

Key choices:
- Portable path resolution from ``Path(__file__)`` with automatic ``$SCRATCH`` override for HPC runs.
- Backbone-based masking: only pixels with backbone=1 are included in the Parquet output (GeoTIFFs remain full extent).
- Dynamic year detection: automatically discovers all available years from the data (no hardcoded range).
- Automatic dataset classification: separates static (time-invariant) from yearly (time-varying) datasets.
- Static datasets are loaded once per window and reused across all years, eliminating artificial NA inflation.
- Forward-fill for yearly datasets to maintain complete panel and avoid gaps in derived indicators.
- Vectorized coordinate generation and Arrow streaming to avoid pandas and large in-memory tables.
- Explicit cleanup (`del`, `gc.collect`) after each window and year to keep RAM usage bounded.

Short overview of what the script does:
1) Initialize Weights & Biases and resolve paths for data and outputs.
2) Open backbone.tif to define the reference grid (shape, CRS, transform) and load the binary mask.
3) Discover all candidate rasters in data/ready and group them by dataset and year.
4) Automatically determine the year range from available data (all years with at least one dataset).
5) Classify datasets into static (time-invariant) and yearly (time-varying) categories.
6) Determine the final band list by probing static and yearly datasets separately.
7) For each year in the detected range:
   - For each window:
     a) Load static datasets ONCE (these remain constant across all years).
     b) Load yearly datasets for the current year (with forward/backward-fill fallback).
     c) Combine static + yearly arrays in consistent order.
   - Reproject on-the-fly to match the reference grid if needed.
   - Concatenate dataset arrays into a single band stack for the year.
   - Write a per-year GeoTIFF and convert the stack to a long Arrow table (vectorized coords).
   - Apply backbone mask to exclude pixels outside South America before writing to Parquet.
   - Append the year's rows to a single Parquet file using a persistent writer.
   - Immediately free memory for arrays and Arrow tables.
"""

import re
import sys
import time
import os
import gc
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import numpy as np
import rasterio
from rasterio.enums import Resampling
from rasterio.warp import reproject
from rasterio.windows import Window
import pyarrow as pa
import pyarrow.parquet as pq
import wandb

# Determine base path - use current script location for portability
SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent.parent

# Use $SCRATCH for data if on cluster, otherwise use project root
if "SCRATCH" in os.environ:
    DATA_ROOT = Path(os.environ["SCRATCH"]) / "data"
    READY_ROOT = DATA_ROOT / "ready"
else:
    DATA_ROOT = PROJECT_ROOT / "data"
    READY_ROOT = DATA_ROOT / "ready"

# Use $SCRATCH if on cluster, otherwise use local outputs
if "SCRATCH" in os.environ:
    OUTPUT_ROOT = Path(os.environ["SCRATCH"]) / "outputs" / "Results"
else:
    OUTPUT_ROOT = PROJECT_ROOT / "outputs" / "Results"

OUTPUT_TIFF_DIR = OUTPUT_ROOT / "merged_tifs"
OUTPUT_TIFF_DIR.mkdir(parents=True, exist_ok=True)

# YEARS will be determined dynamically from available data
TILE_SIZE = 512


def log_progress(message: str, start_time: Optional[float] = None):
    """Log progress with optional timing."""
    if start_time:
        elapsed = time.time() - start_time
        print(f"[time] {message} ({elapsed:.1f}s)")
    else:
        print(f"[info] {message}")


def open_reference_backbone() -> Tuple[Path, rasterio.DatasetReader, np.ndarray]:
    """Open backbone.tif as reference grid and return path, dataset, and mask array."""
    log_progress("Loading backbone reference grid...")
    
    # Search for backbone.tif in ready/backbone
    backbone_paths = [
        READY_ROOT / "backbone" / "backbone.tif",
        DATA_ROOT / "ready" / "backbone" / "backbone.tif",
    ]
    
    backbone_path = None
    for p in backbone_paths:
        if p.exists():
            backbone_path = p
            break
    
    if backbone_path is None:
        print("ERROR: backbone.tif not found.")
        print("\nChecked locations:")
        for p in backbone_paths:
            print(f"   - {p} (exists: {p.exists()})")
        print("\nPlease ensure backbone.tif exists in data/ready/backbone/")
        sys.exit(1)
    
    src = rasterio.open(backbone_path)
    # Read the backbone mask (1 = South America, 0 = outside)
    backbone_mask = src.read(1)
    
    land_pixels = int((backbone_mask == 1).sum())
    total_pixels = backbone_mask.size
    
    log_progress(f"Using backbone {backbone_path.name} as reference grid")
    log_progress(f"Reference grid: {src.shape} pixels, CRS: {src.crs}")
    log_progress(f"South America pixels: {land_pixels:,} ({100*land_pixels/total_pixels:.1f}%)")
    log_progress(f"Masked pixels (outside SA): {total_pixels - land_pixels:,}")
    
    return backbone_path, src, backbone_mask


def discover_raster_catalog() -> Dict[str, Dict[Optional[int], List[Path]]]:
    """
    Scan READY_ROOT subfolders and build a catalog:
    {dataset_name: {year(or None for static): [paths]}}
    """
    log_progress("Building raster catalog...")
    catalog: Dict[str, Dict[Optional[int], List[Path]]] = {}
    year_re = re.compile(r"(19|20)\d{2}")
    if not READY_ROOT.exists():
        print(f"ERROR: Missing directory: {READY_ROOT}")
        sys.exit(1)
    
    total_files = 0
    for subdir in sorted([p for p in READY_ROOT.iterdir() if p.is_dir()]):
        dataset = subdir.name
        catalog.setdefault(dataset, {})
        files_in_dataset = 0
        for tif in sorted(subdir.rglob("*.tif")):
            m = year_re.search(tif.name)
            year = int(m.group(0)) if m else None
            catalog[dataset].setdefault(year, []).append(tif)
            files_in_dataset += 1
            total_files += 1
        if files_in_dataset > 0:
            log_progress(f"  {dataset}: {files_in_dataset} files")
    
    log_progress(f"Total files catalogued: {total_files}")
    return catalog


def detect_year_range(catalog: Dict[str, Dict[Optional[int], List[Path]]]) -> List[int]:
    """
    Automatically detect the year range from available data.
    Returns a sorted list of all years that have at least one dataset.
    """
    all_years = set()
    for dataset, entries in catalog.items():
        for year in entries.keys():
            if year is not None:  # Exclude None (static files)
                all_years.add(year)
    
    if not all_years:
        print("ERROR: No yearly data found in catalog")
        sys.exit(1)
    
    years = sorted(all_years)
    log_progress(f"Detected year range: {min(years)} - {max(years)} ({len(years)} years)")
    log_progress(f"Years: {years}")
    return years


def check_alignment(src_path: Path, ref_profile: dict) -> bool:
    """Check if a raster is already aligned with the reference grid."""
    try:
        with rasterio.open(src_path) as src:
            return (
                src.shape == (ref_profile["height"], ref_profile["width"]) and
                src.crs == ref_profile["crs"] and
                np.allclose(src.transform, ref_profile["transform"], atol=1e-6)
            )
    except Exception:
        return False


def read_reproject_to_ref(path: Path, ref_profile: dict) -> np.ndarray:
    """Read a raster and reproject/resample to the reference profile, returning a float32 array."""
    log_progress(f"  Processing {path.name}...")
    start_time = time.time()
    
    # Check if already aligned
    if check_alignment(path, ref_profile):
        log_progress(f"    Already aligned, reading directly...")
        with rasterio.open(path) as src:
            data = src.read().astype(np.float32)
            # Handle nodata values
            if src.nodata is not None:
                data[data == src.nodata] = np.nan
            log_progress(f"    Read {path.name} ({time.time() - start_time:.1f}s)")
            return data
    
    log_progress(f"    Reprojecting {path.name}...")
    with rasterio.open(path) as src:
        dst_height = ref_profile["height"]
        dst_width = ref_profile["width"]
        dst_transform = ref_profile["transform"]
        dst_crs = ref_profile["crs"]
        count = src.count
        dst = np.full((count, dst_height, dst_width), np.nan, dtype=np.float32)
        
        for b in range(1, count + 1):
            reproject(
                source=rasterio.band(src, b),
                destination=dst[b - 1],
                src_transform=src.transform,
                src_crs=src.crs,
                dst_transform=dst_transform,
                dst_crs=dst_crs,
                resampling=Resampling.nearest,
                dst_nodata=np.nan,
            )
        
        log_progress(f"    Reprojected {path.name} ({time.time() - start_time:.1f}s)")
        return dst if count > 1 else dst[0]


def iter_reference_windows(ref_profile: dict, tile_size: int = TILE_SIZE):
    """Yield rasterio Window objects covering the reference grid in tiles."""
    height = ref_profile["height"]
    width = ref_profile["width"]
    for row_off in range(0, height, tile_size):
        for col_off in range(0, width, tile_size):
            h = min(tile_size, height - row_off)
            w = min(tile_size, width - col_off)
            yield Window(col_off=col_off, row_off=row_off, width=w, height=h)


def read_reproject_window(src_path: Path, ref_profile: dict, window: Window) -> np.ndarray:
    """
    Read a source raster and reproject only the given reference window to a float32 array (B, h, w).
    """
    with rasterio.open(src_path) as src:
        dst_height = int(window.height)
        dst_width = int(window.width)
        dst_transform = rasterio.windows.transform(window, ref_profile["transform"])
        dst_crs = ref_profile["crs"]
        count = src.count
        dst = np.full((count, dst_height, dst_width), np.nan, dtype=np.float32)
        for b in range(1, count + 1):
            reproject(
                source=rasterio.band(src, b),
                destination=dst[b - 1],
                src_transform=src.transform,
                src_crs=src.crs,
                dst_transform=dst_transform,
                dst_crs=dst_crs,
                resampling=Resampling.nearest,
                dst_nodata=np.nan,
            )
        return dst if count > 1 else dst[0][np.newaxis, ...]


def open_year_tif_writer(year: int, band_count: int, band_names: List[str], ref_profile: dict):
    """Open a GeoTIFF for the given year for windowed writing; returns (dst, out_path)."""
    profile = ref_profile.copy()
    profile.update({
        "count": band_count,
        "dtype": rasterio.float32,
        "nodata": np.nan,
        "compress": "lzw",
        "tiled": True,
        "blockxsize": 512,
        "blockysize": 512,
    })
    out_path = OUTPUT_TIFF_DIR / f"merged_SA_1km_{year}.tif"
    dst = rasterio.open(out_path, "w", **profile)
    for i, name in enumerate(band_names):
        try:
            dst.set_band_description(i + 1, name)
        except Exception:
            pass
    return dst, out_path


def window_to_arrow(year: int, window: Window, band_stack: np.ndarray, band_names: List[str], 
                    ref_profile: dict, backbone_window: np.ndarray) -> pa.Table:
    """Create an Arrow table for a window, filtering by backbone mask (only backbone=1 pixels)."""
    h = int(window.height)
    w = int(window.width)
    rows = (np.arange(h, dtype=np.int32) + int(window.row_off)).repeat(w)
    cols = np.tile(np.arange(w, dtype=np.int32) + int(window.col_off), h)
    transform = ref_profile["transform"]
    xs = (transform.a * cols + transform.b * rows + transform.c).astype(np.float64)
    ys = (transform.d * cols + transform.e * rows + transform.f).astype(np.float64)

    # Flatten bands
    flat_bands = [band_stack[i].reshape(-1).astype(np.float32) for i in range(band_stack.shape[0])]

    # Apply backbone mask: only keep pixels where backbone == 1
    backbone_flat = backbone_window.reshape(-1)
    keep = backbone_flat == 1

    # Skip empty windows (no land pixels)
    if keep.sum() == 0:
        return pa.table({
            "year": pa.array([], type=pa.int16()),
            "x": pa.array([], type=pa.float64()),
            "y": pa.array([], type=pa.float64()),
            "row": pa.array([], type=pa.int32()),
            "col": pa.array([], type=pa.int32()),
            **{name: pa.array([], type=pa.float32()) for name in band_names}
        })

    cols_dict = {
        "year": pa.array(np.full(keep.sum(), year, dtype=np.int16)),
        "x": pa.array(xs[keep]),
        "y": pa.array(ys[keep]),
        "row": pa.array(rows[keep]),
        "col": pa.array(cols[keep]),
    }
    for name, fb in zip(band_names, flat_bands):
        cols_dict[name] = pa.array(fb[keep])
    return pa.table(cols_dict)


def build_band_names(dataset: str, arrays: List[np.ndarray]) -> List[str]:
    """Build band names for a dataset given contributing arrays."""
    names: List[str] = []
    idx = 1
    for arr in arrays:
        if arr.ndim == 2:
            names.append(f"{dataset}")
        else:
            for i in range(arr.shape[0]):
                names.append(f"{dataset}_b{idx}")
                idx += 1
    
    # If all were single-band and duplicates resulted, ensure unique suffixes
    if len(names) != len(set(names)):
        names = [f"{dataset}_{i+1}" for i in range(len(names))]
    return names


def classify_datasets(catalog: Dict[str, Dict[Optional[int], List[Path]]]) -> Tuple[List[str], List[str]]:
    """
    Classify datasets as static (time-invariant) or yearly (time-varying).
    
    Static datasets have no year in filename (key = None in catalog).
    Yearly datasets have years in filenames (key = int in catalog).
    
    Returns: (static_datasets, yearly_datasets)
    """
    static = []
    yearly = []
    
    for dataset, entries in catalog.items():
        has_static = None in entries
        has_yearly = any(y is not None for y in entries.keys())
        
        if has_static and not has_yearly:
            # Only static files, no yearly variation
            static.append(dataset)
        elif has_yearly:
            # Has yearly files (may also have some static files, but treat as yearly)
            yearly.append(dataset)
        # Skip datasets with neither (shouldn't happen)
    
    return sorted(static), sorted(yearly)


def load_dataset_window(
    paths: List[Path],
    ref_profile: dict,
    window: Window,
    expected_bands: int,
    window_height: int,
    window_width: int
) -> List[np.ndarray]:
    """
    Load and reproject dataset files for a given window.
    Returns NaN placeholder if no paths provided.
    """
    if not paths:
        # No data available - create NaN placeholder
        placeholder = np.full((expected_bands, window_height, window_width), np.nan, dtype=np.float32)
        return [placeholder]
    
    arrays = []
    for p in paths:
        arr = read_reproject_window(p, ref_profile, window)
        arrays.append(arr)
    
    # If loading failed, create placeholder
    if not arrays:
        placeholder = np.full((expected_bands, window_height, window_width), np.nan, dtype=np.float32)
        return [placeholder]
    
    return arrays


# Removed pandas DataFrame conversions in favor of direct Arrow streaming per window
# Removed global DataFrame creation; coordinates and bands are emitted per window
# Removed process_dataset_for_year and process_dataset functions (unused legacy code)


def main():
    """Main processing function with progress monitoring."""
    # Initialize wandb FIRST before any processing
    print("ðŸ”„ Initializing Weights & Biases connection...")
    
    # Get credentials from environment variables
    wandb_api_key = os.environ.get("WANDB_API_KEY")
    wandb_entity = os.environ.get("WANDB_ENTITY")
    
    if not wandb_api_key:
        print("Warning: WANDB_API_KEY not found in environment variables")
        print("   Set it with: export WANDB_API_KEY='your-key-here'")
    
    if not wandb_entity:
        print("Warning: WANDB_ENTITY not found in environment variables")
        print("   Set it with: export WANDB_ENTITY='your-entity-here'")
    
    total_start_time = time.time()
    log_progress("Starting optimized merge process...")
    
    # Setup reference using backbone
    ref_path, ref_src, backbone_mask = open_reference_backbone()
    ref_profile = ref_src.profile.copy()
    ref_profile.update({
        "count": 1,
        "dtype": rasterio.float32,
        "nodata": np.nan,
        "compress": "lzw",
    })
    
    # Calculate land pixel statistics
    land_pixels = int((backbone_mask == 1).sum())
    total_pixels = backbone_mask.size
    
    ref_src.close()  # Close the reference file
    
    # Build catalog and detect year range
    catalog = discover_raster_catalog()
    YEARS = detect_year_range(catalog)
    
    # Dynamic Parquet path based on detected year range
    PARQUET_PATH = OUTPUT_ROOT / f"merged_panel_{min(YEARS)}_{max(YEARS)}.parquet"
    
    # Now initialize wandb with detected years
    wandb.init(
        project="merge",
        entity=wandb_entity,
        save_code=True,
        config={
            "years": YEARS,
            "year_range": f"{min(YEARS)}-{max(YEARS)}",
            "num_years": len(YEARS),
            "data_root": str(DATA_ROOT),
            "output_root": str(OUTPUT_ROOT),
            "scratch_mode": "SCRATCH" in os.environ,
        }
    )
    print("Weights & Biases connected successfully!")
    
    wandb.log({"status": "started", "start_time": total_start_time})
    
    # Log reference grid info
    wandb.log({
        "reference_grid/height": ref_profile["height"],
        "reference_grid/width": ref_profile["width"],
        "reference_grid/total_pixels": total_pixels,
        "reference_grid/land_pixels": land_pixels,
        "reference_grid/land_percentage": 100 * land_pixels / total_pixels,
        "reference_grid/crs": str(ref_profile["crs"]),
    })

    # Identify datasets to include
    # Note: backbone, gdp_preprocessing, gdp_processed, DynamicWorld, DW, and assetlevel are excluded
    include_priority = [
        "WDPA", "VIIRS", "NDVI", "GPW", "gdp",
        "deforestation", "landcover", "WorldClim",
        "elevation", "slope", "oil_gas", "road_infrastructure",
        "GSN", "powerplants", "wildfire",
    ]

    datasets_present = [d for d in catalog.keys() 
                       if d in include_priority or d.lower() in [x.lower() for x in include_priority]]
    datasets_present = sorted(set(datasets_present), 
                             key=lambda d: (0 if d.upper() == "WDPA" else 1, d.lower()))
    
    if not datasets_present:
        print("ERROR: No datasets found under data/ready.")
        sys.exit(1)
    
    log_progress(f"Datasets to process: {', '.join(datasets_present)}")
    
    # Classify datasets into static (time-invariant) and yearly (time-varying)
    all_static, all_yearly = classify_datasets(catalog)
    
    # Filter to only include datasets in our priority list
    static_datasets = [d for d in all_static if d in datasets_present]
    yearly_datasets = [d for d in all_yearly if d in datasets_present]
    
    log_progress(f"\nDataset classification:")
    log_progress(f"  Static datasets (constant across years): {', '.join(static_datasets) if static_datasets else 'none'}")
    log_progress(f"  Yearly datasets (time-varying): {', '.join(yearly_datasets) if yearly_datasets else 'none'}")
    
    wandb.config.update({
        "datasets": datasets_present, 
        "num_datasets": len(datasets_present),
        "static_datasets": static_datasets,
        "yearly_datasets": yearly_datasets,
        "num_static": len(static_datasets),
        "num_yearly": len(yearly_datasets)
    })

    # Determine band names using a small probe window to avoid full reads
    log_progress("\nDetermining band structure with probe window...")
    probe_window = Window(col_off=0, row_off=0, width=min(8, ref_profile["width"]), height=min(8, ref_profile["height"]))
    
    # Probe static datasets first
    static_band_names: List[str] = []
    static_band_counts: Dict[str, int] = {}
    
    log_progress("  Probing static datasets...")
    for dataset in static_datasets:
        try:
            entries = catalog[dataset]
            if None not in entries:
                log_progress(f"    Warning: {dataset} classified as static but has no static files")
                continue
            
            candidate_paths = entries[None]
            arrays = []
            for p in candidate_paths:
                arr = read_reproject_window(p, ref_profile, probe_window)
                arrays.append(arr)
            
            band_names = build_band_names(dataset, arrays)
            if band_names:
                static_band_names.extend(band_names)
                static_band_counts[dataset] = len(band_names)
                log_progress(f"    {dataset}: {len(band_names)} bands (static)")
        except Exception as e:
            log_progress(f"    Warning: Could not probe static dataset {dataset}: {e}")
            continue
    
    # Probe yearly datasets
    yearly_band_names: List[str] = []
    yearly_band_counts: Dict[str, int] = {}
    
    log_progress("  Probing yearly datasets...")
    for dataset in yearly_datasets:
        try:
            entries = catalog[dataset]
            candidate_paths: List[Path] = []
            
            # Find ANY available year for this dataset
            available_years = sorted([y for y in entries.keys() if y is not None])
            if available_years:
                candidate_paths = entries[available_years[0]]
            
            if not candidate_paths:
                log_progress(f"    Skipping {dataset}: no data files found")
                continue
            
            arrays = []
            for p in candidate_paths:
                arr = read_reproject_window(p, ref_profile, probe_window)
                arrays.append(arr)
            
            band_names = build_band_names(dataset, arrays)
            if band_names:
                yearly_band_names.extend(band_names)
                yearly_band_counts[dataset] = len(band_names)
                log_progress(f"    {dataset}: {len(band_names)} bands (yearly)")
        except Exception as e:
            log_progress(f"    Warning: Could not probe yearly dataset {dataset}: {e}")
            continue
    
    # Combine band names: static first, then yearly (ensures consistent column order)
    all_band_names = static_band_names + yearly_band_names
    dataset_band_counts = {**static_band_counts, **yearly_band_counts}
    
    if not all_band_names:
        print("ERROR: Failed to determine band structure.")
        sys.exit(1)
    
    log_progress(f"\nBand structure summary:")
    log_progress(f"  Static bands: {len(static_band_names)}")
    log_progress(f"  Yearly bands: {len(yearly_band_names)}")
    log_progress(f"  Total bands: {len(all_band_names)}")
    log_progress(f"  All band names: {all_band_names}")
    
    wandb.log({
        "processing/total_bands": len(all_band_names),
        "processing/static_bands": len(static_band_names),
        "processing/yearly_bands": len(yearly_band_names)
    })

    # Setup incremental Parquet writer
    log_progress("Setting up incremental Parquet writer...")
    parquet_writer = None
    total_rows = 0
    
    # Process year-by-year using windowed streaming
    # Key change: Load static datasets ONCE per window and reuse across all years
    log_progress("\nProcessing years sequentially (windowed streaming with static data caching)...")
    
    for year_idx, year in enumerate(YEARS):
        log_progress(f"\n{'='*60}")
        log_progress(f"Processing year {year_idx+1}/{len(YEARS)}: {year}")
        log_progress(f"{'='*60}")
        
        try:
            # Prepare year GeoTIFF writer
            current_band_names = all_band_names
            tif_ds, out_tif = open_year_tif_writer(year, len(current_band_names), current_band_names, ref_profile)
            year_rows = 0
            
            # Iterate windows and stream
            for w in iter_reference_windows(ref_profile, TILE_SIZE):
                window_height = int(w.height)
                window_width = int(w.width)
                
                # Extract backbone mask window
                backbone_window = backbone_mask[
                    int(w.row_off):int(w.row_off + w.height),
                    int(w.col_off):int(w.col_off + w.width)
                ]
                
                # LOAD STATIC DATA FIRST (these are identical across all years)
                static_window_arrays: List[np.ndarray] = []
                for dataset in static_datasets:
                    entries = catalog[dataset]
                    paths = entries.get(None, [])
                    expected_bands = static_band_counts.get(dataset, 1)
                    arrays = load_dataset_window(paths, ref_profile, w, expected_bands, window_height, window_width)
                    static_window_arrays.extend(arrays)
                
                # LOAD YEARLY DATA (varies by year, with forward-fill)
                yearly_window_arrays: List[np.ndarray] = []
                for dataset in yearly_datasets:
                    entries = catalog[dataset]
                    
                    # Find paths: current year, forward-fill, or backward-fill
                    if year in entries:
                        paths = entries[year]
                    else:
                        # Forward-fill: use most recent prior year
                        prevs = [y for y in entries.keys() if y is not None and y < year]
                        if prevs:
                            paths = entries[max(prevs)]
                        else:
                            # Backward-fill: use earliest available year
                            nexts = [y for y in entries.keys() if y is not None and y > year]
                            paths = entries[min(nexts)] if nexts else []
                    
                    expected_bands = yearly_band_counts.get(dataset, 1)
                    arrays = load_dataset_window(paths, ref_profile, w, expected_bands, window_height, window_width)
                    yearly_window_arrays.extend(arrays)
                
                # COMBINE: Static arrays first, then yearly arrays
                all_window_arrays = static_window_arrays + yearly_window_arrays
                
                if not all_window_arrays:
                    continue
                
                # Stack into single band array
                band_stack = np.concatenate([a if a.ndim == 3 else a[np.newaxis, ...] for a in all_window_arrays], axis=0)
                
                # Safety check: ensure band count matches expected
                if band_stack.shape[0] != len(current_band_names):
                    error_msg = (f"Band count mismatch for year {year}, window {w}: "
                               f"expected {len(current_band_names)} bands, got {band_stack.shape[0]}")
                    log_progress(f"ERROR: {error_msg}")
                    raise ValueError(error_msg)
                
                # Write window to GeoTIFF
                for band_idx in range(band_stack.shape[0]):
                    tif_ds.write(band_stack[band_idx].astype(np.float32), band_idx + 1, window=w)
                
                # Parquet streaming with backbone mask
                table = window_to_arrow(year, w, band_stack, current_band_names, ref_profile, backbone_window)
                if parquet_writer is None:
                    log_progress("  Initializing Parquet writer...")
                    parquet_writer = pq.ParquetWriter(PARQUET_PATH, table.schema, compression='zstd')
                if table.num_rows > 0:
                    parquet_writer.write_table(table)
                    year_rows += table.num_rows
                
                # Clean up memory
                del band_stack, static_window_arrays, yearly_window_arrays, all_window_arrays, table, backbone_window
                gc.collect()
            
            tif_ds.close()
            file_size_mb = out_tif.stat().st_size / (1024 * 1024)
            wandb.log({
                f"year/{year}/rows": year_rows,
                f"year/{year}/bands": len(current_band_names),
                f"year/{year}/tiff_size_mb": file_size_mb,
                f"year/{year}/status": "success"
            })
            total_rows += year_rows
            log_progress(f"  Completed year {year}")
            
        except Exception as e:
            log_progress(f"ERROR processing year {year}: {e}")
            import traceback
            traceback.print_exc()
            wandb.log({
                f"year/{year}/status": "error",
                f"year/{year}/error_message": str(e)
            })
            continue

    # Close Parquet writer
    if parquet_writer is not None:
        log_progress("Closing Parquet writer...")
        parquet_writer.close()
        
        if PARQUET_PATH.exists():
            parquet_size_mb = PARQUET_PATH.stat().st_size / (1024 * 1024)
            log_progress(f"Wrote Parquet panel: {PARQUET_PATH}")
            log_progress(f"Panel size: {parquet_size_mb:.1f} MB")
            log_progress(f"Total rows: {total_rows:,}")
            
            wandb.log({
                "output/parquet_rows": total_rows,
                "output/parquet_columns": len(all_band_names) + 5,  # bands + year + x + y + row + col
                "output/parquet_size_mb": parquet_size_mb,
                "output/years_processed": len(YEARS)
            })
        else:
            log_progress("ERROR: Parquet file was not created")
            wandb.log({"output/status": "failed", "output/error": "file_not_created"})
    else:
        log_progress("ERROR: No data to write to Parquet")
        wandb.log({"output/status": "failed", "output/error": "no_data"})

    total_time = time.time() - total_start_time
    log_progress(f"Merge process completed in {total_time/60:.1f} minutes")
    
    wandb.log({
        "summary/total_time_minutes": total_time / 60,
        "summary/total_time_hours": total_time / 3600,
        "summary/status": "completed"
    })
    
    wandb.finish()


if __name__ == "__main__":
    main()
