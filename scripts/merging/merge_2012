#!/usr/bin/env python3
"""
Merge South America 1√ó1 km datasets into a yearly tile-year panel (2012‚Äì2024).

- Loads aligned rasters from data/ready/* for dynamic datasets (WDPA, VIIRS, NDVI, GPW, GDP, deforestation, landcover, DynamicWorld) and static layers (WorldClim, assetlevel, elevation, slope, oil_gas, road_infrastructure, GSN, powerplants).
- Uses WDPA rasters as the spatial backbone (grid, transform, CRS). If multiple WDPA years exist, the first discovered file defines the grid. All datasets are reprojected/warped to match this grid if needed.
- For dynamic datasets, forward-fill missing years; if a dataset starts after 2012, backfill earlier years with the first available year so every pixel has values for every year.
- Writes one GeoTIFF per year with all dataset bands.
- Also writes one Parquet with a long table: one row per pixel-year with all dataset variables.
"""

import re
import sys
import math
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import numpy as np
import rasterio
from rasterio.enums import Resampling
from rasterio.warp import reproject
import pandas as pd


DATA_ROOT = Path("/Users/gianluca/Desktop/Master's Thesis/code/data")
READY_ROOT = DATA_ROOT / "ready"
OUTPUT_ROOT = Path("/Users/gianluca/Desktop/Master's Thesis/code/outputs/Results")
OUTPUT_TIFF_DIR = OUTPUT_ROOT / "merged_tifs"
OUTPUT_TIFF_DIR.mkdir(parents=True, exist_ok=True)
PARQUET_PATH = OUTPUT_ROOT / "merged_panel_2012_2024.parquet"

YEARS = list(range(2012, 2025))


def find_wdpa_files() -> Dict[int, Path]:
    """Locate WDPA yearly files under ready or data root and return mapping year->path."""
    candidates: List[Path] = []
    # Search under ready/WDPA and data/WDPA
    for base in [READY_ROOT / "WDPA", DATA_ROOT / "WDPA"]:
        if base.exists():
            candidates.extend(sorted(base.glob("*.tif")))
    year_re = re.compile(r"(19|20)\d{2}")
    mapping: Dict[int, Path] = {}
    for p in candidates:
        m = year_re.search(p.name)
        if not m:
            continue
        year = int(m.group(0))
        mapping[year] = p
    return mapping


def open_reference_wdpa() -> Tuple[Path, rasterio.DatasetReader]:
    wdpa_map = find_wdpa_files()
    if not wdpa_map:
        print("‚ùå No WDPA files found under data/ready/WDPA or data/WDPA.")
        sys.exit(1)
    # Prefer 2012 else closest available year
    ref_year = 2012 if 2012 in wdpa_map else sorted(wdpa_map.keys())[0]
    ref_path = wdpa_map[ref_year]
    src = rasterio.open(ref_path)
    print(f"‚úÖ Using WDPA backbone {ref_path.name} (year {ref_year}) as reference grid")
    return ref_path, src


def discover_raster_catalog() -> Dict[str, Dict[Optional[int], List[Path]]]:
    """
    Scan READY_ROOT subfolders and build a catalog:
    {dataset_name: {year(or None for static): [paths]}}

    Notes:
    - If multiple files match a year for a dataset, keep all (multi-band via stacking later).
    - Files with no year in their name are treated as static (key None).
    """
    catalog: Dict[str, Dict[Optional[int], List[Path]]] = {}
    year_re = re.compile(r"(19|20)\d{2}")
    if not READY_ROOT.exists():
        print(f"‚ùå Missing directory: {READY_ROOT}")
        sys.exit(1)
    for subdir in sorted([p for p in READY_ROOT.iterdir() if p.is_dir()]):
        dataset = subdir.name
        catalog.setdefault(dataset, {})
        for tif in sorted(subdir.rglob("*.tif")):
            m = year_re.search(tif.name)
            year = int(m.group(0)) if m else None
            catalog[dataset].setdefault(year, []).append(tif)
    return catalog


def read_reproject_to_ref(path: Path, ref_profile: dict) -> np.ndarray:
    """Read a raster and reproject/resample to the reference profile, returning a float32 array (H, W) or (B, H, W)."""
    with rasterio.open(path) as src:
        dst_height = ref_profile["height"]
        dst_width = ref_profile["width"]
        dst_transform = ref_profile["transform"]
        dst_crs = ref_profile["crs"]
        count = src.count
        dst = np.full((count, dst_height, dst_width), np.nan, dtype=np.float32)
        for b in range(1, count + 1):
            reproject(
                source=rasterio.band(src, b),
                destination=dst[b - 1],
                src_transform=src.transform,
                src_crs=src.crs,
                dst_transform=dst_transform,
                dst_crs=dst_crs,
                resampling=Resampling.nearest,
                dst_nodata=np.nan,
            )
        return dst if count > 1 else dst[0]


def stack_arrays(arrays: List[np.ndarray]) -> np.ndarray:
    """Stack arrays into (B, H, W), auto-expanding (H, W) to (1, H, W)."""
    normed: List[np.ndarray] = []
    for a in arrays:
        if a.ndim == 2:
            normed.append(a[np.newaxis, ...])
        elif a.ndim == 3:
            normed.append(a)
        else:
            raise ValueError("Unsupported array ndim")
    return np.concatenate(normed, axis=0) if normed else np.empty((0,))


def build_band_names(dataset: str, arrays: List[np.ndarray]) -> List[str]:
    """
    Build band names for a dataset given contributing arrays.
    - Single-band: [f"{dataset}"]
    - Multi-band: [f"{dataset}_b{i}"]
    - Multiple files that are single/multi-band are concatenated in order.
    """
    names: List[str] = []
    idx = 1
    for arr in arrays:
        if arr.ndim == 2:
            names.append(f"{dataset}")
        else:
            for i in range(arr.shape[0]):
                names.append(f"{dataset}_b{idx}")
                idx += 1
    # If all were single-band and duplicates resulted, ensure unique suffixes
    if len(names) != len(set(names)):
        names = [f"{dataset}_{i+1}" for i in range(len(names))]
    return names


def forward_and_backward_fill(year_to_array: Dict[int, np.ndarray]) -> Dict[int, np.ndarray]:
    """Ensure coverage for YEARS by forward filling within YEARS and backfilling at the beginning."""
    if not year_to_array:
        return {y: np.full_like(next(iter(year_to_array.values())), np.nan) for y in YEARS}
    available_years = sorted(year_to_array.keys())
    first_year = available_years[0]
    last_known = year_to_array[first_year]
    filled: Dict[int, np.ndarray] = {}
    # Backfill years before first available
    for y in YEARS:
        if y < first_year:
            filled[y] = last_known
        else:
            break
    # Iterate years in order, forward fill
    for y in YEARS:
        if y in year_to_array:
            last_known = year_to_array[y]
        filled[y] = last_known
    return filled


def write_year_tif(year: int, band_stack: np.ndarray, band_names: List[str], ref_profile: dict) -> Path:
    profile = ref_profile.copy()
    profile.update({
        "count": band_stack.shape[0],
        "dtype": rasterio.float32,
        "nodata": np.nan,
        "compress": "deflate",
        "predictor": 2,
        "tiled": True,
        "blockxsize": 256,
        "blockysize": 256,
    })
    out_path = OUTPUT_TIFF_DIR / f"merged_SA_1km_{year}.tif"
    with rasterio.open(out_path, "w", **profile) as dst:
        for i in range(band_stack.shape[0]):
            dst.write(band_stack[i].astype(np.float32), i + 1)
            try:
                dst.set_band_description(i + 1, band_names[i])
            except Exception:
                pass
    return out_path


def to_long_dataframe(year: int, band_stack: np.ndarray, band_names: List[str], ref_profile: dict) -> pd.DataFrame:
    h, w = ref_profile["height"], ref_profile["width"]
    transform = ref_profile["transform"]
    rows, cols = np.indices((h, w))
    xs, ys = rasterio.transform.xy(transform, rows, cols)
    xs = np.asarray(xs, dtype=np.float64)
    ys = np.asarray(ys, dtype=np.float64)
    data = {
        "year": np.full(h * w, year, dtype=np.int16),
        "x": xs.reshape(-1),
        "y": ys.reshape(-1),
        "row": rows.reshape(-1).astype(np.int32),
        "col": cols.reshape(-1).astype(np.int32),
    }
    for i, name in enumerate(band_names):
        data[name] = band_stack[i].reshape(-1).astype(np.float32)
    df = pd.DataFrame(data)
    return df


def main():
    ref_path, ref_src = open_reference_wdpa()
    ref_profile = ref_src.profile.copy()
    # Normalize profile for float32 outputs
    ref_profile.update({
        "count": 1,
        "dtype": rasterio.float32,
        "nodata": np.nan,
        "compress": "deflate",
    })
    print("üìÅ Scanning ready datasets...")
    catalog = discover_raster_catalog()

    # Identify datasets to include
    include_priority = [
        "WDPA",
        "VIIRS",
        "NDVI",
        "GPW",
        "gdp",
        "gdp_preprocessing",
        "gdp_processed",
        "deforestation",
        "landcover",
        "DynamicWorld",
        "DW",
        "WorldClim",
        "assetlevel",
        "elevation",
        "slope",
        "oil_gas",
        "road_infrastructure",
        "GSN",
        "powerplants",
    ]

    # Map actual present dataset names preserving folder names
    datasets_present = [d for d in catalog.keys() if d in include_priority or d.lower() in [x.lower() for x in include_priority]]
    # Ensure WDPA is first
    datasets_present = sorted(set(datasets_present), key=lambda d: (0 if d.upper() == "WDPA" else 1, d.lower()))
    if not datasets_present:
        print("‚ùå No datasets found under data/ready.")
        sys.exit(1)
    print(f"üß≠ Datasets included: {', '.join(datasets_present)}")

    # Prepare per-dataset band definitions (names and arrays by year)
    per_year_band_arrays: Dict[int, List[np.ndarray]] = {y: [] for y in YEARS}
    per_year_band_names: Optional[List[str]] = None

    # Iterate datasets
    for dataset in datasets_present:
        entries = catalog[dataset]
        # Build per-year arrays for this dataset
        year_to_array: Dict[int, np.ndarray] = {}
        static_arrays: List[np.ndarray] = []

        # Static files (no year in filename)
        if None in entries:
            for p in entries[None]:
                arr = read_reproject_to_ref(p, ref_profile)
                static_arrays.append(arr)

        # Yearly files
        for year, paths in entries.items():
            if year is None:
                continue
            # Keep only within requested YEARS
            if year not in YEARS:
                continue
            arrays = [read_reproject_to_ref(p, ref_profile) for p in paths]
            year_to_array[year] = stack_arrays(arrays) if len(arrays) > 1 else (arrays[0] if arrays else np.empty((0,)))

        # If there are multiple static files, stack them; else single
        static_stack: Optional[np.ndarray] = None
        if static_arrays:
            static_stack = stack_arrays(static_arrays)

        # Build band names for this dataset
        # Use representative arrays to determine band counts
        sample_arrays: List[np.ndarray] = []
        if static_stack is not None:
            sample_arrays.append(static_stack)
        if year_to_array:
            # pick the first year array as representative
            sample_arrays.append(next(iter(year_to_array.values())))
        if not sample_arrays:
            # Nothing to add for this dataset
            continue
        band_names = build_band_names(dataset, sample_arrays)

        # Fill across years
        if not year_to_array and static_stack is not None:
            # Purely static dataset: reuse for all years
            for y in YEARS:
                per_year_band_arrays[y].append(static_stack if static_stack.ndim == 3 else static_stack[np.newaxis, ...])
        else:
            # Have yearly data (possibly partial)
            filled = forward_and_backward_fill(year_to_array)
            for y in YEARS:
                arr_y = filled[y]
                # If also static_stack exists, concatenate static to yearly
                arrays_to_stack: List[np.ndarray] = []
                if static_stack is not None:
                    arrays_to_stack.append(static_stack)
                arrays_to_stack.append(arr_y)
                stacked = stack_arrays(arrays_to_stack)
                per_year_band_arrays[y].append(stacked)

        # Persist band names on first dataset; subsequently, extend
        if per_year_band_names is None:
            per_year_band_names = band_names.copy()
        else:
            per_year_band_names.extend(band_names)

    if per_year_band_names is None:
        print("‚ùå Failed to assemble any bands.")
        sys.exit(1)

    # Write yearly GeoTIFFs and accumulate Parquet rows
    all_years_df: List[pd.DataFrame] = []
    for year in YEARS:
        # Concatenate dataset stacks for this year along band axis
        ds_stacks = per_year_band_arrays[year]
        band_stack = np.concatenate([s if s.ndim == 3 else s[np.newaxis, ...] for s in ds_stacks], axis=0)
        out_tif = write_year_tif(year, band_stack, per_year_band_names, ref_profile)
        print(f"üíæ Wrote {out_tif}")
        # Make year dataframe
        df_year = to_long_dataframe(year, band_stack, per_year_band_names, ref_profile)
        all_years_df.append(df_year)

    # Concatenate and write Parquet
    panel_df = pd.concat(all_years_df, ignore_index=True)
    panel_df.to_parquet(PARQUET_PATH, index=False)
    print(f"üì¶ Wrote Parquet panel: {PARQUET_PATH}")


if __name__ == "__main__":
    main()


