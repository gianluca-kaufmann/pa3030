#!/usr/bin/python3
"""
Optimized merge script for South America 1√ó1 km datasets into a yearly tile-year panel (2012‚Äì2024).

Key memory optimizations to prevent OOM:
- Year-by-year processing: Process and write each year completely before moving to next
- Incremental Parquet writing: Write each year immediately using pyarrow.ParquetWriter
- Memory cleanup: Explicit del and gc.collect() after each year
- Vectorized coordinate generation: Use affine transform matrix instead of loops
- Process datasets per-year only: Don't load all years for all datasets simultaneously

Performance optimizations:
- Skip reprojection when files are already aligned
- Progress monitoring and logging
- Better error handling

Short overview of what the script does:
1) Initialize Weights & Biases and resolve paths for data and outputs.
2) Open a WDPA raster to define the reference grid (shape, CRS, transform).
3) Discover all candidate rasters in data/ready and group them by dataset and year.
4) Determine the final band list by probing datasets for a sample year.
5) For each year (2012‚Äì2024):
   - Load only that year's rasters per dataset (with forward-fill fallback and static layers).
   - Reproject on-the-fly to match the reference grid if needed.
   - Concatenate dataset arrays into a single band stack for the year.
   - Write a per-year GeoTIFF and convert the stack to a long DataFrame (vectorized coords).
   - Append the year's rows to a single Parquet file using a persistent writer.
   - Immediately free memory for arrays and DataFrame chunks.
"""

import re
import sys
import math
import json
import time
import os
import gc
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import numpy as np
import rasterio
from rasterio.enums import Resampling
from rasterio.warp import reproject
import pandas as pd
from rasterio.windows import Window
import pyarrow as pa
import pyarrow.parquet as pq
import wandb

# Determine base path - use current script location for portability
SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent.parent

# Use $SCRATCH for data if on cluster, otherwise use project root
if "SCRATCH" in os.environ:
    DATA_ROOT = Path(os.environ["SCRATCH"]) / "data"
    READY_ROOT = DATA_ROOT / "ready"
else:
    DATA_ROOT = PROJECT_ROOT / "data"
    READY_ROOT = DATA_ROOT / "ready"

# Use $SCRATCH if on cluster, otherwise use local outputs
if "SCRATCH" in os.environ:
    OUTPUT_ROOT = Path(os.environ["SCRATCH"]) / "outputs" / "Results"
else:
    OUTPUT_ROOT = PROJECT_ROOT / "outputs" / "Results"

OUTPUT_TIFF_DIR = OUTPUT_ROOT / "merged_tifs"
OUTPUT_TIFF_DIR.mkdir(parents=True, exist_ok=True)
PARQUET_PATH = OUTPUT_ROOT / "merged_panel_2012_2024.parquet"

YEARS = list(range(2012, 2025))
TILE_SIZE = 512


def log_progress(message: str, start_time: Optional[float] = None):
    """Log progress with optional timing."""
    if start_time:
        elapsed = time.time() - start_time
        print(f"‚è±Ô∏è  {message} ({elapsed:.1f}s)")
    else:
        print(f"üìä {message}")


def find_wdpa_files() -> Dict[int, Path]:
    """Locate WDPA yearly files under ready or data root and return mapping year->path."""
    log_progress("Scanning for WDPA files...")
    log_progress(f"  DATA_ROOT = {DATA_ROOT}")
    log_progress(f"  READY_ROOT = {READY_ROOT}")
    log_progress(f"  Looking in: {READY_ROOT / 'WDPA'}")
    log_progress(f"  Looking in: {DATA_ROOT / 'WDPA'}")
    
    candidates: List[Path] = []
    # Search under ready/WDPA and data/WDPA
    for base in [READY_ROOT / "WDPA", DATA_ROOT / "WDPA"]:
        log_progress(f"  Checking if {base} exists...")
        if base.exists():
            log_progress(f"    ‚úì {base} exists")
            tifs = sorted(base.glob("*.tif"))
            log_progress(f"    Found {len(tifs)} .tif files")
            if tifs:
                for tif in tifs[:5]:  # Show first 5 files
                    log_progress(f"      - {tif.name}")
                if len(tifs) > 5:
                    log_progress(f"      ... and {len(tifs) - 5} more")
            candidates.extend(tifs)
        else:
            log_progress(f"    ‚úó {base} does NOT exist")
    
    year_re = re.compile(r"(19|20)\d{2}")
    mapping: Dict[int, Path] = {}
    for p in candidates:
        m = year_re.search(p.name)
        if not m:
            log_progress(f"  Skipping {p.name} (no year found in filename)")
            continue
        year = int(m.group(0))
        mapping[year] = p
        log_progress(f"  ‚úì Matched: {p.name} ‚Üí year {year}")
    
    log_progress(f"Found {len(mapping)} WDPA files with years")
    return mapping


def open_reference_wdpa() -> Tuple[Path, rasterio.DatasetReader]:
    wdpa_map = find_wdpa_files()
    if not wdpa_map:
        print("‚ùå No WDPA files found under data/ready/WDPA or data/WDPA.")
        print(f"\nüìÅ Debug Info:")
        print(f"   PROJECT_ROOT = {PROJECT_ROOT}")
        print(f"   DATA_ROOT = {DATA_ROOT}")
        print(f"   READY_ROOT = {READY_ROOT}")
        print(f"\nüîç Checked locations:")
        print(f"   - {READY_ROOT / 'WDPA'} (exists: {(READY_ROOT / 'WDPA').exists()})")
        print(f"   - {DATA_ROOT / 'WDPA'} (exists: {(DATA_ROOT / 'WDPA').exists()})")
        print(f"\nüí° Please ensure WDPA .tif files with years in their names exist in one of these locations")
        sys.exit(1)
    # Prefer 2012 else closest available year
    ref_year = 2012 if 2012 in wdpa_map else sorted(wdpa_map.keys())[0]
    ref_path = wdpa_map[ref_year]
    src = rasterio.open(ref_path)
    log_progress(f"Using WDPA backbone {ref_path.name} (year {ref_year}) as reference grid")
    log_progress(f"Reference grid: {src.shape} pixels, CRS: {src.crs}")
    return ref_path, src


def discover_raster_catalog() -> Dict[str, Dict[Optional[int], List[Path]]]:
    """
    Scan READY_ROOT subfolders and build a catalog:
    {dataset_name: {year(or None for static): [paths]}}
    """
    log_progress("Building raster catalog...")
    catalog: Dict[str, Dict[Optional[int], List[Path]]] = {}
    year_re = re.compile(r"(19|20)\d{2}")
    if not READY_ROOT.exists():
        print(f"‚ùå Missing directory: {READY_ROOT}")
        sys.exit(1)
    
    total_files = 0
    for subdir in sorted([p for p in READY_ROOT.iterdir() if p.is_dir()]):
        dataset = subdir.name
        catalog.setdefault(dataset, {})
        files_in_dataset = 0
        for tif in sorted(subdir.rglob("*.tif")):
            m = year_re.search(tif.name)
            year = int(m.group(0)) if m else None
            catalog[dataset].setdefault(year, []).append(tif)
            files_in_dataset += 1
            total_files += 1
        if files_in_dataset > 0:
            log_progress(f"  {dataset}: {files_in_dataset} files")
    
    log_progress(f"Total files catalogued: {total_files}")
    return catalog


def check_alignment(src_path: Path, ref_profile: dict) -> bool:
    """Check if a raster is already aligned with the reference grid."""
    try:
        with rasterio.open(src_path) as src:
            return (
                src.shape == (ref_profile["height"], ref_profile["width"]) and
                src.crs == ref_profile["crs"] and
                np.allclose(src.transform, ref_profile["transform"], atol=1e-6)
            )
    except Exception:
        return False


def read_reproject_to_ref(path: Path, ref_profile: dict) -> np.ndarray:
    """Read a raster and reproject/resample to the reference profile, returning a float32 array."""
    log_progress(f"  Processing {path.name}...")
    start_time = time.time()
    
    # Check if already aligned
    if check_alignment(path, ref_profile):
        log_progress(f"    Already aligned, reading directly...")
        with rasterio.open(path) as src:
            data = src.read().astype(np.float32)
            # Handle nodata values
            if src.nodata is not None:
                data[data == src.nodata] = np.nan
            log_progress(f"    Read {path.name} ({time.time() - start_time:.1f}s)")
            return data
    
    log_progress(f"    Reprojecting {path.name}...")
    with rasterio.open(path) as src:
        dst_height = ref_profile["height"]
        dst_width = ref_profile["width"]
        dst_transform = ref_profile["transform"]
        dst_crs = ref_profile["crs"]
        count = src.count
        dst = np.full((count, dst_height, dst_width), np.nan, dtype=np.float32)
        
        for b in range(1, count + 1):
            reproject(
                source=rasterio.band(src, b),
                destination=dst[b - 1],
                src_transform=src.transform,
                src_crs=src.crs,
                dst_transform=dst_transform,
                dst_crs=dst_crs,
                resampling=Resampling.nearest,
                dst_nodata=np.nan,
            )
        
        log_progress(f"    Reprojected {path.name} ({time.time() - start_time:.1f}s)")
        return dst if count > 1 else dst[0]


def iter_reference_windows(ref_profile: dict, tile_size: int = TILE_SIZE):
    """Yield rasterio Window objects covering the reference grid in tiles."""
    height = ref_profile["height"]
    width = ref_profile["width"]
    for row_off in range(0, height, tile_size):
        for col_off in range(0, width, tile_size):
            h = min(tile_size, height - row_off)
            w = min(tile_size, width - col_off)
            yield Window(col_off=col_off, row_off=row_off, width=w, height=h)


def read_reproject_window(src_path: Path, ref_profile: dict, window: Window) -> np.ndarray:
    """
    Read a source raster and reproject only the given reference window to a float32 array (B, h, w).
    """
    with rasterio.open(src_path) as src:
        dst_height = int(window.height)
        dst_width = int(window.width)
        dst_transform = rasterio.windows.transform(window, ref_profile["transform"])
        dst_crs = ref_profile["crs"]
        count = src.count
        dst = np.full((count, dst_height, dst_width), np.nan, dtype=np.float32)
        for b in range(1, count + 1):
            reproject(
                source=rasterio.band(src, b),
                destination=dst[b - 1],
                src_transform=src.transform,
                src_crs=src.crs,
                dst_transform=dst_transform,
                dst_crs=dst_crs,
                resampling=Resampling.nearest,
                dst_nodata=np.nan,
            )
        return dst if count > 1 else dst[0][np.newaxis, ...]


def open_year_tif_writer(year: int, band_count: int, band_names: List[str], ref_profile: dict):
    """Open a GeoTIFF for the given year for windowed writing; returns (dst, out_path)."""
    profile = ref_profile.copy()
    profile.update({
        "count": band_count,
        "dtype": rasterio.float32,
        "nodata": np.nan,
        "compress": "lzw",
        "tiled": True,
        "blockxsize": 512,
        "blockysize": 512,
    })
    out_path = OUTPUT_TIFF_DIR / f"merged_SA_1km_{year}.tif"
    dst = rasterio.open(out_path, "w", **profile)
    for i, name in enumerate(band_names):
        try:
            dst.set_band_description(i + 1, name)
        except Exception:
            pass
    return dst, out_path


def window_to_arrow(year: int, window: Window, band_stack: np.ndarray, band_names: List[str], ref_profile: dict) -> pa.Table:
    """Create an Arrow table for a window without using pandas."""
    h = int(window.height)
    w = int(window.width)
    rows = (np.arange(h, dtype=np.int32) + int(window.row_off)).repeat(w)
    cols = np.tile(np.arange(w, dtype=np.int32) + int(window.col_off), h)
    transform = ref_profile["transform"]
    xs = (transform.a * cols + transform.b * rows + transform.c).astype(np.float64)
    ys = (transform.d * cols + transform.e * rows + transform.f).astype(np.float64)

    # Flatten bands
    flat_bands = [band_stack[i].reshape(-1).astype(np.float32) for i in range(band_stack.shape[0])]

    # Drop rows where all bands are NaN
    if flat_bands:
        all_nan = np.ones_like(flat_bands[0], dtype=bool)
        for fb in flat_bands:
            all_nan &= np.isnan(fb)
        keep = ~all_nan
    else:
        keep = np.array([], dtype=bool)

    cols_dict = {
        "year": pa.array(np.full(keep.sum(), year, dtype=np.int16)),
        "x": pa.array(xs[keep]),
        "y": pa.array(ys[keep]),
        "row": pa.array(rows[keep]),
        "col": pa.array(cols[keep]),
    }
    for name, fb in zip(band_names, flat_bands):
        cols_dict[name] = pa.array(fb[keep])
    return pa.table(cols_dict)


def stack_arrays(arrays: List[np.ndarray]) -> np.ndarray:
    """Stack arrays into (B, H, W), auto-expanding (H, W) to (1, H, W)."""
    if not arrays:
        return np.empty((0,))
    
    normed: List[np.ndarray] = []
    for a in arrays:
        if a.ndim == 2:
            normed.append(a[np.newaxis, ...])
        elif a.ndim == 3:
            normed.append(a)
        else:
            raise ValueError(f"Unsupported array ndim: {a.ndim}")
    
    return np.concatenate(normed, axis=0)


def build_band_names(dataset: str, arrays: List[np.ndarray]) -> List[str]:
    """Build band names for a dataset given contributing arrays."""
    names: List[str] = []
    idx = 1
    for arr in arrays:
        if arr.ndim == 2:
            names.append(f"{dataset}")
        else:
            for i in range(arr.shape[0]):
                names.append(f"{dataset}_b{idx}")
                idx += 1
    
    # If all were single-band and duplicates resulted, ensure unique suffixes
    if len(names) != len(set(names)):
        names = [f"{dataset}_{i+1}" for i in range(len(names))]
    return names


def forward_and_backward_fill(year_to_array: Dict[int, np.ndarray]) -> Dict[int, np.ndarray]:
    """Ensure coverage for YEARS by forward filling within YEARS and backfilling at the beginning."""
    if not year_to_array:
        # Return empty arrays if no data
        sample_shape = (1, 100, 100)  # Dummy shape
        return {y: np.full(sample_shape, np.nan) for y in YEARS}
    
    available_years = sorted(year_to_array.keys())
    first_year = available_years[0]
    last_known = year_to_array[first_year]
    filled: Dict[int, np.ndarray] = {}
    
    # Backfill years before first available
    for y in YEARS:
        if y < first_year:
            filled[y] = last_known.copy()
        else:
            break
    
    # Iterate years in order, forward fill
    for y in YEARS:
        if y in year_to_array:
            last_known = year_to_array[y]
        filled[y] = last_known.copy()
    
    return filled


def write_year_tif(year: int, band_stack: np.ndarray, band_names: List[str], ref_profile: dict) -> Path:
    """Write a yearly GeoTIFF with optimized compression."""
    log_progress(f"Writing {year} GeoTIFF...")
    profile = ref_profile.copy()
    profile.update({
        "count": band_stack.shape[0],
        "dtype": rasterio.float32,
        "nodata": np.nan,
        "compress": "lzw",  # Faster compression than deflate
        "tiled": True,
        "blockxsize": 512,
        "blockysize": 512,
    })
    
    out_path = OUTPUT_TIFF_DIR / f"merged_SA_1km_{year}.tif"
    with rasterio.open(out_path, "w", **profile) as dst:
        for i in range(band_stack.shape[0]):
            dst.write(band_stack[i].astype(np.float32), i + 1)
            try:
                dst.set_band_description(i + 1, band_names[i])
            except Exception:
                pass
    
    log_progress(f"Wrote {out_path}")
    return out_path


# Removed pandas DataFrame conversions in favor of direct Arrow streaming per window


# Removed global DataFrame creation; coordinates and bands are emitted per window


def process_dataset_for_year(dataset: str, entries: Dict[Optional[int], List[Path]], 
                             ref_profile: dict, target_year: int) -> Tuple[List[str], Optional[np.ndarray]]:
    """
    Process a single dataset for a specific year only.
    Returns band names and array for that year (or None if no data).
    More memory efficient than processing all years at once.
    """
    log_progress(f"  Processing dataset: {dataset} for year {target_year}")
    start_time = time.time()
    
    static_arrays: List[np.ndarray] = []
    year_arrays: List[np.ndarray] = []

    # Static files (no year in filename) - apply to all years
    if None in entries:
        for p in entries[None]:
            arr = read_reproject_to_ref(p, ref_profile)
            static_arrays.append(arr)

    # Yearly files for target year
    if target_year in entries:
        for path in entries[target_year]:
            arr = read_reproject_to_ref(path, ref_profile)
            year_arrays.append(arr)

    # Forward fill: if no data for this year, check earlier years
    if not year_arrays and target_year > min(YEARS):
        # Try to find data from previous years
        for prev_year in reversed(range(min(YEARS), target_year)):
            if prev_year in entries:
                log_progress(f"    Forward-filling from year {prev_year}")
                for path in entries[prev_year]:
                    arr = read_reproject_to_ref(path, ref_profile)
                    year_arrays.append(arr)
                break

    # Combine static and yearly arrays
    all_arrays = static_arrays + year_arrays
    
    if not all_arrays:
        log_progress(f"    No data found for {dataset} in year {target_year}")
        return [], None

    # Stack arrays
    if len(all_arrays) > 1:
        stacked = stack_arrays(all_arrays)
    else:
        stacked = all_arrays[0]
        if stacked.ndim == 2:
            stacked = stacked[np.newaxis, ...]
    
    # Build band names
    band_names = build_band_names(dataset, all_arrays)
    
    log_progress(f"    Completed {dataset} for year {target_year}: {len(band_names)} bands ({time.time() - start_time:.1f}s)")
    return band_names, stacked


def process_dataset(dataset: str, entries: Dict[Optional[int], List[Path]], ref_profile: dict) -> Tuple[List[str], Dict[int, np.ndarray]]:
    """Process a single dataset and return band names and yearly arrays."""
    log_progress(f"Processing dataset: {dataset}")
    start_time = time.time()
    
    # Build per-year arrays for this dataset
    year_to_array: Dict[int, np.ndarray] = {}
    static_arrays: List[np.ndarray] = []

    # Static files (no year in filename)
    if None in entries:
        log_progress(f"  Processing {len(entries[None])} static files...")
        for i, p in enumerate(entries[None]):
            log_progress(f"    Static file {i+1}/{len(entries[None])}: {p.name}")
            arr = read_reproject_to_ref(p, ref_profile)
            static_arrays.append(arr)

    # Yearly files
    yearly_files = sum(len(paths) for year, paths in entries.items() if year is not None)
    log_progress(f"  Processing {yearly_files} yearly files...")
    
    processed_files = 0
    for year, paths in entries.items():
        if year is None or year not in YEARS:
            continue
        
        log_progress(f"    Year {year}: {len(paths)} files")
        arrays = []
        for path in paths:
            arr = read_reproject_to_ref(path, ref_profile)
            arrays.append(arr)
            processed_files += 1
        
        if arrays:
            year_to_array[year] = stack_arrays(arrays) if len(arrays) > 1 else arrays[0]

    # Handle static data
    static_stack: Optional[np.ndarray] = None
    if static_arrays:
        static_stack = stack_arrays(static_arrays)

    # Build band names for this dataset
    sample_arrays: List[np.ndarray] = []
    if static_stack is not None:
        sample_arrays.append(static_stack)
    if year_to_array:
        sample_arrays.append(next(iter(year_to_array.values())))
    
    if not sample_arrays:
        log_progress(f"  No data found for {dataset}")
        return [], {}
    
    band_names = build_band_names(dataset, sample_arrays)
    log_progress(f"  Band names: {band_names}")

    # Fill across years
    if not year_to_array and static_stack is not None:
        # Purely static dataset: reuse for all years
        filled = {}
        for y in YEARS:
            filled[y] = static_stack if static_stack.ndim == 3 else static_stack[np.newaxis, ...]
    else:
        # Have yearly data (possibly partial)
        filled = forward_and_backward_fill(year_to_array)

    log_progress(f"Completed {dataset} ({time.time() - start_time:.1f}s)")
    return band_names, filled


def main():
    """Main processing function with progress monitoring."""
    # Initialize wandb FIRST before any processing
    print("üîÑ Initializing Weights & Biases connection...")
    
    # Get credentials from environment variables
    wandb_api_key = os.environ.get("WANDB_API_KEY")
    wandb_entity = os.environ.get("WANDB_ENTITY")
    
    if not wandb_api_key:
        print("‚ö†Ô∏è  WANDB_API_KEY not found in environment variables")
        print("   Set it with: export WANDB_API_KEY='your-key-here'")
    
    if not wandb_entity:
        print("‚ö†Ô∏è  WANDB_ENTITY not found in environment variables")
        print("   Set it with: export WANDB_ENTITY='your-entity-here'")
    
    wandb.init(
        project="merge",
        entity=wandb_entity,
        save_code=True,
        config={
            "years": YEARS,
            "year_range": f"{min(YEARS)}-{max(YEARS)}",
            "data_root": str(DATA_ROOT),
            "output_root": str(OUTPUT_ROOT),
            "scratch_mode": "SCRATCH" in os.environ,
        }
    )
    print("‚úÖ Weights & Biases connected successfully!")
    
    total_start_time = time.time()
    log_progress("Starting optimized merge process...")
    wandb.log({"status": "started", "start_time": total_start_time})
    
    # Setup reference
    ref_path, ref_src = open_reference_wdpa()
    ref_profile = ref_src.profile.copy()
    ref_profile.update({
        "count": 1,
        "dtype": rasterio.float32,
        "nodata": np.nan,
        "compress": "lzw",
    })
    
    # Log reference grid info
    wandb.log({
        "reference_grid/height": ref_profile["height"],
        "reference_grid/width": ref_profile["width"],
        "reference_grid/total_pixels": ref_profile["height"] * ref_profile["width"],
        "reference_grid/crs": str(ref_profile["crs"]),
    })
    
    ref_src.close()  # Close the reference file
    
    # Build catalog
    catalog = discover_raster_catalog()

    # Identify datasets to include
    include_priority = [
        "WDPA", "VIIRS", "NDVI", "GPW", "gdp", "gdp_preprocessing", "gdp_processed",
        "deforestation", "landcover", "DynamicWorld", "DW", "WorldClim",
        "assetlevel", "elevation", "slope", "oil_gas", "road_infrastructure",
        "GSN", "powerplants",
    ]

    datasets_present = [d for d in catalog.keys() 
                       if d in include_priority or d.lower() in [x.lower() for x in include_priority]]
    datasets_present = sorted(set(datasets_present), 
                             key=lambda d: (0 if d.upper() == "WDPA" else 1, d.lower()))
    
    if not datasets_present:
        print("‚ùå No datasets found under data/ready.")
        sys.exit(1)
    
    log_progress(f"Datasets to process: {', '.join(datasets_present)}")
    wandb.config.update({"datasets": datasets_present, "num_datasets": len(datasets_present)})

    # Determine band names using a small probe window to avoid full reads
    # Check ALL years to ensure we capture datasets that appear in any year
    log_progress("Determining band structure with probe window...")
    probe_window = Window(col_off=0, row_off=0, width=min(8, ref_profile["width"]), height=min(8, ref_profile["height"]))
    all_band_names: List[str] = []
    dataset_band_counts: Dict[str, int] = {}  # Track how many bands each dataset contributes
    for dataset in datasets_present:
        try:
            entries = catalog[dataset]
            candidate_paths: List[Path] = []
            
            # First check for static files (None = no year in filename)
            if None in entries:
                candidate_paths = entries[None]
            else:
                # Find ANY available year for this dataset (not just <= first year)
                # This ensures we capture datasets that start later (e.g., WDPA starting at 2013)
                available_years = sorted([y for y in entries.keys() if y is not None])
                if available_years:
                    # Use the first available year to determine band structure
                    candidate_paths = entries[available_years[0]]
            
            if not candidate_paths:
                log_progress(f"  Skipping {dataset}: no data files found")
                continue
                
            arrays = []
            for p in candidate_paths:
                arr = read_reproject_window(p, ref_profile, probe_window)
                arrays.append(arr)
            band_names = build_band_names(dataset, arrays)
            if band_names:
                all_band_names.extend(band_names)
                # Store the number of bands this dataset contributes (matches band_names length)
                dataset_band_counts[dataset] = len(band_names)
                log_progress(f"  {dataset}: {len(band_names)} bands")
        except Exception as e:
            log_progress(f"  Warning: Could not probe {dataset}: {e}")
            continue

    if not all_band_names:
        print("‚ùå Failed to determine band structure.")
        sys.exit(1)

    log_progress(f"Total bands: {len(all_band_names)}")
    log_progress(f"Band names: {all_band_names}")
    wandb.log({"processing/total_bands": len(all_band_names)})

    # Setup incremental Parquet writer
    log_progress("Setting up incremental Parquet writer...")
    parquet_writer = None
    total_rows = 0
    
    # Process year-by-year using windowed streaming
    log_progress("Processing years sequentially (windowed streaming)...")
    
    for i, year in enumerate(YEARS):
        log_progress(f"\n{'='*60}")
        log_progress(f"Processing year {i+1}/{len(YEARS)}: {year}")
        log_progress(f"{'='*60}")
        
        try:
            # Prepare year GeoTIFF writer
            current_band_names = all_band_names
            tif_ds, out_tif = open_year_tif_writer(year, len(current_band_names), current_band_names, ref_profile)
            year_rows = 0
            # Iterate windows and stream
            for w in iter_reference_windows(ref_profile, TILE_SIZE):
                window_arrays: List[np.ndarray] = []
                window_height = int(w.height)
                window_width = int(w.width)
                
                for dataset in datasets_present:
                    entries = catalog[dataset]
                    paths: List[Path] = []
                    if year in entries:
                        paths = entries[year]
                    elif None in entries:
                        paths = entries[None]
                    else:
                        prevs = [y for y in entries.keys() if y is not None and y < year]
                        if prevs:
                            paths = entries[max(prevs)]
                    
                    # Collect arrays for this dataset
                    dataset_arrays: List[np.ndarray] = []
                    for p in paths:
                        arr = read_reproject_window(p, ref_profile, w)
                        dataset_arrays.append(arr)
                    
                    # If no data found, create NaN placeholder arrays
                    if not dataset_arrays and dataset in dataset_band_counts:
                        expected_bands = dataset_band_counts[dataset]
                        # Create a single placeholder array with all expected bands
                        placeholder = np.full((expected_bands, window_height, window_width), np.nan, dtype=np.float32)
                        dataset_arrays.append(placeholder)
                    
                    window_arrays.extend(dataset_arrays)
                
                if not window_arrays:
                    continue
                band_stack = np.concatenate([a if a.ndim == 3 else a[np.newaxis, ...] for a in window_arrays], axis=0)
                
                # Safety check: ensure band count matches expected
                if band_stack.shape[0] != len(current_band_names):
                    error_msg = (f"Band count mismatch for year {year}, window {w}: "
                               f"expected {len(current_band_names)} bands, got {band_stack.shape[0]}")
                    log_progress(f"‚ùå {error_msg}")
                    raise ValueError(error_msg)
                
                # Write window to GeoTIFF
                for i in range(band_stack.shape[0]):
                    tif_ds.write(band_stack[i].astype(np.float32), i + 1, window=w)
                # Parquet streaming
                table = window_to_arrow(year, w, band_stack, current_band_names, ref_profile)
                if parquet_writer is None:
                    log_progress("  Initializing Parquet writer...")
                    parquet_writer = pq.ParquetWriter(PARQUET_PATH, table.schema, compression='zstd')
                if table.num_rows > 0:
                    parquet_writer.write_table(table)
                    year_rows += table.num_rows
                del band_stack, window_arrays, table
                gc.collect()
            tif_ds.close()
            file_size_mb = out_tif.stat().st_size / (1024 * 1024)
            wandb.log({
                f"year/{year}/rows": year_rows,
                f"year/{year}/bands": len(current_band_names),
                f"year/{year}/tiff_size_mb": file_size_mb,
                f"year/{year}/status": "success"
            })
            total_rows += year_rows
            log_progress(f"  ‚úÖ Completed year {year}")
            
        except Exception as e:
            log_progress(f"‚ùå Error processing year {year}: {e}")
            import traceback
            traceback.print_exc()
            wandb.log({
                f"year/{year}/status": "error",
                f"year/{year}/error_message": str(e)
            })
            continue

    # Close Parquet writer
    if parquet_writer is not None:
        log_progress("Closing Parquet writer...")
        parquet_writer.close()
        
        if PARQUET_PATH.exists():
            parquet_size_mb = PARQUET_PATH.stat().st_size / (1024 * 1024)
            log_progress(f"Wrote Parquet panel: {PARQUET_PATH}")
            log_progress(f"Panel size: {parquet_size_mb:.1f} MB")
            log_progress(f"Total rows: {total_rows:,}")
            
            wandb.log({
                "output/parquet_rows": total_rows,
                "output/parquet_columns": len(all_band_names) + 5,  # bands + year + x + y + row + col
                "output/parquet_size_mb": parquet_size_mb,
                "output/years_processed": len(YEARS)
            })
        else:
            log_progress("‚ùå Parquet file was not created")
            wandb.log({"output/status": "failed", "output/error": "file_not_created"})
    else:
        log_progress("‚ùå No data to write to Parquet")
        wandb.log({"output/status": "failed", "output/error": "no_data"})

    total_time = time.time() - total_start_time
    log_progress(f"‚úÖ Merge process completed in {total_time/60:.1f} minutes")
    
    wandb.log({
        "summary/total_time_minutes": total_time / 60,
        "summary/total_time_hours": total_time / 3600,
        "summary/status": "completed"
    })
    
    wandb.finish()


if __name__ == "__main__":
    main()
