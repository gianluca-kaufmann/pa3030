#!/usr/bin/env python3

"""RF Transition Model (0→1 Prediction)

Train Random Forest to predict transitions to protected areas using temporal split:
- train_early: 2000-2014 (sampled), val_late: 2015-2017 (full), test: 2018-2024 (full)
- Final model trained on combined 2000-2017 data, evaluated on 2018-2024
Input: train_early_sampled.parquet, val_late_full.parquet, test_full.parquet, rf_best_params.json
Output: model.pkl, predictions.parquet, metrics.json
"""

from __future__ import annotations

import gc
import json
import os
import sys
import time
import pickle
from datetime import datetime
from pathlib import Path
from typing import Dict, Any

import numpy as np
import pandas as pd
import wandb
import pyarrow as pa
import pyarrow.parquet as pq
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.metrics import roc_auc_score, average_precision_score
from sklearn.pipeline import Pipeline


class Tee:
    """Write to both stdout and a file."""
    def __init__(self, file_path: Path):
        self.file = open(file_path, 'w', encoding='utf-8')
        self.stdout = sys.stdout
    
    def write(self, text: str) -> None:
        self.stdout.write(text)
        self.file.write(text)
        self.stdout.flush()
        self.file.flush()
    
    def flush(self) -> None:
        self.stdout.flush()
        self.file.flush()
    
    def close(self) -> None:
        self.file.close()


# Configuration
RANDOM_STATE = 42
EXCLUDE_COLS = {'transition_01', 'WDPA_b1', 'WDPA_prev', 'x', 'y', 'row', 'col', 'year'}
TRAIN_EARLY_YEAR_MAX = 2014
VAL_LATE_YEAR_MIN = 2015
VAL_LATE_YEAR_MAX = 2017
FIXED_PARAMS = {'random_state': RANDOM_STATE, 'verbose': 0, 'class_weight': 'balanced_subsample', 'bootstrap': True}
RF_GUARDRAIL_PARAMS = {
    "n_estimators": 500, "max_depth": 12, "min_samples_split": 20, "min_samples_leaf": 10,
    "max_features": "sqrt", "bootstrap": True, "max_samples": 0.2, 
    "class_weight": "balanced_subsample", "verbose": 0,
}
MAX_SAMPLES_FULL_TRAIN = 5_000_000
MAX_SAMPLES_FULL_VAL = 2_000_000

def get_n_jobs() -> int:
    """Get number of CPUs to use, capped at 8."""
    slurm_cpus = os.environ.get("SLURM_CPUS_PER_TASK")
    if slurm_cpus:
        try:
            return min(int(slurm_cpus), 8)
        except ValueError:
            return -1
    return -1


def convert_numpy_types(obj: Any) -> Any:
    """Recursively convert NumPy types to native Python types for JSON serialization."""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [convert_numpy_types(item) for item in obj]
    elif obj is None or isinstance(obj, (str, int, float, bool)):
        return obj
    else:
        try:
            return obj.item() if hasattr(obj, 'item') else obj
        except (ValueError, TypeError, AttributeError):
            return str(obj)


def compute_precision_at_k(y_true: np.ndarray, y_proba: np.ndarray, k: float) -> float:
    """Compute precision at top k% of predictions."""
    n_top_k = max(1, int(len(y_true) * k / 100))
    top_k_idx = np.argsort(y_proba)[-n_top_k:]
    return y_true[top_k_idx].sum() / n_top_k


def resolve_parquet_file(filename: str) -> Path:
    """Locate parquet file (prefer $SCRATCH if present)."""
    repo_root = Path(__file__).resolve().parents[3]
    scratch_root = Path(os.environ["SCRATCH"]) if os.environ.get("SCRATCH") else None

    candidates = []
    if scratch_root is not None:
        candidates.append(scratch_root / f"data/ml/{filename}")
    candidates.append(repo_root / f"data/ml/{filename}")

    for cand in candidates:
        if cand.exists():
            return cand

    raise FileNotFoundError(f"{filename} not found in expected locations")


def resolve_best_params_json() -> Path:
    """Locate rf_best_params.json (in same directory as this script)."""
    script_dir = Path(__file__).resolve().parent
    params_path = script_dir / "rf_best_params.json"
    
    if not params_path.exists():
        raise FileNotFoundError(f"rf_best_params.json not found at {params_path}")
    
    return params_path


def load_best_params(params_path: Path) -> Dict[str, Any]:
    """Load best parameters from JSON and merge with fixed params."""
    with open(params_path, 'r') as f:
        params_data = json.load(f)
    best_params = params_data.get('best_params', {})
    json_fixed_params = params_data.get('fixed_params', {})
    cleaned_params = {(key[4:] if key.startswith('rf__') else key): value for key, value in best_params.items()}
    final_params = {**FIXED_PARAMS, **json_fixed_params, **cleaned_params}
    final_params.update(RF_GUARDRAIL_PARAMS)
    final_params["random_state"] = RANDOM_STATE
    final_params["n_jobs"] = get_n_jobs()
    final_params.pop("criterion", None)
    return final_params, params_data.get('best_cv_score', None)


def downcast_numeric_dtypes(df: pd.DataFrame) -> None:
    """Downcast numeric dtypes to reduce memory usage (in-place)."""
    for col in df.select_dtypes(include=['float64']).columns:
        df[col] = df[col].astype('float32', copy=False)
    for col in df.select_dtypes(include=['int64']).columns:
        df[col] = df[col].astype('int32', copy=False)


def load_and_preprocess(df_path: Path, columns: list, target_col: str, name: str = "") -> pd.DataFrame:
    """Load, downcast, and preprocess DataFrame."""
    print(f"Loading {name or df_path.name}...")
    load_start = time.time()
    df = pd.read_parquet(df_path, columns=columns)
    print(f"  {len(df):,} rows in {time.time() - load_start:.1f}s")
    downcast_numeric_dtypes(df)
    gc.collect()
    if target_col not in df.columns:
        raise ValueError(f"Target column '{target_col}' not found")
    df.dropna(subset=[target_col], inplace=True)
    return df


def extract_split(df: pd.DataFrame, feature_cols: list, target_col: str, 
                  year_filter, split_name: str, max_samples: int | None = None) -> tuple:
    """Extract X, y and stats from filtered DataFrame."""
    df_split = df[year_filter(df['year'])].copy()
    y = (df_split[target_col] > 0).astype(np.int8)
    
    if max_samples is not None and len(df_split) > max_samples:
        original_len = len(df_split)
        n_pos = int(y.sum())
        n_neg_needed = max_samples - n_pos
        if n_neg_needed < 0:
            print(f"  Warning: {n_pos:,} positives exceed max_samples={max_samples:,}, keeping all positives")
            n_neg_needed = 0
        pos_mask = y == 1
        pos_idx = df_split.index[pos_mask]
        neg_idx = df_split.index[~pos_mask]
        if n_neg_needed > 0 and len(neg_idx) > 0:
            n_neg_to_sample = min(n_neg_needed, len(neg_idx))
            neg_sampled_idx = np.random.RandomState(RANDOM_STATE).choice(
                neg_idx, size=n_neg_to_sample, replace=False
            )
            selected_idx = pos_idx.union(pd.Index(neg_sampled_idx))
        else:
            selected_idx = pos_idx
        df_split = df_split.loc[selected_idx].sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)
        y = (df_split[target_col] > 0).astype(np.int8)
        print(f"  Downsampled from {original_len:,} to {len(df_split):,} samples")
    
    X = df_split[feature_cols].copy()
    pos = int(y.sum())
    neg = len(y) - pos
    years = sorted(df_split['year'].unique())
    pos_pct = pos / len(y) * 100
    print(f"{split_name}: {neg:,} neg ({100-pos_pct:.3f}%), {pos:,} pos ({pos_pct:.3f}%), ratio 1:{neg/max(pos,1):.1f}, years {years[0]}-{years[-1]}")
    return X, y, pos, neg, years


def compute_metrics(y_true: np.ndarray, y_proba: np.ndarray) -> Dict[str, float]:
    """Compute all validation/test metrics."""
    roc_auc = roc_auc_score(y_true, y_proba)
    pr_auc = average_precision_score(y_true, y_proba)
    baseline_rate = y_true.mean()
    prec_at_1 = compute_precision_at_k(y_true, y_proba, 1)
    prec_at_5 = compute_precision_at_k(y_true, y_proba, 5)
    prec_at_10 = compute_precision_at_k(y_true, y_proba, 10)
    return {
        "roc_auc": float(roc_auc), "pr_auc": float(pr_auc),
        "precision_at_1pct": float(prec_at_1),
        "precision_at_5pct": float(prec_at_5),
        "precision_at_10pct": float(prec_at_10),
        "baseline_rate": float(baseline_rate),
        "lift_at_1pct": float(prec_at_1 / baseline_rate) if baseline_rate > 0 else 0,
        "lift_at_5pct": float(prec_at_5 / baseline_rate) if baseline_rate > 0 else 0,
        "lift_at_10pct": float(prec_at_10 / baseline_rate) if baseline_rate > 0 else 0,
    }


# =============================================================================
# Main Pipeline
# =============================================================================

def phase1_training(
    train_path: Path,
    val_path: Path,
    feature_cols: list,
    keep_cols_train: list,
    best_params: Dict[str, Any],
    timestamp: str,
    model_dir: Path,
    output_dir: Path,
    use_wandb: bool,
    target_col: str,
) -> tuple[Path, Dict[str, Any], Dict[str, Any], float, float, float, float, int, int]:
    """Phase 1: Training phase - loads train/val splits, trains model, saves it."""
    print("\n" + "=" * 70)
    print("PHASE 1: TRAINING")
    print("=" * 70)
    print(f"\nSTEP A: Train on train_early (year <= {TRAIN_EARLY_YEAR_MAX})")
    
    df_train = load_and_preprocess(train_path, keep_cols_train, target_col, "train_early_sampled.parquet")
    X_train_early, y_train_early, train_early_pos, train_early_neg, _ = extract_split(
        df_train, feature_cols, target_col, 
        lambda y: y <= TRAIN_EARLY_YEAR_MAX, "Train early",
        max_samples=None
    )
    del df_train
    gc.collect()
    
    rf_model = RandomForestClassifier(**best_params)
    pipeline = Pipeline([('imputer', SimpleImputer(strategy='median')), ('rf', rf_model)])
    
    print(f"Training on {len(X_train_early):,} samples...")
    train_early_start = time.time()
    model_train_early = pipeline.fit(X_train_early, y_train_early)
    train_early_time = time.time() - train_early_start
    print(f"  Completed in {train_early_time/60:.1f} min")
    
    del X_train_early, y_train_early
    gc.collect()
    
    print(f"\nSTEP B: Validate on val_late ({VAL_LATE_YEAR_MIN}-{VAL_LATE_YEAR_MAX})")
    df_val = load_and_preprocess(val_path, keep_cols_train, target_col, "val_late_full.parquet")
    X_val_late, y_val_late, val_late_pos, val_late_neg, _ = extract_split(
        df_val, feature_cols, target_col,
        lambda y: (y >= VAL_LATE_YEAR_MIN) & (y <= VAL_LATE_YEAR_MAX), "Val late",
        max_samples=None
    )
    del df_val
    gc.collect()
    
    val_start = time.time()
    y_val_proba = model_train_early.predict_proba(X_val_late)[:, 1]
    val_time = time.time() - val_start
    
    validation_metrics = compute_metrics(y_val_late.values, y_val_proba)
    print(f"Validation: ROC-AUC {validation_metrics['roc_auc']:.4f}, PR-AUC {validation_metrics['pr_auc']:.4f}, " +
          f"P@1% {validation_metrics['precision_at_1pct']:.4f}, P@5% {validation_metrics['precision_at_5pct']:.4f}, " +
          f"P@10% {validation_metrics['precision_at_10pct']:.4f}")
    
    if use_wandb:
        wandb.log({
            **{f"val/{k}": v for k, v in validation_metrics.items()},
            "val/n_samples": int(len(y_val_late)),
            "val/n_positives": int(y_val_late.sum()),
            "val/time_seconds": val_time,
        })
    
    del X_val_late, y_val_late, model_train_early
    gc.collect()
    
    print(f"\nSTEP D: Train final model on full data (2000-2017, capped at {MAX_SAMPLES_FULL_TRAIN:,} train + {MAX_SAMPLES_FULL_VAL:,} val)")
    
    df_train_full = load_and_preprocess(train_path, keep_cols_train, target_col, "train_early_sampled.parquet")
    X_train_part, y_train_part, train_part_pos, train_part_neg, train_part_years = extract_split(
        df_train_full, feature_cols, target_col,
        lambda y: y <= TRAIN_EARLY_YEAR_MAX, "Train part",
        max_samples=MAX_SAMPLES_FULL_TRAIN
    )
    del df_train_full
    gc.collect()
    
    df_val_full = load_and_preprocess(val_path, keep_cols_train, target_col, "val_late_full.parquet")
    X_val_part, y_val_part, val_part_pos, val_part_neg, val_part_years = extract_split(
        df_val_full, feature_cols, target_col,
        lambda y: (y >= VAL_LATE_YEAR_MIN) & (y <= VAL_LATE_YEAR_MAX), "Val part",
        max_samples=MAX_SAMPLES_FULL_VAL
    )
    del df_val_full
    gc.collect()
    
    X = pd.concat([X_train_part, X_val_part], ignore_index=True)
    y = np.concatenate([y_train_part, y_val_part])
    del X_train_part, X_val_part, y_train_part, y_val_part
    gc.collect()
    
    train_pos = train_part_pos + val_part_pos
    train_neg = train_part_neg + val_part_neg
    n_train_full = len(X)
    train_pos_pct = train_pos / n_train_full * 100
    train_years = sorted(set(train_part_years) | set(val_part_years))
    
    print(f"Full training set: {n_train_full:,} samples ({train_pos:,} pos, {train_neg:,} neg, {train_pos_pct:.3f}%), " +
          f"years {train_years[0] if train_years else 'none'}-{train_years[-1] if train_years else 'none'}")
    
    train_stats = {
        "n_train": n_train_full, "train_positives": train_pos, "train_negatives": train_neg,
        "train_positive_pct": train_pos_pct, 
        "train_years": f"{train_years[0]}-{train_years[-1]}" if train_years else "none",
        "max_samples_full_train": MAX_SAMPLES_FULL_TRAIN,
        "max_samples_full_val": MAX_SAMPLES_FULL_VAL,
    }
    
    if use_wandb:
        wandb.log({
            "data/n_features": len(feature_cols), "data/n_train": n_train_full,
            "data/train_positives": train_pos, "data/train_negatives": train_neg,
            "data/train_positive_pct": train_pos_pct,
            "data/max_samples_full_train": MAX_SAMPLES_FULL_TRAIN,
            "data/max_samples_full_val": MAX_SAMPLES_FULL_VAL,
        })
    
    print(f"Training on {len(X):,} samples (2000-2017)...")
    train_start = time.time()
    final_model = pipeline.fit(X, y)
    train_time = time.time() - train_start
    print(f"  Completed in {train_time/60:.1f} min")
    
    model_path = model_dir / f"model1_rf_{timestamp}.pkl"
    with open(model_path, 'wb') as f:
        pickle.dump(final_model, f)
    print(f"  Model saved: {model_path}")
    
    if use_wandb:
        wandb.log({
            "training/train_early_time_seconds": train_early_time,
            "training/train_early_time_minutes": train_early_time / 60,
            "training/final_train_time_seconds": train_time,
            "training/final_train_time_minutes": train_time / 60,
        })
    
    del X, y, final_model
    gc.collect()
    print("Phase 1 completed.")
    
    return (model_path, validation_metrics, train_stats, train_early_time, 
            val_time, train_time, train_pos, train_neg, n_train_full)


def phase2_testing(
    test_path: Path, model_path: Path, feature_cols: list, keep_cols_test: list,
    timestamp: str, output_dir: Path, use_wandb: bool, target_col: str,
    validation_metrics: Dict[str, Any], train_stats: Dict[str, Any],
    train_early_time: float, val_time: float, train_time: float,
    train_pos: int, train_neg: int, n_train_full: int,
    best_params: Dict[str, Any], start_time: float,
) -> None:
    """Phase 2: Testing phase - batched inference to prevent OOM errors."""
    print("\n" + "=" * 70)
    print("PHASE 2: TESTING")
    print("=" * 70)
    
    print(f"Loading model from: {model_path}")
    with open(model_path, 'rb') as f:
        final_model = pickle.load(f)
    
    parquet_file = pq.ParquetFile(test_path)
    batch_size = 100000
    scored_path = output_dir / f"model1_rf_scored_{timestamp}.parquet"
    print(f"Processing test data in {batch_size:,} row batches → {scored_path}")
    
    y_true_list = []
    y_proba_list = []
    
    test_pos = 0
    test_neg = 0
    test_years_set = set()
    total_samples = 0
    batch_idx = -1
    parquet_writer = None
    
    pred_start = time.time()
    
    for batch_idx, batch in enumerate(parquet_file.iter_batches(batch_size=batch_size, columns=keep_cols_test)):
        df_batch = batch.to_pandas()
        downcast_numeric_dtypes(df_batch)
        df_batch.dropna(subset=[target_col], inplace=True)
        if len(df_batch) == 0:
            del df_batch
            gc.collect()
            continue
        
        y_batch = (df_batch[target_col] > 0).astype(np.int8)
        X_batch = df_batch[feature_cols].copy()
        test_pos += int(y_batch.sum())
        test_neg += int((y_batch == 0).sum())
        test_years_set.update(df_batch['year'].unique())
        total_samples += len(y_batch)
        y_proba_batch = final_model.predict_proba(X_batch)[:, 1]
        y_true_list.append(y_batch.values)
        y_proba_list.append(y_proba_batch)
        
        result_dict = {
            'row': df_batch['row'].values, 'col': df_batch['col'].values,
            'year': df_batch['year'].values, 'y_true': y_batch.values,
            'y_pred_proba': y_proba_batch,
        }
        if 'x' in df_batch.columns:
            result_dict['x'] = df_batch['x'].values
        if 'y' in df_batch.columns:
            result_dict['y'] = df_batch['y'].values
        batch_result = pd.DataFrame(result_dict)
        batch_table = pa.Table.from_pandas(batch_result)
        
        if parquet_writer is None:
            parquet_writer = pq.ParquetWriter(scored_path, batch_table.schema)
        parquet_writer.write_table(batch_table)
        
        del df_batch, X_batch, y_batch, y_proba_batch, batch_result, batch_table
        gc.collect()
        if (batch_idx + 1) % 10 == 0:
            print(f"  Processed {batch_idx + 1} batches ({total_samples:,} samples)...")
    
    if parquet_writer is not None:
        parquet_writer.close()
    
    pred_time = time.time() - pred_start
    print(f"Completed {batch_idx + 1} batches ({total_samples:,} samples) in {pred_time:.1f}s")
    
    y_test = np.concatenate(y_true_list)
    y_proba = np.concatenate(y_proba_list)
    del y_true_list, y_proba_list
    gc.collect()
    
    test_years = sorted(test_years_set)
    test_pos_pct = test_pos / len(y_test) * 100
    print(f"Test set: {test_neg:,} neg, {test_pos:,} pos ({test_pos_pct:.3f}%), years {test_years[0]}-{test_years[-1]}")
    
    if use_wandb:
        wandb.log({
            "data/n_test": int(len(y_test)),
            "data/test_positives": test_pos,
            "data/test_negatives": test_neg,
            "data/test_positive_pct": test_pos_pct,
        })
    
    print("\nComputing metrics...")
    test_metrics = compute_metrics(y_test, y_proba)
    prec_at_1, prec_at_5, prec_at_10 = test_metrics['precision_at_1pct'], test_metrics['precision_at_5pct'], test_metrics['precision_at_10pct']
    
    print(f"Test: ROC-AUC {test_metrics['roc_auc']:.4f}, PR-AUC {test_metrics['pr_auc']:.4f}")
    print(f"  P@1%: {prec_at_1:.4f} ({test_metrics['lift_at_1pct']:.2f}x), " +
          f"P@5%: {prec_at_5:.4f} ({test_metrics['lift_at_5pct']:.2f}x), " +
          f"P@10%: {prec_at_10:.4f} ({test_metrics['lift_at_10pct']:.2f}x)")
    
    if use_wandb:
        wandb.log({**{f"test/{k}": v for k, v in test_metrics.items()}})
    
    rf_model = final_model.named_steps['rf']
    feature_importance = pd.DataFrame({
        'feature': feature_cols, 'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False).head(20)
    print("\nTop 20 Features:")
    print(feature_importance.to_string(index=False))
    
    print("\nSaving metrics...")
    metrics = {
        "metadata": {
            "timestamp": timestamp,
            "model": "RandomForest",
            "task": "transition_01_prediction",
            "random_state": RANDOM_STATE,
            "n_features": len(feature_cols),
            "features": feature_cols,
        },
        "data": {
            "n_train_full": n_train_full,
            "n_test": int(len(y_test)),
            "train_positives": train_pos,
            "train_negatives": train_neg,
            "test_positives": test_pos,
            "test_negatives": test_neg,
        },
        "temporal_split": {
            "method": "strict_temporal_split",
            "train_early_max_year": TRAIN_EARLY_YEAR_MAX,
            "val_late_min_year": VAL_LATE_YEAR_MIN,
            "val_late_max_year": VAL_LATE_YEAR_MAX,
        },
        "validation_performance": validation_metrics,
        "model_parameters": best_params,
        "test_performance": test_metrics,
        "feature_importance": feature_importance.head(20).to_dict('records'),
        "timing": {
            "train_early_seconds": train_early_time,
            "validation_seconds": val_time,
            "final_training_seconds": train_time,
            "prediction_seconds": pred_time,
            "total_seconds": time.time() - start_time,
        },
    }
    
    metrics = convert_numpy_types(metrics)
    metrics_path = output_dir / f"model1_rf_metrics_{timestamp}.json"
    with open(metrics_path, 'w') as f:
        json.dump(metrics, f, indent=2)
    print(f"  Saved to: {metrics_path}")
    
    total_time = time.time() - start_time
    print("\n" + "=" * 70)
    print("SUMMARY")
    print("=" * 70)
    print(f"Model: Random Forest ({len(feature_cols)} features)")
    print(f"Validation (≤{TRAIN_EARLY_YEAR_MAX} → {VAL_LATE_YEAR_MIN}-{VAL_LATE_YEAR_MAX}): " +
          f"ROC-AUC {validation_metrics['roc_auc']:.4f}, PR-AUC {validation_metrics['pr_auc']:.4f}")
    print(f"Test ({len(y_test):,} samples, {test_pos_pct:.3f}% pos): " +
          f"ROC-AUC {test_metrics['roc_auc']:.4f}, PR-AUC {test_metrics['pr_auc']:.4f}")
    print(f"  P@1%: {prec_at_1:.4f} ({test_metrics['lift_at_1pct']:.1f}x), " +
          f"P@5%: {prec_at_5:.4f} ({test_metrics['lift_at_5pct']:.1f}x), " +
          f"P@10%: {prec_at_10:.4f} ({test_metrics['lift_at_10pct']:.1f}x)")
    print(f"Time: train_early {train_early_time/60:.1f}m, val {val_time:.1f}s, " +
          f"final_train {train_time/60:.1f}m, total {total_time/60:.1f}m")
    print("=" * 70)
    
    if use_wandb:
        wandb.log({
            "summary/total_time_seconds": total_time,
            "summary/total_time_minutes": total_time / 60,
            "status": "success"
        })
        wandb.finish()


def main() -> None:
    start_time = time.time()
    repo_root = Path(__file__).resolve().parents[3]
    train_path = resolve_parquet_file("train_early_sampled.parquet")
    val_path = resolve_parquet_file("val_late_full.parquet")
    test_path = resolve_parquet_file("test_full.parquet")
    params_path = resolve_best_params_json()
    output_dir = repo_root / "outputs/Results/ml_models"
    output_dir.mkdir(parents=True, exist_ok=True)
    model_dir = repo_root / "data/ml/models"
    model_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    use_wandb = False
    try:
        wandb.init(
            project="ml-training-transitions",
            entity=os.environ.get("WANDB_ENTITY"),
            name=f"model1_rf_{timestamp}",
            config={
                "model": "RandomForest", "task": "transition_01_prediction",
                "random_state": RANDOM_STATE,
                "temporal_split": {
                    "train_early_max_year": TRAIN_EARLY_YEAR_MAX,
                    "val_late_min_year": VAL_LATE_YEAR_MIN,
                    "val_late_max_year": VAL_LATE_YEAR_MAX,
                },
            },
        )
        use_wandb = True
        print("W&B connected")
    except Exception as err:
        print(f"W&B failed: {err}")
    
    print("=" * 70)
    print("RF TRANSITION MODEL (0→1 PREDICTION)")
    print("=" * 70)
    print(f"Train: {train_path}\nVal: {val_path}\nTest: {test_path}\nParams: {params_path}\nOutput: {output_dir}")
    
    print("\nLoading parameters...")
    best_params, best_cv_score = load_best_params(params_path)
    print(f"Parameters: {', '.join(f'{k}={v}' for k, v in sorted(best_params.items()) if k != 'verbose')}")
    if best_cv_score is not None:
        print(f"Best CV score: {best_cv_score:.4f}")
    
    print("\nSchema checks...")
    all_cols_train = pq.ParquetFile(train_path).schema_arrow.names
    all_cols_test = pq.ParquetFile(test_path).schema_arrow.names
    target_col = "transition_01"
    df_schema = pd.read_parquet(train_path, columns=all_cols_train).head(1)
    numeric_cols = df_schema.select_dtypes(include=['number']).columns.tolist()
    feature_cols = [col for col in numeric_cols if col.lower() not in {c.lower() for c in EXCLUDE_COLS}]
    required_cols = [target_col, 'year', 'row', 'col']
    optional_cols = [col for col in ['x', 'y'] if col in all_cols_train]
    keep_cols_train = feature_cols + required_cols + optional_cols
    keep_cols_test = feature_cols + required_cols + [col for col in ['x', 'y'] if col in all_cols_test]
    print(f"  {len(feature_cols)} features, loading {len(keep_cols_train)} cols from train, {len(keep_cols_test)} from test")
    del df_schema
    gc.collect()
    
    (model_path, validation_metrics, train_stats, train_early_time, 
     val_time, train_time, train_pos, train_neg, n_train_full) = phase1_training(
        train_path, val_path, feature_cols, keep_cols_train, best_params,
        timestamp, model_dir, output_dir, use_wandb, target_col)
    
    phase2_testing(
        test_path, model_path, feature_cols, keep_cols_test, timestamp,
        output_dir, use_wandb, target_col, validation_metrics, train_stats,
        train_early_time, val_time, train_time, train_pos, train_neg,
        n_train_full, best_params, start_time)


if __name__ == "__main__":
    # Set up output file capture
    repo_root = Path(__file__).resolve().parents[3]
    output_dir = repo_root / "outputs/Results/ml_models"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"model1_rf_{timestamp}.txt"
    
    tee = Tee(output_file)
    sys.stdout = tee
    
    try:
        main()
    finally:
        sys.stdout = tee.stdout
        tee.close()
        print(f"\nOutput saved to: {output_file}")
