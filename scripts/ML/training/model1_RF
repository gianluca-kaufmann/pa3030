#!/usr/bin/env python3

"""RF Transition Model (0→1 Prediction)

Train Random Forest to predict transitions to protected areas using temporal split:
- train_early: 2000-2014 (sampled), val_late: 2015-2017 (full), test: 2018-2024 (full)
- Final model trained on combined 2000-2017 data, evaluated on 2018-2024
Input: train_early_sampled.parquet, val_late_full.parquet, test_full.parquet, rf_best_params.json
Output: model.pkl, predictions.parquet, metrics.json
"""

from __future__ import annotations

import gc
import json
import os
import sys
import time
import pickle
from datetime import datetime
from pathlib import Path
from typing import Dict, Any

import numpy as np
import pandas as pd
import wandb
import pyarrow as pa
import pyarrow.parquet as pq
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.metrics import roc_auc_score, average_precision_score
from sklearn.pipeline import Pipeline


class Tee:
    """Write to both stdout and a file."""
    def __init__(self, file_path: Path):
        self.file = open(file_path, 'w', encoding='utf-8')
        self.stdout = sys.stdout
    
    def write(self, text: str) -> None:
        self.stdout.write(text)
        self.file.write(text)
        self.stdout.flush()
        self.file.flush()
    
    def flush(self) -> None:
        self.stdout.flush()
        self.file.flush()
    
    def close(self) -> None:
        self.file.close()


# Configuration
RANDOM_STATE = 42
EXCLUDE_COLS = {'transition_01', 'WDPA_b1', 'WDPA_prev', 'x', 'y', 'row', 'col', 'year'}
TRAIN_EARLY_YEAR_MAX = 2014
VAL_LATE_YEAR_MIN = 2015
VAL_LATE_YEAR_MAX = 2017
FIXED_PARAMS = {'random_state': RANDOM_STATE, 'verbose': 0, 'class_weight': 'balanced_subsample', 'bootstrap': True}
RF_GUARDRAIL_PARAMS = {
    "n_estimators": 500, "max_depth": 12, "min_samples_split": 20, "min_samples_leaf": 10,
    "max_features": "sqrt", "bootstrap": True, "max_samples": 0.2, 
    "class_weight": "balanced_subsample", "verbose": 0,
}
MAX_SAMPLES_FULL_TRAIN = 5_000_000
MAX_SAMPLES_FULL_VAL = 2_000_000

def get_n_jobs() -> int:
    """Get number of CPUs (-1 for all available, may limit for memory efficiency)."""
    slurm_cpus = os.environ.get("SLURM_CPUS_PER_TASK")
    n_jobs = -1
    if slurm_cpus:
        try:
            n_jobs = int(slurm_cpus)
        except ValueError:
            n_jobs = -1
    
    # Check for memory-constrained mode
    memory_constrained = os.environ.get("MEMORY_CONSTRAINED", "").lower() in ("1", "true", "yes")
    if memory_constrained and n_jobs < 0:
        # Limit to fewer cores in memory-constrained environments
        try:
            import os as os_module
            n_jobs = max(1, os_module.cpu_count() // 2)
            print(f"  [MEMORY_CONSTRAINED mode] Limiting to {n_jobs} cores")
        except:
            n_jobs = 4
    
    # Cap at 8 for Random Forest to avoid excessive memory usage
    if n_jobs < 0:
        n_jobs = min(8, os.cpu_count() if hasattr(os, 'cpu_count') else 8)
    else:
        n_jobs = min(n_jobs, 8)
    
    return n_jobs


def report_memory_usage(label: str = "") -> None:
    """Report current memory usage."""
    try:
        import psutil
        process = psutil.Process()
        mem_info = process.memory_info()
        mem_gb = mem_info.rss / 1024**3
        print(f"  [Memory {label}] RSS: {mem_gb:.2f} GB")
    except ImportError:
        pass  # psutil not available


def convert_numpy_types(obj: Any) -> Any:
    """Recursively convert NumPy types to native Python types for JSON serialization."""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [convert_numpy_types(item) for item in obj]
    elif obj is None or isinstance(obj, (str, int, float, bool)):
        return obj
    else:
        try:
            return obj.item() if hasattr(obj, 'item') else obj
        except (ValueError, TypeError, AttributeError):
            return str(obj)


def compute_precision_at_k(y_true: np.ndarray, y_proba: np.ndarray, k: float) -> float:
    """Compute precision at top k% of predictions."""
    n_top_k = max(1, int(len(y_true) * k / 100))
    top_k_idx = np.argsort(y_proba)[-n_top_k:]
    return y_true[top_k_idx].sum() / n_top_k


def resolve_parquet_file(filename: str) -> Path:
    """Locate parquet file (prefer $SCRATCH if present)."""
    repo_root = Path(__file__).resolve().parents[3]
    scratch_root = Path(os.environ["SCRATCH"]) if os.environ.get("SCRATCH") else None

    candidates = []
    if scratch_root is not None:
        candidates.append(scratch_root / f"data/ml/{filename}")
    candidates.append(repo_root / f"data/ml/{filename}")

    for cand in candidates:
        if cand.exists():
            return cand

    raise FileNotFoundError(f"{filename} not found in expected locations")


def resolve_best_params_json() -> Path:
    """Locate rf_best_params.json (in same directory as this script)."""
    script_dir = Path(__file__).resolve().parent
    params_path = script_dir / "rf_best_params.json"
    
    if not params_path.exists():
        raise FileNotFoundError(f"rf_best_params.json not found at {params_path}")
    
    return params_path


def load_best_params(params_path: Path) -> Dict[str, Any]:
    """Load best parameters from JSON and merge with fixed params."""
    with open(params_path, 'r') as f:
        params_data = json.load(f)
    best_params = params_data.get('best_params', {})
    json_fixed_params = params_data.get('fixed_params', {})
    cleaned_params = {(key[4:] if key.startswith('rf__') else key): value for key, value in best_params.items()}
    final_params = {**FIXED_PARAMS, **json_fixed_params, **cleaned_params}
    final_params.update(RF_GUARDRAIL_PARAMS)
    final_params["random_state"] = RANDOM_STATE
    final_params["n_jobs"] = get_n_jobs()
    final_params.pop("criterion", None)
    return final_params, params_data.get('best_cv_score', None)


def downcast_numeric_dtypes(df: pd.DataFrame) -> None:
    """Downcast numeric dtypes to reduce memory usage (in-place)."""
    for col in df.select_dtypes(include=['float64']).columns:
        df[col] = df[col].astype('float32', copy=False)
    for col in df.select_dtypes(include=['int64']).columns:
        df[col] = df[col].astype('int32', copy=False)


def load_numpy_arrays_two_pass(
    df_path: Path, 
    feature_cols: list, 
    target_col: str, 
    year_filter, 
    name: str = "",
    batch_size: int = 50_000,
    max_samples: int | None = None
) -> tuple[np.ndarray, np.ndarray, int, int, list]:
    """
    Two-pass NumPy loader: bypasses Pandas to minimize memory overhead.
    
    Pass 1: Count rows matching filter (after dropna on target).
    Pass 2: Pre-allocate float32 arrays and stream data directly into them.
    If max_samples is provided, performs stratified sampling (keep all positives, sample negatives).
    
    Returns:
        X: Feature matrix (n_samples, n_features) as float32
        y: Target vector (n_samples,) as int8
        pos: Number of positive samples
        neg: Number of negative samples
        years: Sorted list of unique years
    """
    print(f"\nLoading {name or df_path.name} (two-pass NumPy loader)...")
    load_start = time.time()
    essential_cols = feature_cols + [target_col, 'year']
    
    # Pass 1: Count rows matching filter
    print(f"  Pass 1: Counting rows matching filter...")
    parquet_file = pq.ParquetFile(df_path)
    n_samples = 0
    years_set = set()
    pos_count = 0
    
    try:
        for batch in parquet_file.iter_batches(batch_size=batch_size, columns=essential_cols):
            # Convert batch to numpy arrays directly (bypass pandas)
            batch_table = batch
            target_array = batch_table[target_col].to_numpy(zero_copy_only=False)
            year_array = batch_table['year'].to_numpy(zero_copy_only=False)
            
            # Filter out NaN in target
            valid_mask = ~np.isnan(target_array)
            if not valid_mask.any():
                continue
            
            target_valid = target_array[valid_mask]
            year_valid = year_array[valid_mask]
            
            # Apply year filter (works directly on numpy arrays)
            year_mask = year_filter(year_valid)
            n_samples += int(year_mask.sum())
            pos_count += int((target_valid[year_mask] > 0).sum())
            years_set.update(year_valid[year_mask].tolist())
            
            del batch_table, target_array, year_array, target_valid, year_valid, batch
    finally:
        del parquet_file
        gc.collect()
    
    print(f"  Found {n_samples:,} samples matching filter ({pos_count:,} positives)")
    
    if n_samples == 0:
        raise ValueError(f"No samples found matching filter in {name or df_path.name}")
    
    # Handle max_samples: if needed, we'll collect all data first, then sample
    # This is less memory-efficient but necessary for stratified sampling
    needs_sampling = max_samples is not None and n_samples > max_samples
    if needs_sampling:
        print(f"  Sampling required: {n_samples:,} > {max_samples:,} - will collect then sample")
        # We'll collect all matching data, then sample
        # This requires more memory but ensures proper stratified sampling
    
    # Pass 2: Pre-allocate arrays and fill directly
    if needs_sampling:
        # For sampling, we need to collect all data first
        print(f"  Pass 2a: Collecting all matching data for sampling...")
        collected_X = []
        collected_y = []
        
        parquet_file = pq.ParquetFile(df_path)
        batch_num = 0
        
        try:
            for batch in parquet_file.iter_batches(batch_size=batch_size, columns=essential_cols):
                batch_num += 1
                batch_table = batch
                
                target_array = batch_table[target_col].to_numpy(zero_copy_only=False)
                year_array = batch_table['year'].to_numpy(zero_copy_only=False)
                
                valid_mask = ~np.isnan(target_array)
                if not valid_mask.any():
                    del batch_table, target_array, year_array, batch
                    continue
                
                target_valid = target_array[valid_mask]
                year_valid = year_array[valid_mask]
                year_mask = year_filter(year_valid)
                
                if not year_mask.any():
                    del batch_table, target_array, year_array, target_valid, year_valid, batch
                    continue
                
                target_filtered = target_valid[year_mask]
                year_filtered = year_valid[year_mask]
                
                feature_arrays = []
                for col in feature_cols:
                    col_array = batch_table[col].to_numpy(zero_copy_only=False)
                    col_valid = col_array[valid_mask]
                    col_filtered = col_valid[year_mask]
                    col_float32 = col_filtered.astype(np.float32)
                    col_float32 = np.nan_to_num(col_float32, nan=0.0, posinf=0.0, neginf=0.0)
                    feature_arrays.append(col_float32)
                
                X_batch = np.column_stack(feature_arrays)
                y_batch = (target_filtered > 0).astype(np.int8)
                
                collected_X.append(X_batch)
                collected_y.append(y_batch)
                
                del batch_table, target_array, year_array, target_valid, year_valid, target_filtered, year_filtered
                del feature_arrays, X_batch, y_batch, batch
                gc.collect()
                
                if batch_num % 10 == 0:
                    print(f"  {batch_num} batches collected")
                    report_memory_usage(f"collect batch {batch_num}")
        finally:
            del parquet_file
            gc.collect()
        
        # Concatenate collected data
        print(f"  Pass 2b: Concatenating collected data...")
        X_all = np.vstack(collected_X).astype(np.float32)
        y_all = np.concatenate(collected_y).astype(np.int8)
        del collected_X, collected_y
        gc.collect()
        
        # Perform stratified sampling
        print(f"  Pass 2c: Performing stratified sampling...")
        pos_mask = y_all == 1
        pos_indices = np.where(pos_mask)[0]
        neg_indices = np.where(~pos_mask)[0]
        
        n_pos = len(pos_indices)
        n_neg_needed = max_samples - n_pos
        if n_neg_needed < 0:
            print(f"  Warning: {n_pos:,} positives exceed max_samples={max_samples:,}, keeping all positives")
            n_neg_needed = 0
        
        if n_neg_needed > 0 and len(neg_indices) > 0:
            n_neg_to_sample = min(n_neg_needed, len(neg_indices))
            rng = np.random.RandomState(RANDOM_STATE)
            sampled_neg_indices = rng.choice(neg_indices, size=n_neg_to_sample, replace=False)
            selected_indices = np.concatenate([pos_indices, sampled_neg_indices])
        else:
            selected_indices = pos_indices
        
        # Shuffle
        rng = np.random.RandomState(RANDOM_STATE)
        rng.shuffle(selected_indices)
        
        # Extract sampled data
        X = X_all[selected_indices].astype(np.float32)
        y = y_all[selected_indices].astype(np.int8)
        del X_all, y_all, pos_indices, neg_indices, selected_indices
        gc.collect()
        
        n_samples = len(X)
        print(f"  Sampled to {n_samples:,} samples")
    else:
        # No sampling needed - use direct streaming approach
        print(f"  Pass 2: Pre-allocating arrays ({n_samples:,} samples, {len(feature_cols)} features)...")
        X = np.empty((n_samples, len(feature_cols)), dtype=np.float32)
        y = np.empty(n_samples, dtype=np.int8)
        
        parquet_file = pq.ParquetFile(df_path)
        idx_offset = 0
        batch_num = 0
        
        try:
            for batch in parquet_file.iter_batches(batch_size=batch_size, columns=essential_cols):
                batch_num += 1
                batch_table = batch
                
                target_array = batch_table[target_col].to_numpy(zero_copy_only=False)
                year_array = batch_table['year'].to_numpy(zero_copy_only=False)
                
                valid_mask = ~np.isnan(target_array)
                if not valid_mask.any():
                    del batch_table, target_array, year_array, batch
                    continue
                
                target_valid = target_array[valid_mask]
                year_valid = year_array[valid_mask]
                year_mask = year_filter(year_valid)
                
                if not year_mask.any():
                    del batch_table, target_array, year_array, target_valid, year_valid, batch
                    continue
                
                target_filtered = target_valid[year_mask]
                year_filtered = year_valid[year_mask]
                
                feature_arrays = []
                for col in feature_cols:
                    col_array = batch_table[col].to_numpy(zero_copy_only=False)
                    col_valid = col_array[valid_mask]
                    col_filtered = col_valid[year_mask]
                    col_float32 = col_filtered.astype(np.float32)
                    col_float32 = np.nan_to_num(col_float32, nan=0.0, posinf=0.0, neginf=0.0)
                    feature_arrays.append(col_float32)
                
                X_batch = np.column_stack(feature_arrays)
                y_batch = (target_filtered > 0).astype(np.int8)
                
                batch_size_actual = len(y_batch)
                if idx_offset + batch_size_actual > n_samples:
                    batch_size_actual = n_samples - idx_offset
                    X_batch = X_batch[:batch_size_actual]
                    y_batch = y_batch[:batch_size_actual]
                
                X[idx_offset:idx_offset + batch_size_actual, :] = X_batch
                y[idx_offset:idx_offset + batch_size_actual] = y_batch
                idx_offset += batch_size_actual
                
                if idx_offset >= n_samples:
                    break
                
                del batch_table, target_array, year_array, target_valid, year_valid, target_filtered, year_filtered
                del feature_arrays, X_batch, y_batch, batch
                gc.collect()
                
                if batch_num % 10 == 0:
                    print(f"  {batch_num} batches, {idx_offset:,}/{n_samples:,} samples loaded")
                    report_memory_usage(f"load batch {batch_num}")
        finally:
            del parquet_file
            gc.collect()
        
        if idx_offset < n_samples:
            X = X[:idx_offset]
            y = y[:idx_offset]
            n_samples = idx_offset
    
    # Compute stats
    pos = int(y.sum())
    neg = int((y == 0).sum())
    years = sorted(years_set)
    
    load_time = time.time() - load_start
    mem_gb = (X.nbytes + y.nbytes) / 1024**3
    pos_pct = pos / len(y) * 100 if len(y) > 0 else 0
    print(f"  Loaded {n_samples:,} rows in {load_time:.1f}s ({mem_gb:.2f} GB)")
    print(f"  {neg:,} neg ({100-pos_pct:.3f}%), {pos:,} pos ({pos_pct:.3f}%), ratio 1:{neg/max(pos,1):.1f}, years {years[0]}-{years[-1]}")
    
    return X, y, pos, neg, years


def compute_metrics(y_true: np.ndarray, y_proba: np.ndarray) -> Dict[str, float]:
    """Compute all validation/test metrics."""
    roc_auc = roc_auc_score(y_true, y_proba)
    pr_auc = average_precision_score(y_true, y_proba)
    baseline_rate = y_true.mean()
    prec_at_1 = compute_precision_at_k(y_true, y_proba, 1)
    prec_at_5 = compute_precision_at_k(y_true, y_proba, 5)
    prec_at_10 = compute_precision_at_k(y_true, y_proba, 10)
    return {
        "roc_auc": float(roc_auc), "pr_auc": float(pr_auc),
        "precision_at_1pct": float(prec_at_1),
        "precision_at_5pct": float(prec_at_5),
        "precision_at_10pct": float(prec_at_10),
        "baseline_rate": float(baseline_rate),
        "lift_at_1pct": float(prec_at_1 / baseline_rate) if baseline_rate > 0 else 0,
        "lift_at_5pct": float(prec_at_5 / baseline_rate) if baseline_rate > 0 else 0,
        "lift_at_10pct": float(prec_at_10 / baseline_rate) if baseline_rate > 0 else 0,
    }


# =============================================================================
# Main Pipeline
# =============================================================================

def phase1_training(
    train_path: Path,
    val_path: Path,
    feature_cols: list,
    keep_cols_train: list,
    best_params: Dict[str, Any],
    timestamp: str,
    model_dir: Path,
    output_dir: Path,
    use_wandb: bool,
    target_col: str,
) -> tuple[Path, Dict[str, Any], Dict[str, Any], float, float, float, float, int, int]:
    """Phase 1: Training phase - loads train/val splits, trains model, saves it."""
    print("\n" + "=" * 70)
    print("PHASE 1: TRAINING")
    print("=" * 70)
    report_memory_usage("start of Phase 1")
    
    print(f"\nSTEP A: Train on train_early (year <= {TRAIN_EARLY_YEAR_MAX})")
    X_train_early, y_train_early, train_early_pos, train_early_neg, train_years = load_numpy_arrays_two_pass(
        train_path, feature_cols, target_col, 
        lambda y: y <= TRAIN_EARLY_YEAR_MAX, "train_early_sampled.parquet",
        max_samples=None
    )
    report_memory_usage("after STEP A")
    
    # Convert to DataFrame for sklearn Pipeline (SimpleImputer expects DataFrame)
    # But we'll keep it as numpy for memory efficiency - SimpleImputer works with numpy too
    print(f"Training on {len(X_train_early):,} samples...")
    train_early_start = time.time()
    report_memory_usage("before training")
    
    # Fit imputer first to get median values
    imputer = SimpleImputer(strategy='median')
    X_train_early_imputed = imputer.fit_transform(X_train_early)
    del X_train_early
    gc.collect()
    report_memory_usage("after imputation")
    
    rf_model = RandomForestClassifier(**best_params)
    rf_model.fit(X_train_early_imputed, y_train_early)
    train_early_time = time.time() - train_early_start
    print(f"  Completed in {train_early_time/60:.1f} min")
    
    # Create pipeline for consistency (though we fit separately)
    pipeline = Pipeline([('imputer', imputer), ('rf', rf_model)])
    model_train_early = pipeline
    
    del X_train_early_imputed, y_train_early
    gc.collect()
    report_memory_usage("after STEP A training")
    
    print(f"\nSTEP B: Validate on val_late ({VAL_LATE_YEAR_MIN}-{VAL_LATE_YEAR_MAX})")
    X_val_late, y_val_late, val_late_pos, val_late_neg, _ = load_numpy_arrays_two_pass(
        val_path, feature_cols, target_col,
        lambda y: (y >= VAL_LATE_YEAR_MIN) & (y <= VAL_LATE_YEAR_MAX), "val_late_full.parquet",
        max_samples=None
    )
    report_memory_usage("after STEP B")
    
    val_start = time.time()
    y_val_proba = model_train_early.predict_proba(X_val_late)[:, 1]
    val_time = time.time() - val_start
    
    validation_metrics = compute_metrics(y_val_late, y_val_proba)
    print(f"Validation: ROC-AUC {validation_metrics['roc_auc']:.4f}, PR-AUC {validation_metrics['pr_auc']:.4f}, " +
          f"P@1% {validation_metrics['precision_at_1pct']:.4f}, P@5% {validation_metrics['precision_at_5pct']:.4f}, " +
          f"P@10% {validation_metrics['precision_at_10pct']:.4f}")
    
    if use_wandb:
        wandb.log({
            **{f"val/{k}": v for k, v in validation_metrics.items()},
            "val/n_samples": int(len(y_val_late)),
            "val/n_positives": int(y_val_late.sum()),
            "val/time_seconds": val_time,
        })
    
    del X_val_late, y_val_late, model_train_early
    gc.collect()
    report_memory_usage("after STEP B validation")
    
    print(f"\nSTEP D: Train final model on full data (2000-2017, capped at {MAX_SAMPLES_FULL_TRAIN:,} train + {MAX_SAMPLES_FULL_VAL:,} val)")
    
    X_train_part, y_train_part, train_part_pos, train_part_neg, train_part_years = load_numpy_arrays_two_pass(
        train_path, feature_cols, target_col,
        lambda y: y <= TRAIN_EARLY_YEAR_MAX, "train_early_sampled.parquet",
        max_samples=MAX_SAMPLES_FULL_TRAIN
    )
    report_memory_usage("after loading train_part")
    
    X_val_part, y_val_part, val_part_pos, val_part_neg, val_part_years = load_numpy_arrays_two_pass(
        val_path, feature_cols, target_col,
        lambda y: (y >= VAL_LATE_YEAR_MIN) & (y <= VAL_LATE_YEAR_MAX), "val_late_full.parquet",
        max_samples=MAX_SAMPLES_FULL_VAL
    )
    report_memory_usage("after loading val_part")
    
    # Concatenate using numpy (more memory efficient than pandas)
    print(f"  Concatenating train and val parts...")
    X = np.vstack([X_train_part, X_val_part]).astype(np.float32)
    y = np.concatenate([y_train_part, y_val_part]).astype(np.int8)
    del X_train_part, X_val_part, y_train_part, y_val_part
    gc.collect()
    report_memory_usage("after concatenation")
    
    train_pos = train_part_pos + val_part_pos
    train_neg = train_part_neg + val_part_neg
    n_train_full = len(X)
    train_pos_pct = train_pos / n_train_full * 100
    train_years = sorted(set(train_part_years) | set(val_part_years))
    
    print(f"Full training set: {n_train_full:,} samples ({train_pos:,} pos, {train_neg:,} neg, {train_pos_pct:.3f}%), " +
          f"years {train_years[0] if train_years else 'none'}-{train_years[-1] if train_years else 'none'}")
    
    train_stats = {
        "n_train": n_train_full, "train_positives": train_pos, "train_negatives": train_neg,
        "train_positive_pct": train_pos_pct, 
        "train_years": f"{train_years[0]}-{train_years[-1]}" if train_years else "none",
        "max_samples_full_train": MAX_SAMPLES_FULL_TRAIN,
        "max_samples_full_val": MAX_SAMPLES_FULL_VAL,
    }
    
    if use_wandb:
        wandb.log({
            "data/n_features": len(feature_cols), "data/n_train": n_train_full,
            "data/train_positives": train_pos, "data/train_negatives": train_neg,
            "data/train_positive_pct": train_pos_pct,
            "data/max_samples_full_train": MAX_SAMPLES_FULL_TRAIN,
            "data/max_samples_full_val": MAX_SAMPLES_FULL_VAL,
        })
    
    print(f"Training on {len(X):,} samples (2000-2017)...")
    train_start = time.time()
    report_memory_usage("before final training")
    
    # Fit imputer and model
    imputer_final = SimpleImputer(strategy='median')
    X_imputed = imputer_final.fit_transform(X)
    del X
    gc.collect()
    report_memory_usage("after final imputation")
    
    rf_model_final = RandomForestClassifier(**best_params)
    rf_model_final.fit(X_imputed, y)
    train_time = time.time() - train_start
    print(f"  Completed in {train_time/60:.1f} min")
    
    # Create pipeline
    final_model = Pipeline([('imputer', imputer_final), ('rf', rf_model_final)])
    
    del X_imputed, y
    gc.collect()
    report_memory_usage("after final training")
    
    model_path = model_dir / f"model1_rf_{timestamp}.pkl"
    with open(model_path, 'wb') as f:
        pickle.dump(final_model, f)
    print(f"  Model saved: {model_path}")
    
    if use_wandb:
        wandb.log({
            "training/train_early_time_seconds": train_early_time,
            "training/train_early_time_minutes": train_early_time / 60,
            "training/final_train_time_seconds": train_time,
            "training/final_train_time_minutes": train_time / 60,
        })
    
    del final_model
    gc.collect()
    print("Phase 1 completed.")
    report_memory_usage("end of Phase 1")
    
    return (model_path, validation_metrics, train_stats, train_early_time, 
            val_time, train_time, train_pos, train_neg, n_train_full)


def phase2_testing(
    test_path: Path, model_path: Path, feature_cols: list, keep_cols_test: list,
    timestamp: str, output_dir: Path, use_wandb: bool, target_col: str,
    validation_metrics: Dict[str, Any], train_stats: Dict[str, Any],
    train_early_time: float, val_time: float, train_time: float,
    train_pos: int, train_neg: int, n_train_full: int,
    best_params: Dict[str, Any], start_time: float,
) -> None:
    """Phase 2: Testing phase - batched inference to prevent OOM errors."""
    print("\n" + "=" * 70)
    print("PHASE 2: TESTING")
    print("=" * 70)
    report_memory_usage("start of Phase 2")
    
    print(f"\nLoading model from: {model_path}")
    with open(model_path, 'rb') as f:
        final_model = pickle.load(f)
    report_memory_usage("after loading model")
    
    # Use smaller batch size to reduce memory pressure
    batch_size = 50_000
    print(f"Processing test in batches of {batch_size:,}...")
    
    # Pass 1: Count rows (pre-allocate arrays to avoid concatenation memory spike)
    print(f"  Pass 1: Counting test rows...")
    parquet_file = pq.ParquetFile(test_path)
    n_test_total = 0
    test_years_set = set()
    
    try:
        for batch in parquet_file.iter_batches(batch_size=batch_size, columns=[target_col, 'year']):
            batch_table = batch
            target_array = batch_table[target_col].to_numpy(zero_copy_only=False)
            year_array = batch_table['year'].to_numpy(zero_copy_only=False)
            
            # Filter out NaN in target
            valid_mask = ~np.isnan(target_array)
            if valid_mask.any():
                n_test_total += int(valid_mask.sum())
                test_years_set.update(year_array[valid_mask].tolist())
            
            del batch_table, target_array, year_array, batch
    finally:
        del parquet_file
        gc.collect()
    
    print(f"  Found {n_test_total:,} test samples")
    
    if n_test_total == 0:
        raise ValueError("No test samples found")
    
    # Pre-allocate arrays
    print(f"  Pre-allocating arrays for {n_test_total:,} samples...")
    y_test = np.empty(n_test_total, dtype=np.int8)
    y_proba = np.empty(n_test_total, dtype=np.float32)
    report_memory_usage("after pre-allocation")
    
    # Pass 2: Process batches and fill pre-allocated arrays
    parquet_file = pq.ParquetFile(test_path)
    scored_path = output_dir / f"model1_rf_scored_{timestamp}.parquet"
    writer = None
    test_pos = test_neg = batch_num = idx_offset = 0
    pred_start = time.time()
    
    try:
        for batch in parquet_file.iter_batches(batch_size=batch_size, columns=keep_cols_test):
            batch_num += 1
            batch_df = batch.to_pandas().dropna(subset=[target_col])
            if len(batch_df) == 0:
                del batch_df, batch
                gc.collect()
                continue
            
            downcast_numeric_dtypes(batch_df)
            y_batch = (batch_df[target_col] > 0).astype(np.int8)
            X_batch = batch_df[feature_cols].copy()
            
            # Get predictions
            y_proba_batch = final_model.predict_proba(X_batch)[:, 1].astype(np.float32)
            
            # Fill pre-allocated arrays directly
            batch_size_actual = len(y_batch)
            y_test[idx_offset:idx_offset + batch_size_actual] = y_batch.values
            y_proba[idx_offset:idx_offset + batch_size_actual] = y_proba_batch
            idx_offset += batch_size_actual
            
            test_pos += int(y_batch.sum())
            test_neg += int((y_batch == 0).sum())
            
            result_dict = {
                'row': batch_df['row'].values, 'col': batch_df['col'].values,
                'year': batch_df['year'].values, 'y_true': y_batch.values,
                'y_pred_proba': y_proba_batch,
            }
            for col in ['x', 'y']:
                if col in batch_df.columns:
                    result_dict[col] = batch_df[col].values
            
            result_df = pd.DataFrame(result_dict)
            result_table = pa.Table.from_pandas(result_df, preserve_index=False)
            
            if writer is None:
                writer = pq.ParquetWriter(scored_path, result_table.schema)
                print(f"Writing to {scored_path}")
            
            writer.write_table(result_table)
            
            del batch_df, X_batch, y_batch, y_proba_batch, result_df, result_table, batch
            # More aggressive garbage collection during batch processing
            gc.collect()
            
            if batch_num % 10 == 0:
                print(f"  {batch_num} batches, {idx_offset:,}/{n_test_total:,} rows")
                report_memory_usage(f"batch {batch_num}")
            elif batch_num % 50 == 0:
                # Extra cleanup every 50 batches
                gc.collect()
    finally:
        if writer is not None:
            writer.close()
        del parquet_file
        gc.collect()
    
    if idx_offset != n_test_total:
        raise ValueError(f"Mismatch: expected {n_test_total:,} samples but processed {idx_offset:,}")
    
    pred_time = time.time() - pred_start
    print(f"Completed {batch_num} batches in {pred_time:.1f}s, {n_test_total:,} rows")
    report_memory_usage("after all predictions")
    
    test_years = sorted(test_years_set)
    del test_years_set
    gc.collect()
    test_pos_pct = test_pos / len(y_test) * 100
    print(f"\nTest set: {test_neg:,} neg, {test_pos:,} pos ({test_pos_pct:.3f}%), years {test_years[0]}-{test_years[-1]}")
    
    if use_wandb:
        wandb.log({
            "data/n_test": int(len(y_test)),
            "data/test_positives": test_pos,
            "data/test_negatives": test_neg,
            "data/test_positive_pct": test_pos_pct,
        })
    
    print("\nComputing metrics...")
    test_metrics = compute_metrics(y_test, y_proba)
    prec_at_1, prec_at_5, prec_at_10 = test_metrics['precision_at_1pct'], test_metrics['precision_at_5pct'], test_metrics['precision_at_10pct']
    
    print(f"Test: ROC-AUC {test_metrics['roc_auc']:.4f}, PR-AUC {test_metrics['pr_auc']:.4f}")
    print(f"  P@1%: {prec_at_1:.4f} ({test_metrics['lift_at_1pct']:.2f}x), " +
          f"P@5%: {prec_at_5:.4f} ({test_metrics['lift_at_5pct']:.2f}x), " +
          f"P@10%: {prec_at_10:.4f} ({test_metrics['lift_at_10pct']:.2f}x)")
    
    if use_wandb:
        wandb.log({**{f"test/{k}": v for k, v in test_metrics.items()}})
    
    rf_model = final_model.named_steps['rf']
    feature_importance = pd.DataFrame({
        'feature': feature_cols, 'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False).head(20)
    print("\nTop 20 Features:")
    print(feature_importance.to_string(index=False))
    
    print("\nSaving metrics...")
    metrics = {
        "metadata": {
            "timestamp": timestamp,
            "model": "RandomForest",
            "task": "transition_01_prediction",
            "random_state": RANDOM_STATE,
            "n_features": len(feature_cols),
            "features": feature_cols,
        },
        "data": {
            "n_train_full": n_train_full,
            "n_test": int(len(y_test)),
            "train_positives": train_pos,
            "train_negatives": train_neg,
            "test_positives": test_pos,
            "test_negatives": test_neg,
        },
        "temporal_split": {
            "method": "strict_temporal_split",
            "train_early_max_year": TRAIN_EARLY_YEAR_MAX,
            "val_late_min_year": VAL_LATE_YEAR_MIN,
            "val_late_max_year": VAL_LATE_YEAR_MAX,
        },
        "validation_performance": validation_metrics,
        "model_parameters": best_params,
        "test_performance": test_metrics,
        "feature_importance": feature_importance.head(20).to_dict('records'),
        "timing": {
            "train_early_seconds": train_early_time,
            "validation_seconds": val_time,
            "final_training_seconds": train_time,
            "prediction_seconds": pred_time,
            "total_seconds": time.time() - start_time,
        },
    }
    
    metrics = convert_numpy_types(metrics)
    metrics_path = output_dir / f"model1_rf_metrics_{timestamp}.json"
    with open(metrics_path, 'w') as f:
        json.dump(metrics, f, indent=2)
    print(f"  Saved to: {metrics_path}")
    
    total_time = time.time() - start_time
    print("\n" + "=" * 70)
    print("SUMMARY")
    print("=" * 70)
    print(f"Model: Random Forest ({len(feature_cols)} features)")
    print(f"Validation (≤{TRAIN_EARLY_YEAR_MAX} → {VAL_LATE_YEAR_MIN}-{VAL_LATE_YEAR_MAX}): " +
          f"ROC-AUC {validation_metrics['roc_auc']:.4f}, PR-AUC {validation_metrics['pr_auc']:.4f}")
    print(f"Test ({len(y_test):,} samples, {test_pos_pct:.3f}% pos): " +
          f"ROC-AUC {test_metrics['roc_auc']:.4f}, PR-AUC {test_metrics['pr_auc']:.4f}")
    print(f"  P@1%: {prec_at_1:.4f} ({test_metrics['lift_at_1pct']:.1f}x), " +
          f"P@5%: {prec_at_5:.4f} ({test_metrics['lift_at_5pct']:.1f}x), " +
          f"P@10%: {prec_at_10:.4f} ({test_metrics['lift_at_10pct']:.1f}x)")
    print(f"Time: train_early {train_early_time/60:.1f}m, val {val_time:.1f}s, " +
          f"final_train {train_time/60:.1f}m, total {total_time/60:.1f}m")
    print("=" * 70)
    
    if use_wandb:
        wandb.log({
            "summary/total_time_seconds": total_time,
            "summary/total_time_minutes": total_time / 60,
            "status": "success"
        })
        wandb.finish()


def main() -> None:
    start_time = time.time()
    repo_root = Path(__file__).resolve().parents[3]
    train_path = resolve_parquet_file("train_early_sampled.parquet")
    val_path = resolve_parquet_file("val_late_full.parquet")
    test_path = resolve_parquet_file("test_full.parquet")
    params_path = resolve_best_params_json()
    output_dir = repo_root / "outputs/Results/ml_models"
    output_dir.mkdir(parents=True, exist_ok=True)
    model_dir = repo_root / "data/ml/models"
    model_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    use_wandb = False
    try:
        wandb.init(
            project="ml-training-transitions",
            entity=os.environ.get("WANDB_ENTITY"),
            name=f"model1_rf_{timestamp}",
            config={
                "model": "RandomForest", "task": "transition_01_prediction",
                "random_state": RANDOM_STATE,
                "temporal_split": {
                    "train_early_max_year": TRAIN_EARLY_YEAR_MAX,
                    "val_late_min_year": VAL_LATE_YEAR_MIN,
                    "val_late_max_year": VAL_LATE_YEAR_MAX,
                },
            },
        )
        use_wandb = True
        print("W&B connected")
    except Exception as err:
        print(f"W&B failed: {err}")
    
    print("=" * 70)
    print("RF TRANSITION MODEL (0→1 PREDICTION)")
    print("=" * 70)
    print(f"Train: {train_path}\nVal: {val_path}\nTest: {test_path}\nParams: {params_path}\nOutput: {output_dir}")
    
    print("\nLoading parameters...")
    best_params, best_cv_score = load_best_params(params_path)
    print(f"Parameters: {', '.join(f'{k}={v}' for k, v in sorted(best_params.items()) if k != 'verbose')}")
    if best_cv_score is not None:
        print(f"Best CV score: {best_cv_score:.4f}")
    
    print("\nSchema checks...")
    all_cols_train = pq.ParquetFile(train_path).schema_arrow.names
    all_cols_test = pq.ParquetFile(test_path).schema_arrow.names
    target_col = "transition_01"
    # Build feature columns from PyArrow schema only (avoid loading full parquet into pandas)
    schema = pq.ParquetFile(train_path).schema_arrow
    numeric_cols = [
        name
        for name, field in zip(schema.names, schema)
        if pa.types.is_integer(field.type) or pa.types.is_floating(field.type)
    ]
    # Exclude target and coordinates from features (year is included as a feature)
    feature_cols = [c for c in numeric_cols if c not in EXCLUDE_COLS]
    required_cols = [target_col, 'year', 'row', 'col']
    optional_cols = [col for col in ['x', 'y'] if col in all_cols_train]
    keep_cols_train = feature_cols + required_cols + optional_cols
    keep_cols_test = feature_cols + required_cols + [col for col in ['x', 'y'] if col in all_cols_test]
    print(f"Selected {len(feature_cols)} features, loading {len(keep_cols_train)} cols from train, {len(keep_cols_test)} from test")
    gc.collect()
    
    (model_path, validation_metrics, train_stats, train_early_time, 
     val_time, train_time, train_pos, train_neg, n_train_full) = phase1_training(
        train_path, val_path, feature_cols, keep_cols_train, best_params,
        timestamp, model_dir, output_dir, use_wandb, target_col)
    
    phase2_testing(
        test_path, model_path, feature_cols, keep_cols_test, timestamp,
        output_dir, use_wandb, target_col, validation_metrics, train_stats,
        train_early_time, val_time, train_time, train_pos, train_neg,
        n_train_full, best_params, start_time)


if __name__ == "__main__":
    # Set up output file capture
    repo_root = Path(__file__).resolve().parents[3]
    output_dir = repo_root / "outputs/Results/ml_models"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"model1_rf_{timestamp}.txt"
    
    tee = Tee(output_file)
    sys.stdout = tee
    
    try:
        main()
    finally:
        sys.stdout = tee.stdout
        tee.close()
        print(f"\nOutput saved to: {output_file}")
