#!/usr/bin/env python3

"""Baseline Suitability Model for Protected Area Prediction

Purpose: Estimate how "protection-like" each pixel looked in year 2000.
         A model trained only on 2000 (protected vs unprotected) learns the
         spatial signature of protected areas. Its predicted probability for
         each unprotected pixel becomes a suitability score. We then check
         whether high-suitability pixels actually became protected by 2024.

Input:   `data/ml/merged_panel_2000_2024.parquet`
Output:  - Trained LightGBM model (baseline_lgbm_2000.txt)
         - Suitability scores (baseline_scores_2000.parquet)
         - Evaluation metrics (baseline_lgbm_metrics.json)
         - Scored outcome table (baseline_scored_outcomes.parquet)
"""

from __future__ import annotations

import gc
import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Optional

import numpy as np
import pandas as pd
import lightgbm as lgb
import wandb
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_auc_score,
    average_precision_score,
)
from sklearn.model_selection import train_test_split


class Tee:
    """Write to both stdout and a file."""
    def __init__(self, file_path: Path):
        self.file = open(file_path, 'w', encoding='utf-8')
        self.stdout = sys.stdout
    
    def write(self, text: str) -> None:
        self.stdout.write(text)
        self.file.write(text)
        self.file.flush()
    
    def flush(self) -> None:
        self.stdout.flush()
        self.file.flush()
    
    def close(self) -> None:
        self.file.close()


# =============================================================================
# Configuration
# =============================================================================
RANDOM_STATE = 42
TEST_SIZE = 0.2

# Columns to exclude from features (prevent data leakage)
EXCLUDE_COLS = {
    # Target and derived fields
    "WDPA_b1",          # Target variable (protection status)
    "is_protected_2000", # Target variable (derived from WDPA_b1)
    "dist_wdpa",        # Derived from WDPA (distance to protected area)
    "dist_wdpa_decay",  # Derived from WDPA (decay function)
    # Identifiers and coordinates
    "row",
    "col",
    "x",
    "y",
    "year",
}

# LightGBM parameters (increased flexibility for complex patterns)
LGBM_PARAMS = {
    'objective': 'binary',
    'metric': 'auc',
    'boosting_type': 'gbdt',
    'num_leaves': 511,          # Increased from 255 for more flexibility
    'learning_rate': 0.05,
    'feature_fraction': 0.8,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'max_depth': -1,            # Unlimited depth (num_leaves controls complexity)
    'min_child_samples': 20,    # Reduced from 100 for more granular splits
    'min_data_in_leaf': 20,     # Additional constraint for leaf size
    'random_state': RANDOM_STATE,
    'n_jobs': -1,
    'verbose': -1,
}


# =============================================================================
# Utility Functions
# =============================================================================

def convert_numpy_types(obj):
    """Recursively convert NumPy types to native Python types for JSON serialization.
    
    Handles:
    - NumPy scalars (int64, float64, etc.) -> int, float
    - NumPy arrays -> lists
    - Dicts and lists (recursively)
    - None, bool, str pass through unchanged
    
    Args:
        obj: Object that may contain NumPy types
        
    Returns:
        Object with all NumPy types converted to native Python types
    """
    if isinstance(obj, (np.integer, np.int_, np.intc, np.intp, np.int8,
                        np.int16, np.int32, np.int64, np.uint8, np.uint16,
                        np.uint32, np.uint64)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float_, np.float16, np.float32, np.float64)):
        return float(obj)
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [convert_numpy_types(item) for item in obj]
    elif obj is None or isinstance(obj, (str, int, float, bool)):
        return obj
    else:
        # Fallback: try to convert to native type
        try:
            if hasattr(obj, 'item'):  # NumPy scalar
                return obj.item()
            else:
                return obj
        except (ValueError, TypeError, AttributeError):
            return str(obj)  # Last resort: convert to string


def compute_precision_at_k(y_true: np.ndarray, y_proba: np.ndarray, k: float) -> float:
    """Compute precision at top k% of predictions.
    
    Args:
        y_true: True labels (0 or 1)
        y_proba: Predicted probabilities for positive class
        k: Percentage (e.g., 1 for top 1%, 5 for top 5%)
    
    Returns:
        Precision among top k% predictions
    """
    n_samples = len(y_true)
    n_top_k = max(1, int(n_samples * k / 100))
    
    # Get indices of top k% predictions
    top_k_idx = np.argsort(y_proba)[-n_top_k:]
    
    # Calculate precision among top k%
    return y_true[top_k_idx].sum() / n_top_k


def load_year_data(parquet_path: Path, year: int, columns: Optional[list] = None) -> pd.DataFrame:
    """Load data for a specific year from parquet file efficiently.
    
    Uses row group filtering to minimize memory usage.
    """
    print(f"Loading data for year {year} from {parquet_path} ...")
    
    # Read parquet file with filter
    filters = [('year', '==', year)]
    load_start = time.time()
    df = pd.read_parquet(parquet_path, filters=filters, columns=columns)
    load_time = time.time() - load_start
    
    print(f"  Loaded {len(df):,} rows in {load_time:.1f}s")
    return df


def load_future_protection_status(parquet_path: Path, years_after: range) -> pd.DataFrame:
    """Load WDPA status for years after 2000 to compute ever_protected_after_2000.
    
    Returns DataFrame with row, col, and ever_protected_after_2000 columns.
    """
    print(f"\nLoading future protection status (years {years_after.start}-{years_after.stop - 1}) ...")
    
    # Only load necessary columns
    columns = ['row', 'col', 'year', 'WDPA_b1']
    
    # Load all future years
    load_start = time.time()
    filters = [('year', '>=', years_after.start), ('year', '<=', years_after.stop - 1)]
    df = pd.read_parquet(parquet_path, filters=filters, columns=columns)
    load_time = time.time() - load_start
    
    print(f"  Loaded {len(df):,} rows across {df['year'].nunique()} years in {load_time:.1f}s")
    
    # Compute max WDPA_b1 per pixel (ever protected after 2000)
    print("  Computing ever_protected_after_2000 ...")
    compute_start = time.time()
    protection_max = df.groupby(['row', 'col'])['WDPA_b1'].max().reset_index()
    protection_max.columns = ['row', 'col', 'ever_protected_after_2000']
    
    # Convert to binary
    protection_max['ever_protected_after_2000'] = (
        protection_max['ever_protected_after_2000'] > 0
    ).astype(np.int8)
    compute_time = time.time() - compute_start
    
    del df
    gc.collect()
    
    print(f"  Computed for {len(protection_max):,} unique pixels in {compute_time:.1f}s")
    return protection_max


def get_feature_columns(df: pd.DataFrame) -> list:
    """Get valid feature columns, excluding identifiers and leakage columns."""
    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
    feature_cols = [
        col for col in numeric_cols
        if col.lower() not in {c.lower() for c in EXCLUDE_COLS}
    ]
    return feature_cols


# =============================================================================
# Main Pipeline
# =============================================================================

def main() -> None:
    start_time = time.time()
    
    # -------------------------------------------------------------------------
    # Setup paths
    # -------------------------------------------------------------------------
    repo_root = Path(__file__).resolve().parents[3]
    scratch_root = Path(os.environ["SCRATCH"]) if os.environ.get("SCRATCH") else None
    
    # Input path (prefer SCRATCH on Euler; check multiple possible locations)
    candidate_inputs = []
    if scratch_root is not None and scratch_root.exists():
        # Euler cluster: outputs/Results/merged_panel_2000_2024.parquet
        candidate_inputs.append(scratch_root / "outputs/Results/merged_panel_2000_2024.parquet")
        # Alternative: data/ml/merged_panel_2000_2024.parquet
        candidate_inputs.append(scratch_root / "data/ml/merged_panel_2000_2024.parquet")
    # Local fallback
    candidate_inputs.append(repo_root / "data/ml/merged_panel_2000_2024.parquet")
    
    input_path = candidate_inputs[-1]  # Default to last (local repo path)
    for cand in candidate_inputs:
        if cand.exists():
            input_path = cand
            break
    
    if input_path is None or not input_path.exists():
        error_msg = (
            "Could not find merged_panel_2000_2024.parquet in any expected location.\n"
            f"Checked:\n"
        )
        for cand in candidate_inputs:
            error_msg += f"  - {cand}\n"
        raise FileNotFoundError(error_msg)
    
    # Output directories
    output_dir = repo_root / "outputs/Results/ml_baselines"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    data_output_dir = repo_root / "data/ml"
    data_output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Initialize W&B
    wandb_api_key = os.environ.get("WANDB_API_KEY")
    wandb_entity = os.environ.get("WANDB_ENTITY")
    
    if not wandb_api_key:
        print("Warning: WANDB_API_KEY not found in environment")
    if not wandb_entity:
        print("Warning: WANDB_ENTITY not found in environment")
    
    print("Initializing Weights & Biases...")
    wandb.init(
        project="ml-training-baselines",
        entity=wandb_entity,
        name=f"baseline_lgbm_suitability_{timestamp}",
        config={
            "model": "LightGBM",
            "task": "baseline_suitability_2000",
            "random_state": RANDOM_STATE,
            "test_size": TEST_SIZE,
            "target": "is_protected_2000",
            "future_prediction": "ever_protected_after_2000",
        },
    )
    print("W&B connected\n")
    
    print("=" * 70)
    print("BASELINE SUITABILITY MODEL FOR PROTECTED AREA PREDICTION")
    print("=" * 70)
    print(f"\nInput:  {input_path}")
    print(f"Output: {output_dir}")
    
    # -------------------------------------------------------------------------
    # Step 1: Load 2000 snapshot
    # -------------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("STEP 1: LOAD YEAR 2000 SNAPSHOT")
    print("=" * 70)
    
    df_2000 = load_year_data(input_path, year=2000)
    
    # Target: is_protected_2000 = WDPA_b1
    target_col = "is_protected_2000"
    df_2000[target_col] = df_2000['WDPA_b1']
    
    # Drop rows with missing target
    df_2000_clean = df_2000.dropna(subset=[target_col])
    dropped = len(df_2000) - len(df_2000_clean)
    if dropped > 0:
        print(f"Dropped {dropped:,} rows with missing target")
    
    print(f"Total pixels in 2000: {len(df_2000_clean):,}")
    
    # Get feature columns
    feature_cols = get_feature_columns(df_2000_clean)
    excluded_cols = sorted(EXCLUDE_COLS & set(df_2000_clean.columns))
    
    print(f"\nUsing {len(feature_cols)} features")
    print(f"Excluded columns ({len(excluded_cols)}): {excluded_cols}")
    print(f"\nFeature list:")
    for i, feat in enumerate(sorted(feature_cols), 1):
        print(f"  {i:3d}. {feat}")
    
    # Target distribution
    n_protected = (df_2000_clean[target_col] > 0).sum()
    n_unprotected = (df_2000_clean[target_col] == 0).sum()
    pct_protected = n_protected / len(df_2000_clean) * 100
    
    print(f"\n" + "-" * 40)
    print("Protection status in 2000:")
    print(f"  Protected:   {n_protected:>12,}  ({pct_protected:.2f}%)")
    print(f"  Unprotected: {n_unprotected:>12,}  ({100 - pct_protected:.2f}%)")
    print("-" * 40)
    
    # Log data statistics to wandb
    wandb.log({
        "data/n_features": len(feature_cols),
        "data/n_total_pixels_2000": int(len(df_2000_clean)),
        "data/n_protected_2000": int(n_protected),
        "data/n_unprotected_2000": int(n_unprotected),
        "data/pct_protected_2000": float(pct_protected),
        "data/class_ratio_2000": float(n_unprotected / max(n_protected, 1)),
        "data/features": feature_cols,  # Log feature names
        "data/excluded_columns": excluded_cols,
    })
    
    # -------------------------------------------------------------------------
    # Step 2: Prepare features and target
    # -------------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("STEP 2: PREPARE FEATURES AND TARGET")
    print("=" * 70)
    
    # Binary target
    y = (df_2000_clean[target_col] > 0).astype(np.int8)
    X = df_2000_clean[feature_cols]
    
    # Store pixel identifiers for scoring later
    pixel_ids = df_2000_clean[['row', 'col', 'x', 'y']].copy()
    
    print(f"Feature matrix shape: {X.shape}")
    print(f"Target shape: {y.shape}")
    
    # Check for missing values
    missing_counts = X.isnull().sum()
    cols_with_missing = missing_counts[missing_counts > 0]
    if len(cols_with_missing) > 0:
        print(f"\nColumns with missing values: {len(cols_with_missing)}")
        print("  Top 5:")
        for col, count in cols_with_missing.head().items():
            print(f"    {col}: {count:,} ({count/len(X)*100:.2f}%)")
    
    # -------------------------------------------------------------------------
    # Step 3: Train/test split and model training
    # -------------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("STEP 3: TRAIN LIGHTGBM MODEL")
    print("=" * 70)
    
    # Split data
    print(f"\nSplitting data (80/20) with stratification ...")
    X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(
        X, y, pixel_ids.index,
        test_size=TEST_SIZE,
        stratify=y,
        random_state=RANDOM_STATE
    )
    
    print(f"Training set: {len(X_train):,} samples")
    print(f"  Protected:   {y_train.sum():,}")
    print(f"  Unprotected: {(y_train == 0).sum():,}")
    print(f"\nTest set: {len(X_test):,} samples")
    print(f"  Protected:   {y_test.sum():,}")
    print(f"  Unprotected: {(y_test == 0).sum():,}")
    
    # Calculate scale_pos_weight for imbalanced classification
    n_train_pos = y_train.sum()
    n_train_neg = (y_train == 0).sum()
    scale_pos_weight = n_train_neg / max(n_train_pos, 1)
    
    print(f"\nClass imbalance handling:")
    print(f"  Positive (protected): {n_train_pos:,}")
    print(f"  Negative (unprotected): {n_train_neg:,}")
    print(f"  Imbalance ratio: 1:{n_train_neg / max(n_train_pos, 1):.2f}")
    print(f"  Scale pos weight (auto): {scale_pos_weight:.2f}")
    
    # Log split information
    wandb.log({
        "data/train_size": len(X_train),
        "data/test_size": len(X_test),
        "data/train_protected": int(n_train_pos),
        "data/train_unprotected": int(n_train_neg),
        "data/test_protected": int(y_test.sum()),
        "data/test_unprotected": int((y_test == 0).sum()),
        "data/train_imbalance_ratio": float(n_train_neg / max(n_train_pos, 1)),
        "model/scale_pos_weight": float(scale_pos_weight),
    })
    
    # Update params with scale_pos_weight for balanced learning
    lgbm_params = LGBM_PARAMS.copy()
    lgbm_params['scale_pos_weight'] = scale_pos_weight
    lgbm_params['is_unbalance'] = True  # Explicit imbalance flag
    
    print("\nLightGBM parameters:")
    for param, value in lgbm_params.items():
        if param != 'verbose':
            print(f"  {param}: {value}")
    
    # Create and train model
    print("\nTraining LightGBM ...")
    model = lgb.LGBMClassifier(n_estimators=500, **lgbm_params)
    
    # Use part of training set for early stopping
    X_train_fit, X_val, y_train_fit, y_val = train_test_split(
        X_train, y_train,
        test_size=0.1,
        stratify=y_train,
        random_state=RANDOM_STATE
    )
    
    print(f"  Training on {len(X_train_fit):,} samples")
    print(f"  Validating on {len(X_val):,} samples")
    print(f"  Using early stopping (patience=50, period=100) ...\n")
    
    train_start = time.time()
    model.fit(
        X_train_fit, y_train_fit,
        eval_set=[(X_val, y_val)],
        eval_metric='auc',
        callbacks=[
            lgb.early_stopping(stopping_rounds=50, verbose=False),
            lgb.log_evaluation(period=100)
        ]
    )
    train_time = time.time() - train_start
    print(f"\nTraining completed in {train_time:.1f}s")
    
    print(f"\nTraining complete!")
    print(f"Best iteration: {model.best_iteration_}")
    print(f"Best validation AUC: {model.best_score_['valid_0']['auc']:.4f}")
    
    # Log training progress
    wandb.log({
        "training/best_iteration": model.best_iteration_,
        "training/best_val_auc": float(model.best_score_['valid_0']['auc']),
    })
    
    # Save model
    model_path = data_output_dir / "baseline_lgbm_2000.txt"
    model.booster_.save_model(str(model_path))
    print(f"\nModel saved to: {model_path}")
    
    # -------------------------------------------------------------------------
    # Step 4: Evaluate on test set
    # -------------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("STEP 4: EVALUATE ON TEST SET (YEAR 2000)")
    print("=" * 70)
    
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]
    
    # Classification report
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, digits=4))
    
    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()
    
    print("\nConfusion Matrix:")
    print(f"                 Predicted 0    Predicted 1")
    print(f"Actual 0  {tn:>15,}  {fp:>14,}")
    print(f"Actual 1  {fn:>15,}  {tp:>14,}")
    
    # AUC metrics
    roc_auc = roc_auc_score(y_test, y_proba)
    pr_auc = average_precision_score(y_test, y_proba)
    
    print(f"\nROC-AUC: {roc_auc:.4f}")
    print(f"PR-AUC:  {pr_auc:.4f}")
    
    # Log 2000 classification metrics
    wandb.log({
        "metrics_2000/roc_auc": float(roc_auc),
        "metrics_2000/pr_auc": float(pr_auc),
        "metrics_2000/true_negatives": int(tn),
        "metrics_2000/false_positives": int(fp),
        "metrics_2000/false_negatives": int(fn),
        "metrics_2000/true_positives": int(tp),
        "metrics_2000/accuracy": float((tp + tn) / (tp + tn + fp + fn)),
        "metrics_2000/precision": float(tp / (tp + fp)) if (tp + fp) > 0 else 0,
        "metrics_2000/recall": float(tp / (tp + fn)) if (tp + fn) > 0 else 0,
        "metrics_2000/f1": float(2 * tp / (2 * tp + fp + fn)) if (2 * tp + fp + fn) > 0 else 0,
    })
    
    # Feature importance
    print("\n" + "-" * 40)
    print("Top 20 Most Important Features:")
    print("-" * 40)
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    print(feature_importance.head(20).to_string(index=False))
    
    # -------------------------------------------------------------------------
    # Step 5: Score all unprotected 2000 pixels
    # -------------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("STEP 5: SCORE UNPROTECTED 2000 PIXELS")
    print("=" * 70)
    
    # Get unprotected pixels in 2000
    unprotected_mask = y == 0
    X_unprotected = X[unprotected_mask]
    pixel_ids_unprotected = pixel_ids[unprotected_mask].copy()
    
    print(f"\nUnprotected pixels in 2000: {len(X_unprotected):,}")
    
    # Compute suitability scores
    print("Computing suitability scores ...")
    score_start = time.time()
    suitability_scores = model.predict_proba(X_unprotected)[:, 1]
    score_time = time.time() - score_start
    print(f"  Scored {len(X_unprotected):,} pixels in {score_time:.1f}s")
    
    # Create scores dataframe
    scores_df = pixel_ids_unprotected.copy()
    scores_df['score'] = suitability_scores
    
    # Save scores
    scores_path = data_output_dir / "baseline_scores_2000.parquet"
    scores_df.to_parquet(scores_path, index=False)
    print(f"\nScores saved to: {scores_path}")
    print(f"Score statistics:")
    print(f"  Min:    {suitability_scores.min():.4f}")
    print(f"  Max:    {suitability_scores.max():.4f}")
    print(f"  Mean:   {suitability_scores.mean():.4f}")
    print(f"  Median: {np.median(suitability_scores):.4f}")
    
    # Log score statistics
    wandb.log({
        "scores/n_unprotected_scored": len(X_unprotected),
        "scores/min": float(suitability_scores.min()),
        "scores/max": float(suitability_scores.max()),
        "scores/mean": float(suitability_scores.mean()),
        "scores/median": float(np.median(suitability_scores)),
        "scores/std": float(suitability_scores.std()),
    })
    
    # Free memory
    del df_2000, df_2000_clean, X, y, pixel_ids
    del X_train, X_test, y_train, y_test
    del X_train_fit, X_val, y_train_fit, y_val
    gc.collect()
    
    # -------------------------------------------------------------------------
    # Step 6: Build 2000→2024 outcome
    # -------------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("STEP 6: BUILD 2000→2024 OUTCOME")
    print("=" * 70)
    
    # Load future protection status
    future_protection = load_future_protection_status(input_path, years_after=range(2001, 2025))
    
    # Join with suitability scores
    print("\nJoining scores with future protection status ...")
    outcome_df = scores_df.merge(
        future_protection,
        on=['row', 'col'],
        how='left'
    )
    
    # Fill NaN (pixels that might not exist in future years) with 0
    outcome_df['ever_protected_after_2000'] = (
        outcome_df['ever_protected_after_2000'].fillna(0).astype(np.int8)
    )
    
    n_became_protected = outcome_df['ever_protected_after_2000'].sum()
    n_remained_unprotected = len(outcome_df) - n_became_protected
    pct_became_protected = n_became_protected / len(outcome_df) * 100
    
    print(f"\nOutcome for unprotected 2000 pixels:")
    print(f"  Became protected (2001-2024): {n_became_protected:>12,}  ({pct_became_protected:.2f}%)")
    print(f"  Remained unprotected:         {n_remained_unprotected:>12,}  ({100 - pct_became_protected:.2f}%)")
    
    # Log future protection outcome
    wandb.log({
        "future/n_unprotected_2000": int(len(outcome_df)),
        "future/n_became_protected": int(n_became_protected),
        "future/n_remained_unprotected": int(n_remained_unprotected),
        "future/pct_became_protected": float(pct_became_protected),
    })
    
    # -------------------------------------------------------------------------
    # Step 7: Evaluate suitability scores as predictor of future protection
    # -------------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("STEP 7: EVALUATE SUITABILITY AS PREDICTOR OF FUTURE PROTECTION")
    print("=" * 70)
    
    y_future = outcome_df['ever_protected_after_2000'].values
    y_score = outcome_df['score'].values
    
    # ROC-AUC and PR-AUC
    future_roc_auc = roc_auc_score(y_future, y_score)
    future_pr_auc = average_precision_score(y_future, y_score)
    
    print(f"\nPredicting future protection (2001-2024) from 2000 suitability:")
    print(f"  ROC-AUC: {future_roc_auc:.4f}")
    print(f"  PR-AUC:  {future_pr_auc:.4f}")
    
    # Precision at top k%
    print("\nPrecision @ Top-K:")
    print("(Among top k% highest suitability scores, what fraction became protected?)")
    print()
    
    prec_at_k = {}
    for k in [1, 5, 10]:
        prec = compute_precision_at_k(y_future, y_score, k)
        prec_at_k[k] = prec
        n_top_k = max(1, int(len(y_future) * k / 100))
        n_protected_in_top_k = int(n_top_k * prec)
        print(f"  Precision @ top {k:>2}%:  {prec:.4f}  "
              f"({n_protected_in_top_k:>7,} protected in top {n_top_k:>9,})")
    
    # Baseline (random)
    baseline_rate = y_future.mean()
    print(f"\n  Baseline (random):    {baseline_rate:.4f}  "
          f"(overall protection rate among unprotected 2000 pixels)")
    
    # Lift over baseline
    print("\nLift over baseline (precision / baseline):")
    for k, prec in prec_at_k.items():
        lift = prec / baseline_rate if baseline_rate > 0 else 0
        print(f"  Top {k:>2}%: {lift:.2f}x")
    
    # Log future prediction metrics
    wandb.log({
        "metrics_future/roc_auc": float(future_roc_auc),
        "metrics_future/pr_auc": float(future_pr_auc),
        "metrics_future/precision_at_1pct": float(prec_at_k[1]),
        "metrics_future/precision_at_5pct": float(prec_at_k[5]),
        "metrics_future/precision_at_10pct": float(prec_at_k[10]),
        "metrics_future/baseline_rate": float(baseline_rate),
        "metrics_future/lift_at_1pct": float(prec_at_k[1] / baseline_rate) if baseline_rate > 0 else 0,
        "metrics_future/lift_at_5pct": float(prec_at_k[5] / baseline_rate) if baseline_rate > 0 else 0,
        "metrics_future/lift_at_10pct": float(prec_at_k[10] / baseline_rate) if baseline_rate > 0 else 0,
    })
    
    # -------------------------------------------------------------------------
    # Step 8: Save results
    # -------------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("STEP 8: SAVE RESULTS")
    print("=" * 70)
    
    # Save scored outcomes
    outcomes_path = data_output_dir / "baseline_scored_outcomes.parquet"
    outcome_df.to_parquet(outcomes_path, index=False)
    print(f"\nScored outcomes saved to: {outcomes_path}")
    
    # Compile metrics
    metrics = {
        "metadata": {
            "timestamp": timestamp,
            "model": "LightGBM",
            "random_state": RANDOM_STATE,
            "test_size": TEST_SIZE,
            "n_features": len(feature_cols),
            "best_iteration": model.best_iteration_,
        },
        "year_2000_data": {
            "n_total_pixels": n_protected + n_unprotected,
            "n_protected": int(n_protected),
            "n_unprotected": int(n_unprotected),
            "pct_protected": float(pct_protected),
        },
        "model_performance_2000": {
            "roc_auc": float(roc_auc),
            "pr_auc": float(pr_auc),
            "confusion_matrix": {
                "tn": int(tn), "fp": int(fp), "fn": int(fn), "tp": int(tp)
            },
        },
        "future_prediction_2001_2024": {
            "n_unprotected_2000": int(len(outcome_df)),
            "n_became_protected": int(n_became_protected),
            "pct_became_protected": float(pct_became_protected),
            "roc_auc": float(future_roc_auc),
            "pr_auc": float(future_pr_auc),
            "precision_at_1pct": float(prec_at_k[1]),
            "precision_at_5pct": float(prec_at_k[5]),
            "precision_at_10pct": float(prec_at_k[10]),
            "baseline_rate": float(baseline_rate),
            "lift_at_1pct": float(prec_at_k[1] / baseline_rate) if baseline_rate > 0 else 0,
            "lift_at_5pct": float(prec_at_k[5] / baseline_rate) if baseline_rate > 0 else 0,
            "lift_at_10pct": float(prec_at_k[10] / baseline_rate) if baseline_rate > 0 else 0,
        },
        "top_features": feature_importance.head(20).to_dict('records'),
    }
    
    # Save metrics
    metrics_path = output_dir / "baseline_lgbm_metrics.json"
    with open(metrics_path, 'w') as f:
        # Convert NumPy types to native Python types for JSON serialization
        metrics_serializable = convert_numpy_types(metrics)
        json.dump(metrics_serializable, f, indent=2)
    print(f"Metrics saved to: {metrics_path}")
    
    # -------------------------------------------------------------------------
    # Summary
    # -------------------------------------------------------------------------
    total_time = time.time() - start_time
    
    print("\n" + "=" * 70)
    print("SUMMARY")
    print("=" * 70)
    print(f"Model:                       LightGBM ({model.best_iteration_} iterations)")
    print(f"Features used:               {len(feature_cols)}")
    print(f"\n2000 Classification (protected vs unprotected):")
    print(f"  ROC-AUC:                   {roc_auc:.4f}")
    print(f"  PR-AUC:                    {pr_auc:.4f}")
    print(f"\nFuture Protection Prediction (unprotected 2000 → protected by 2024):")
    print(f"  ROC-AUC:                   {future_roc_auc:.4f}")
    print(f"  PR-AUC:                    {future_pr_auc:.4f}")
    print(f"  Precision @ top 1%:        {prec_at_k[1]:.4f} ({prec_at_k[1]/baseline_rate:.1f}x lift)")
    print(f"  Precision @ top 5%:        {prec_at_k[5]:.4f} ({prec_at_k[5]/baseline_rate:.1f}x lift)")
    print(f"  Precision @ top 10%:       {prec_at_k[10]:.4f} ({prec_at_k[10]/baseline_rate:.1f}x lift)")
    print(f"\nTotal time: {total_time:.1f}s ({total_time/60:.1f} min)")
    print("=" * 70)
    print("Done.")
    
    # Log final timing
    wandb.log({
        "timing/total_seconds": total_time,
        "timing/total_minutes": total_time / 60,
        "status": "success"
    })
    
    wandb.finish()


if __name__ == "__main__":
    # Set up output file capture
    repo_root = Path(__file__).resolve().parents[3]
    output_dir = repo_root / "outputs/Results/ml_baselines"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"baseline_lgbm_{timestamp}.txt"
    
    tee = Tee(output_file)
    sys.stdout = tee
    
    try:
        main()
    finally:
        sys.stdout = tee.stdout
        tee.close()
        print(f"\nOutput saved to: {output_file}")
