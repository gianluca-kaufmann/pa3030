#!/usr/bin/env python3

"""LGBM Transition Model (0→1 Prediction)

Purpose: Train a LightGBM model to predict transitions to protected areas (0→1).
         Uses temporal split (train: 2000-2017, test: 2018-2024) with pre-tuned
         hyperparameters from the sample script.

Input:   `merged_panel_final.parquet`
Output:  - Trained model (.pkl)
         - Test predictions (.parquet)
         - Evaluation metrics (.json)
"""

from __future__ import annotations

import gc
import json
import os
import sys
import time
import pickle
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any

import numpy as np
import pandas as pd
import wandb
from lightgbm import LGBMClassifier
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_auc_score,
    average_precision_score,
)


class Tee:
    """Write to both stdout and a file."""
    def __init__(self, file_path: Path):
        self.file = open(file_path, 'w', encoding='utf-8')
        self.stdout = sys.stdout
    
    def write(self, text: str) -> None:
        self.stdout.write(text)
        self.file.write(text)
        self.file.flush()
    
    def flush(self) -> None:
        self.stdout.flush()
        self.file.flush()
    
    def close(self) -> None:
        self.file.close()


# =============================================================================
# Configuration
# =============================================================================
RANDOM_STATE = 42

# Temporal split
TRAIN_YEARS = range(2000, 2018)  # 2000-2017
TEST_YEARS = range(2018, 2025)   # 2018-2024

# Columns to exclude from features
EXCLUDE_COLS = {
    'transition_01',  # Target variable
    'WDPA_b1',        # Leakage
    'WDPA_prev',      # Leakage
    'x',              # Coordinate
    'y',              # Coordinate
    'row',            # Identifier
    'col',            # Identifier
    'year',           # Temporal identifier
}

# Fixed parameters
FIXED_PARAMS = {
    'random_state': RANDOM_STATE,
    'n_jobs': -1,
    'boosting_type': 'gbdt',
    'objective': 'binary',
    'class_weight': 'balanced',
    'verbose': -1,
}


# =============================================================================
# Utility Functions
# =============================================================================

def convert_numpy_types(obj: Any) -> Any:
    """Recursively convert NumPy types to native Python types for JSON serialization."""
    if isinstance(obj, (np.integer, np.int_, np.intc, np.intp, np.int8,
                        np.int16, np.int32, np.int64, np.uint8, np.uint16,
                        np.uint32, np.uint64)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float_, np.float16, np.float32, np.float64)):
        return float(obj)
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [convert_numpy_types(item) for item in obj]
    elif obj is None or isinstance(obj, (str, int, float, bool)):
        return obj
    else:
        try:
            if hasattr(obj, 'item'):
                return obj.item()
            else:
                return obj
        except (ValueError, TypeError, AttributeError):
            return str(obj)


def compute_precision_at_k(y_true: np.ndarray, y_proba: np.ndarray, k: float) -> float:
    """Compute precision at top k% of predictions.
    
    Args:
        y_true: True labels (0 or 1)
        y_proba: Predicted probabilities for positive class
        k: Percentage (e.g., 1 for top 1%, 5 for top 5%)
    
    Returns:
        Precision among top k% predictions
    """
    n_samples = len(y_true)
    n_top_k = max(1, int(n_samples * k / 100))
    
    # Get indices of top k% predictions
    top_k_idx = np.argsort(y_proba)[-n_top_k:]
    
    # Calculate precision among top k%
    return y_true[top_k_idx].sum() / n_top_k


def load_data(parquet_path: Path, years: range, columns: Optional[list] = None) -> pd.DataFrame:
    """Load data for specified years from parquet file efficiently."""
    print(f"Loading data for years {years.start}-{years.stop-1} from {parquet_path} ...")
    
    # Read parquet file with filter
    filters = [('year', '>=', years.start), ('year', '<=', years.stop - 1)]
    load_start = time.time()
    df = pd.read_parquet(parquet_path, filters=filters, columns=columns)
    load_time = time.time() - load_start
    
    print(f"  Loaded {len(df):,} rows in {load_time:.1f}s")
    print(f"  Years: {sorted(df['year'].unique())}")
    return df


def get_feature_columns(df: pd.DataFrame) -> list:
    """Get valid feature columns, excluding identifiers and leakage columns."""
    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
    feature_cols = [
        col for col in numeric_cols
        if col.lower() not in {c.lower() for c in EXCLUDE_COLS}
    ]
    return feature_cols


def downcast_float64_to_float32(df: pd.DataFrame) -> pd.DataFrame:
    """Downcast float64 columns to float32 to save memory."""
    float64_cols = df.select_dtypes(include=["float64"]).columns
    if len(float64_cols) > 0:
        print(f"  Downcasting {len(float64_cols)} float64 columns to float32")
        df[float64_cols] = df[float64_cols].astype(np.float32)
    return df


# =============================================================================
# Main Pipeline
# =============================================================================

def main() -> None:
    start_time = time.time()
    
    # -------------------------------------------------------------------------
    # Setup paths
    # -------------------------------------------------------------------------
    repo_root = Path(__file__).resolve().parents[3]
    scratch_root = Path(os.environ["SCRATCH"]) if os.environ.get("SCRATCH") else None
    
    # Input path (prefer SCRATCH on Euler; check multiple possible locations)
    candidate_inputs = []
    if scratch_root is not None and scratch_root.exists():
        # Euler cluster: Check common locations
        candidate_inputs.append(scratch_root / "outputs/Results/merged_panel_final.parquet")
        candidate_inputs.append(scratch_root / "data/ml/merged_panel_final.parquet")
    # Local fallback
    candidate_inputs.append(repo_root / "data/ml/merged_panel_final.parquet")
    
    input_path = None
    for cand in candidate_inputs:
        if cand.exists():
            input_path = cand
            break
    
    if input_path is None:
        error_msg = (
            "Could not find merged_panel_final.parquet in any expected location.\n"
            f"Checked:\n"
        )
        for cand in candidate_inputs:
            error_msg += f"  - {cand}\n"
        raise FileNotFoundError(error_msg)
    
    # Output directories
    output_dir = repo_root / "outputs/Results/ml_models"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    model_dir = repo_root / "data/ml/models"
    model_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    print("=" * 70)
    print("LGBM TRANSITION MODEL (0→1 PREDICTION)")
    print("=" * 70)
    print(f"\nInput:  {input_path}")
    print(f"Output: {output_dir}")
    
    # -------------------------------------------------------------------------
    # Step 1: Load and prepare training data (only)
    # -------------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("STEP 1: LOAD TRAINING DATA")
    print("=" * 70)
    
    target_col = "transition_01"
    
    df_train = load_data(input_path, TRAIN_YEARS)
    df_train = downcast_float64_to_float32(df_train)
    
    if target_col not in df_train.columns:
        raise ValueError(f"Target column '{target_col}' not found in training data")
    
    df_train_clean = df_train.dropna(subset=[target_col])
    dropped_train = len(df_train) - len(df_train_clean)
    if dropped_train > 0:
        print(f"\nDropped {dropped_train:,} training rows with missing target")
    
    print(f"\nTraining set: {len(df_train_clean):,} rows")
    
    feature_cols = get_feature_columns(df_train_clean)
    excluded_cols = sorted(EXCLUDE_COLS & set(df_train_clean.columns))
    
    print(f"\nUsing {len(feature_cols)} features")
    print(f"Excluded columns ({len(excluded_cols)}): {excluded_cols}")
    
    train_pos = (df_train_clean[target_col] > 0).sum()
    train_neg = (df_train_clean[target_col] == 0).sum()
    train_pos_pct = train_pos / len(df_train_clean) * 100
    
    print(f"\n" + "-" * 40)
    print("Training set distribution:")
    print(f"  No transition (0): {train_neg:>12,}  ({100 - train_pos_pct:.3f}%)")
    print(f"  Transition (0→1):  {train_pos:>12,}  ({train_pos_pct:.3f}%)")
    print(f"  Class ratio:       1 : {train_neg / max(train_pos, 1):.1f}")
    print("-" * 40)
    
    # -------------------------------------------------------------------------
    # Step 2: Prepare training features and target
    # -------------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("STEP 2: PREPARE TRAINING FEATURES AND TARGET")
    print("=" * 70)
    
    y_train = (df_train_clean[target_col] > 0).astype(np.int8)
    X_train = df_train_clean[feature_cols]
    n_train_samples = len(X_train)
    
    print(f"Training feature matrix shape: {X_train.shape}")
    print(f"Training target shape: {y_train.shape}")
    
    missing_train = X_train.isnull().sum()
    cols_with_missing = missing_train[missing_train > 0]
    if len(cols_with_missing) > 0:
        print(f"\nWarning: {len(cols_with_missing)} columns have missing values in training set")
        print("  Top 5:")
        for col, count in cols_with_missing.head().items():
            print(f"    {col}: {count:,} ({count/len(X_train)*100:.2f}%)")
    
    # -------------------------------------------------------------------------
    # Step 3: Train model with optimal parameters
    # -------------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("STEP 3: TRAIN MODEL WITH OPTIMAL PARAMETERS")
    print("=" * 70)
    
    # Optimal parameters from sample script tuning (update from model1_lgbm_sample_results JSON)
    # TODO: Update these values from the latest model1_lgbm_sample_results_*.json output
    best_params = {
        **FIXED_PARAMS,
        'n_estimators': 500,  # Update from sample script output
        'learning_rate': 0.1,  # Update from sample script output
        'num_leaves': 63,  # Update from sample script output
        'max_depth': 20,  # Update from sample script output
        'min_child_samples': 20,  # Update from sample script output
        'subsample': 1.0,  # Update from sample script output
        'colsample_bytree': 1.0,  # Update from sample script output
        'scale_pos_weight': 1.0,  # Update from sample script output
        'verbose': 1,  # Show training progress
    }
    
    # Initialize W&B
    wandb_api_key = os.environ.get("WANDB_API_KEY")
    wandb_entity = os.environ.get("WANDB_ENTITY")
    
    if not wandb_api_key:
        print("Warning: WANDB_API_KEY not found in environment")
    if not wandb_entity:
        print("Warning: WANDB_ENTITY not found in environment")
    
    print("Initializing Weights & Biases...")
    wandb.init(
        project="ml-training-transitions",
        entity=wandb_entity,
        name=f"model1_lgbm_{timestamp}",
        config={
            "model": "LightGBM",
            "task": "transition_01_prediction",
            "random_state": RANDOM_STATE,
            "train_years": f"{TRAIN_YEARS.start}-{TRAIN_YEARS.stop-1}",
            "test_years": f"{TEST_YEARS.start}-{TEST_YEARS.stop-1}",
            "parameters": best_params,
        },
    )
    print("W&B connected\n")
    
    # Log data statistics to wandb
    wandb.log({
        "data/n_features": len(feature_cols),
        "data/n_train": int(len(df_train_clean)),
        "data/n_test": int(len(df_test_clean)),
        "data/train_positives": int(train_pos),
        "data/train_negatives": int(train_neg),
        "data/train_positive_pct": float(train_pos_pct),
        "data/test_positives": int(test_pos),
        "data/test_negatives": int(test_neg),
        "data/test_positive_pct": float(test_pos_pct),
    })
    
    print(f"\nUsing optimal parameters from sample script tuning:")
    for param, value in sorted(best_params.items()):
        if param != 'verbose':  # Skip verbose in display
            print(f"  {param}: {value}")
    
    print(f"\nTraining LightGBM on full dataset...")
    print(f"  Training on {len(X_train):,} samples")
    print(f"  This may take several minutes...\n")
    
    final_model = LGBMClassifier(**best_params)
    
    gc.collect()
    train_start = time.time()
    final_model.fit(X_train, y_train)
    train_time = time.time() - train_start
    
    print(f"\nTraining completed in {train_time:.1f}s ({train_time/60:.1f} min)")
    
    # Save model
    model_path = model_dir / f"model1_lgbm_{timestamp}.pkl"
    with open(model_path, 'wb') as f:
        pickle.dump(final_model, f)
    print(f"\nModel saved to: {model_path}")
    
    # Log training time and parameters
    wandb.log({
        "training/time_seconds": train_time,
        "training/time_minutes": train_time / 60,
        "model/parameters": best_params,
    })
    
    # Free memory before loading test set
    del df_train, df_train_clean, X_train, y_train
    gc.collect()
    
    # -------------------------------------------------------------------------
    # Step 4: Load and prepare test data after training
    # -------------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("STEP 4: LOAD TEST DATA")
    print("=" * 70)
    
    df_test = load_data(input_path, TEST_YEARS, columns=list(feature_cols + [target_col]))
    df_test = downcast_float64_to_float32(df_test)
    
    if target_col not in df_test.columns:
        raise ValueError(f"Target column '{target_col}' not found in test data")
    
    df_test_clean = df_test.dropna(subset=[target_col])
    dropped_test = len(df_test) - len(df_test_clean)
    if dropped_test > 0:
        print(f"Dropped {dropped_test:,} test rows with missing target")
    
    print(f"\nTest set: {len(df_test_clean):,} rows")
    
    test_pos = (df_test_clean[target_col] > 0).sum()
    test_neg = (df_test_clean[target_col] == 0).sum()
    test_pos_pct = test_pos / len(df_test_clean) * 100
    
    print(f"\n" + "-" * 40)
    print("Test set distribution:")
    print(f"  No transition (0): {test_neg:>12,}  ({100 - test_pos_pct:.3f}%)")
    print(f"  Transition (0→1):  {test_pos:>12,}  ({test_pos_pct:.3f}%)")
    print(f"  Class ratio:       1 : {test_neg / max(test_pos, 1):.1f}")
    print("-" * 40)
    
    print("\n" + "=" * 70)
    print("STEP 5: PREPARE TEST FEATURES AND TARGET")
    print("=" * 70)
    
    y_test = (df_test_clean[target_col] > 0).astype(np.int8)
    X_test = df_test_clean[feature_cols]
    n_test_samples = len(X_test)
    
    print(f"Test feature matrix shape: {X_test.shape}")
    print(f"Test target shape: {y_test.shape}")
    
    gc.collect()
    
    # -------------------------------------------------------------------------
    # Step 6: Evaluate on test set
    # -------------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("STEP 6: EVALUATE ON TEST SET (2018-2024)")
    print("=" * 70)
    
    print("\nComputing predictions...")
    pred_start = time.time()
    y_pred = final_model.predict(X_test)
    y_proba = final_model.predict_proba(X_test)[:, 1]
    pred_time = time.time() - pred_start
    print(f"  Predicted {len(y_test):,} samples in {pred_time:.1f}s")
    
    # Classification report
    print("\n" + "-" * 40)
    print("Classification Report:")
    print("-" * 40)
    print(classification_report(y_test, y_pred, digits=4))
    
    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()
    
    print("\n" + "-" * 40)
    print("Confusion Matrix:")
    print("-" * 40)
    print(f"                 Predicted 0    Predicted 1")
    print(f"Actual 0  {tn:>15,}  {fp:>14,}")
    print(f"Actual 1  {fn:>15,}  {tp:>14,}")
    
    # AUC metrics
    roc_auc = roc_auc_score(y_test, y_proba)
    pr_auc = average_precision_score(y_test, y_proba)
    
    print(f"\n" + "-" * 40)
    print("Area Under Curve Metrics:")
    print("-" * 40)
    print(f"ROC-AUC: {roc_auc:.4f}")
    print(f"PR-AUC:  {pr_auc:.4f}")
    
    # Precision at top k%
    print(f"\n" + "-" * 40)
    print("Precision @ Top-K Predictions:")
    print("-" * 40)
    print("(Precision among highest-confidence predictions)")
    print()
    
    prec_at_k = {}
    for k in [1, 5, 10]:
        prec = compute_precision_at_k(y_test.values, y_proba, k)
        prec_at_k[k] = prec
        n_top_k = max(1, int(len(y_test) * k / 100))
        n_protected_in_top_k = int(n_top_k * prec)
        print(f"  Precision @ top {k:>2}%:  {prec:.4f}  "
              f"({n_protected_in_top_k:>7,} transitions in top {n_top_k:>9,})")
    
    # Baseline (random)
    baseline_rate = y_test.mean()
    print(f"\n  Baseline (random):    {baseline_rate:.4f}  "
          f"(overall transition rate in test set)")
    
    # Lift over baseline
    print("\nLift over baseline:")
    for k, prec in prec_at_k.items():
        lift = prec / baseline_rate if baseline_rate > 0 else 0
        print(f"  Top {k:>2}%: {lift:.2f}x")
    
    # Log test metrics
    wandb.log({
        "test/roc_auc": float(roc_auc),
        "test/pr_auc": float(pr_auc),
        "test/true_negatives": int(tn),
        "test/false_positives": int(fp),
        "test/false_negatives": int(fn),
        "test/true_positives": int(tp),
        "test/accuracy": float((tp + tn) / (tp + tn + fp + fn)),
        "test/precision": float(tp / (tp + fp)) if (tp + fp) > 0 else 0,
        "test/recall": float(tp / (tp + fn)) if (tp + fn) > 0 else 0,
        "test/f1": float(2 * tp / (2 * tp + fp + fn)) if (2 * tp + fp + fn) > 0 else 0,
        "test/precision_at_1pct": float(prec_at_k[1]),
        "test/precision_at_5pct": float(prec_at_k[5]),
        "test/precision_at_10pct": float(prec_at_k[10]),
        "test/baseline_rate": float(baseline_rate),
        "test/lift_at_1pct": float(prec_at_k[1] / baseline_rate) if baseline_rate > 0 else 0,
        "test/lift_at_5pct": float(prec_at_k[5] / baseline_rate) if baseline_rate > 0 else 0,
        "test/lift_at_10pct": float(prec_at_k[10] / baseline_rate) if baseline_rate > 0 else 0,
    })
    
    # Feature importance
    print("\n" + "-" * 40)
    print("Top 20 Most Important Features:")
    print("-" * 40)
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': final_model.feature_importances_
    }).sort_values('importance', ascending=False)
    print(feature_importance.head(20).to_string(index=False))
    
    # -------------------------------------------------------------------------
    # Step 5: Save predictions and metrics
    # -------------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("STEP 5: SAVE PREDICTIONS AND METRICS")
    print("=" * 70)
    
    # Create predictions dataframe
    predictions_df = pd.DataFrame({
        'row': df_test_clean['row'].values,
        'col': df_test_clean['col'].values,
        'year': df_test_clean['year'].values,
        'x': df_test_clean['x'].values if 'x' in df_test_clean.columns else np.nan,
        'y': df_test_clean['y'].values if 'y' in df_test_clean.columns else np.nan,
        'true_label': y_test.values,
        'predicted_label': y_pred,
        'predicted_proba': y_proba,
    })
    
    # Save predictions
    predictions_path = output_dir / f"model1_lgbm_predictions_{timestamp}.parquet"
    predictions_df.to_parquet(predictions_path, index=False)
    print(f"\nPredictions saved to: {predictions_path}")
    
    # Compile metrics
    metrics = {
        "metadata": {
            "timestamp": timestamp,
            "model": "LightGBM",
            "task": "transition_01_prediction",
            "random_state": RANDOM_STATE,
            "train_years": f"{TRAIN_YEARS.start}-{TRAIN_YEARS.stop-1}",
            "test_years": f"{TEST_YEARS.start}-{TEST_YEARS.stop-1}",
            "n_features": len(feature_cols),
            "features": feature_cols,
        },
        "data": {
            "n_train": int(n_train_samples),
            "n_test": int(n_test_samples),
            "train_positives": int(train_pos),
            "train_negatives": int(train_neg),
            "test_positives": int(test_pos),
            "test_negatives": int(test_neg),
        },
        "hyperparameter_tuning": {
            "method": "Pre-tuned (from sample script)",
            "source": "model1_lgbm_sample_results JSON",
            "best_params": best_params,
        },
        "model_parameters": best_params,
        "test_performance": {
            "roc_auc": float(roc_auc),
            "pr_auc": float(pr_auc),
            "confusion_matrix": {
                "tn": int(tn),
                "fp": int(fp),
                "fn": int(fn),
                "tp": int(tp),
            },
            "accuracy": float((tp + tn) / (tp + tn + fp + fn)),
            "precision": float(tp / (tp + fp)) if (tp + fp) > 0 else 0,
            "recall": float(tp / (tp + fn)) if (tp + fn) > 0 else 0,
            "f1": float(2 * tp / (2 * tp + fp + fn)) if (2 * tp + fp + fn) > 0 else 0,
            "precision_at_1pct": float(prec_at_k[1]),
            "precision_at_5pct": float(prec_at_k[5]),
            "precision_at_10pct": float(prec_at_k[10]),
            "baseline_rate": float(baseline_rate),
            "lift_at_1pct": float(prec_at_k[1] / baseline_rate) if baseline_rate > 0 else 0,
            "lift_at_5pct": float(prec_at_k[5] / baseline_rate) if baseline_rate > 0 else 0,
            "lift_at_10pct": float(prec_at_k[10] / baseline_rate) if baseline_rate > 0 else 0,
        },
        "feature_importance": feature_importance.head(20).to_dict('records'),
        "timing": {
            "training_seconds": train_time,
            "prediction_seconds": pred_time,
            "total_seconds": time.time() - start_time,
        },
    }
    
    # Convert NumPy types to native Python types
    metrics = convert_numpy_types(metrics)
    
    # Save metrics
    metrics_path = output_dir / f"model1_lgbm_metrics_{timestamp}.json"
    with open(metrics_path, 'w') as f:
        json.dump(metrics, f, indent=2)
    print(f"Metrics saved to: {metrics_path}")
    
    # -------------------------------------------------------------------------
    # Summary
    # -------------------------------------------------------------------------
    total_time = time.time() - start_time
    
    print("\n" + "=" * 70)
    print("SUMMARY")
    print("=" * 70)
    print(f"Model:                       LightGBM")
    print(f"Parameters:                  {best_params}")
    print(f"Features used:               {len(feature_cols)}")
    print(f"\nTest Set Performance (2018-2024):")
    print(f"  Samples:                   {len(y_test):,}")
    print(f"  Positive rate:             {test_pos_pct:.3f}%")
    print(f"  ROC-AUC:                   {roc_auc:.4f}")
    print(f"  PR-AUC:                    {pr_auc:.4f}")
    print(f"  Precision @ top 1%:        {prec_at_k[1]:.4f} ({prec_at_k[1]/baseline_rate:.1f}x lift)")
    print(f"  Precision @ top 5%:        {prec_at_k[5]:.4f} ({prec_at_k[5]/baseline_rate:.1f}x lift)")
    print(f"  Precision @ top 10%:       {prec_at_k[10]:.4f} ({prec_at_k[10]/baseline_rate:.1f}x lift)")
    print(f"\nTimings:")
    print(f"  Model training:            {train_time:.1f}s ({train_time/60:.1f} min)")
    print(f"  Total time:                {total_time:.1f}s ({total_time/60:.1f} min)")
    print("=" * 70)
    print("Done.")
    
    # Log final summary
    wandb.log({
        "summary/total_time_seconds": total_time,
        "summary/total_time_minutes": total_time / 60,
        "status": "success"
    })
    
    wandb.finish()


if __name__ == "__main__":
    # Set up output file capture
    repo_root = Path(__file__).resolve().parents[3]
    output_dir = repo_root / "outputs/Results/ml_models"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"model1_lgbm_{timestamp}.txt"
    
    tee = Tee(output_file)
    sys.stdout = tee
    
    try:
        main()
    finally:
        sys.stdout = tee.stdout
        tee.close()
        print(f"\nOutput saved to: {output_file}")
