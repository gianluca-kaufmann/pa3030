#!/usr/bin/env python3

"""LGBM Transition Model (0→1 Prediction)

Purpose: Train a LightGBM model to predict transitions to protected areas (0→1).
         Uses train.parquet (2000-2017) and test.parquet (2018-2024) with pre-tuned hyperparameters
         from lgbm_best_params.json. Uses strict temporal split for internal validation:
         - train_early (≤2014) for training
         - val_late (2015-2017) for validation
         - Final model retrained on full train.parquet (2000-2017)
         - Evaluation on test.parquet (2018-2024)

Input:   - train.parquet (2000-2017)
         - test.parquet (2018-2024)
         - lgbm_best_params.json
Output:  - Trained model (.pkl)
         - Test predictions (.parquet)
         - Evaluation metrics (.json)
"""

from __future__ import annotations

import gc
import json
import os
import sys
import time
import pickle
from datetime import datetime
from pathlib import Path
from typing import Dict, Any

import numpy as np
import pandas as pd
import wandb
import pyarrow.parquet as pq
import lightgbm as lgb
from lightgbm import LGBMClassifier
from sklearn.metrics import roc_auc_score, average_precision_score


class Tee:
    """Write to both stdout and a file."""
    def __init__(self, file_path: Path):
        self.file = open(file_path, 'w', encoding='utf-8')
        self.stdout = sys.stdout
    
    def write(self, text: str) -> None:
        self.stdout.write(text)
        self.file.write(text)
        self.stdout.flush()
        self.file.flush()
    
    def flush(self) -> None:
        self.stdout.flush()
        self.file.flush()
    
    def close(self) -> None:
        self.file.close()


# =============================================================================
# Configuration
# =============================================================================
RANDOM_STATE = 42

# Columns to exclude from features
EXCLUDE_COLS = {
    'transition_01',  # Target variable
    'WDPA_b1',        # Leakage
    'WDPA_prev',      # Leakage
    'x',              # Coordinate
    'y',              # Coordinate
    'row',            # Identifier
    'col',            # Identifier
    'year',           # Temporal identifier
}

# Temporal split configuration for internal validation
TRAIN_EARLY_YEAR_MAX = 2014  # train_early: year <= 2014
VAL_LATE_YEAR_MIN = 2015     # val_late: 2015-2017
VAL_LATE_YEAR_MAX = 2017

# Fixed parameters (will be merged with best_params from JSON)
FIXED_PARAMS = {
    'random_state': RANDOM_STATE,
    'boosting_type': 'gbdt',
    'objective': 'binary',
    'verbose': -1,
}


# =============================================================================
# Utility Functions
# =============================================================================

def get_n_jobs() -> int:
    """Get safe number of jobs for parallel processing.
    
    Returns:
        Number of CPUs to use, or -1 for all available CPUs.
    """
    slurm_cpus = os.environ.get("SLURM_CPUS_PER_TASK")
    if slurm_cpus:
        try:
            return int(slurm_cpus)
        except ValueError:
            return -1
    return -1


def convert_numpy_types(obj: Any) -> Any:
    """Recursively convert NumPy types to native Python types for JSON serialization."""
    if isinstance(obj, (np.integer, np.int_, np.intc, np.intp, np.int8, np.int16, np.int32, 
                        np.int64, np.uint8, np.uint16, np.uint32, np.uint64)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float_, np.float16, np.float32, np.float64)):
        return float(obj)
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [convert_numpy_types(item) for item in obj]
    elif obj is None or isinstance(obj, (str, int, float, bool)):
        return obj
    else:
        try:
            return obj.item() if hasattr(obj, 'item') else obj
        except (ValueError, TypeError, AttributeError):
            return str(obj)


def compute_precision_at_k(y_true: np.ndarray, y_proba: np.ndarray, k: float) -> float:
    """Compute precision at top k% of predictions.
    
    Args:
        y_true: True labels (0 or 1)
        y_proba: Predicted probabilities for positive class
        k: Percentage (e.g., 1 for top 1%, 5 for top 5%)
    
    Returns:
        Precision among top k% predictions
    """
    n_samples = len(y_true)
    n_top_k = max(1, int(n_samples * k / 100))
    
    # Get indices of top k% predictions
    top_k_idx = np.argsort(y_proba)[-n_top_k:]
    
    # Calculate precision among top k%
    return y_true[top_k_idx].sum() / n_top_k


def resolve_parquet_file(filename: str) -> Path:
    """Locate parquet file (prefer $SCRATCH if present)."""
    repo_root = Path(__file__).resolve().parents[3]
    scratch_root = Path(os.environ["SCRATCH"]) if os.environ.get("SCRATCH") else None

    candidates = []
    if scratch_root is not None:
        candidates.append(scratch_root / f"data/ml/{filename}")
    candidates.append(repo_root / f"data/ml/{filename}")

    for cand in candidates:
        if cand.exists():
            return cand

    raise FileNotFoundError(f"{filename} not found in expected locations")


def resolve_best_params_json() -> Path:
    """Locate lgbm_best_params.json (in same directory as this script)."""
    script_dir = Path(__file__).resolve().parent
    params_path = script_dir / "lgbm_best_params.json"
    
    if not params_path.exists():
        raise FileNotFoundError(f"lgbm_best_params.json not found at {params_path}")
    
    return params_path


def load_best_params(params_path: Path) -> tuple[Dict[str, Any], Any]:
    """Load best parameters from JSON and merge with fixed params.
    
    Merge order (later sources override earlier):
    1. FIXED_PARAMS (from this script)
    2. json_fixed_params (from JSON file)
    3. best_params (from JSON file)
    """
    with open(params_path, 'r') as f:
        params_data = json.load(f)
    
    # Extract best_params
    best_params = params_data.get('best_params', {})
    json_fixed_params = params_data.get('fixed_params', {})
    
    # Merge with fixed params in correct order: FIXED_PARAMS → json_fixed_params → best_params
    final_params = {**FIXED_PARAMS, **json_fixed_params, **best_params}
    
    # Enforce strict parameter guardrails (override any tuned values)
    guardrail_params = {
        "max_depth": 8,
        "num_leaves": 255,
        "min_child_samples": 50,
        "subsample": 0.7,
        "subsample_freq": 1,
        "colsample_bytree": 0.7,
        "learning_rate": 0.03,
        "n_estimators": 3000,
        "reg_alpha": 1.0,
        "reg_lambda": 1.0,
        "is_unbalance": True,
        "metric": "average_precision",
        "n_jobs": get_n_jobs(),
        "verbose": -1,
    }
    final_params.update(guardrail_params)
    
    # Remove scale_pos_weight if present
    final_params.pop("scale_pos_weight", None)
    
    return final_params, params_data.get('best_cv_score', None)


def downcast_numeric_dtypes(df: pd.DataFrame) -> None:
    """Downcast numeric dtypes to reduce memory usage (in-place)."""
    for col in df.select_dtypes(include=['float64']).columns:
        df[col] = df[col].astype('float32', copy=False)
    for col in df.select_dtypes(include=['int64']).columns:
        df[col] = df[col].astype('int32', copy=False)


def load_and_preprocess(df_path: Path, columns: list, target_col: str, name: str = "") -> pd.DataFrame:
    """Load, downcast, and preprocess DataFrame."""
    print(f"\nLoading {name or df_path.name} (selected columns only)...")
    load_start = time.time()
    df = pd.read_parquet(df_path, columns=columns)
    print(f"  Loaded {len(df):,} rows in {time.time() - load_start:.1f}s")
    print("\nDowncasting numeric dtypes...")
    downcast_numeric_dtypes(df)
    gc.collect()
    if target_col not in df.columns:
        raise ValueError(f"Target column '{target_col}' not found")
    df.dropna(subset=[target_col], inplace=True)
    print(f"  {len(df):,} rows after dropna")
    return df


def extract_split(df: pd.DataFrame, feature_cols: list, target_col: str, 
                  year_filter, split_name: str) -> tuple:
    """Extract X, y and stats from filtered DataFrame."""
    df_split = df[year_filter(df['year'])].copy()
    y = (df_split[target_col] > 0).astype(np.int8)
    X = df_split[feature_cols].copy()
    pos, neg = int(y.sum()), int((y == 0).sum())
    years = sorted(df_split['year'].unique())
    print(f"\n{split_name} distribution:")
    print(f"  No transition (0): {neg:>12,}  ({100 - pos/len(y)*100:.3f}%)")
    print(f"  Transition (0→1):  {pos:>12,}  ({pos/len(y)*100:.3f}%)")
    print(f"  Class ratio:       1 : {neg / max(pos, 1):.1f}")
    print(f"  Years: {years[0]}-{years[-1]}")
    return X, y, pos, neg


def compute_metrics(y_true: np.ndarray, y_proba: np.ndarray) -> Dict[str, float]:
    """Compute all validation/test metrics."""
    roc_auc = roc_auc_score(y_true, y_proba)
    pr_auc = average_precision_score(y_true, y_proba)
    baseline_rate = y_true.mean()
    prec_at_k = {k: compute_precision_at_k(y_true, y_proba, k) for k in [1, 5, 10]}
    return {
        "roc_auc": float(roc_auc), "pr_auc": float(pr_auc),
        "precision_at_1pct": float(prec_at_k[1]),
        "precision_at_5pct": float(prec_at_k[5]),
        "precision_at_10pct": float(prec_at_k[10]),
        "baseline_rate": float(baseline_rate),
        "lift_at_1pct": float(prec_at_k[1] / baseline_rate) if baseline_rate > 0 else 0,
        "lift_at_5pct": float(prec_at_k[5] / baseline_rate) if baseline_rate > 0 else 0,
        "lift_at_10pct": float(prec_at_k[10] / baseline_rate) if baseline_rate > 0 else 0,
    }




# =============================================================================
# Main Pipeline
# =============================================================================

def phase1_training(
    train_path: Path,
    params_path: Path,
    feature_cols: list,
    keep_cols_train: list,
    best_params: Dict[str, Any],
    timestamp: str,
    model_dir: Path,
    output_dir: Path,
    use_wandb: bool,
    target_col: str,
) -> tuple[Path, Dict[str, Any], Dict[str, Any], float, float, float, float, int, int]:
    """Phase 1: Training phase - loads only train.parquet, trains model, saves it.
    
    Uses multi-pass loading to prevent memory spikes by ensuring splits never coexist.
    
    Returns:
        Tuple of (model_path, validation_metrics, train_stats, train_early_time, 
                 val_time, train_time, train_pos, train_neg, n_train_full)
    """
    print("\n" + "=" * 70)
    print("PHASE 1: TRAINING (MULTI-PASS LOADING)")
    print("=" * 70)
    
    # =========================================================================
    # STEP A: Load train_early (year <= 2014)
    # =========================================================================
    print("\n" + "=" * 70)
    print("STEP A: LOAD TRAIN_EARLY (year <= 2014)")
    print("=" * 70)
    
    df_train = load_and_preprocess(train_path, keep_cols_train, target_col, "train.parquet")
    print(f"\nFiltering train_early (year <= {TRAIN_EARLY_YEAR_MAX})...")
    X_train_early, y_train_early, train_early_pos, train_early_neg = extract_split(
        df_train, feature_cols, target_col, 
        lambda y: y <= TRAIN_EARLY_YEAR_MAX, "Train early"
    )
    del df_train
    gc.collect()
    print("  Memory cleaned up after train_early extraction")
    
    # =========================================================================
    # STEP B: Load val_late (2015 <= year <= 2017)
    # =========================================================================
    print("\n" + "=" * 70)
    print("STEP B: LOAD VAL_LATE (2015 <= year <= 2017)")
    print("=" * 70)
    
    df_train = load_and_preprocess(train_path, keep_cols_train, target_col, "train.parquet")
    print(f"\nFiltering val_late ({VAL_LATE_YEAR_MIN} <= year <= {VAL_LATE_YEAR_MAX})...")
    X_val_late, y_val_late, val_late_pos, val_late_neg = extract_split(
        df_train, feature_cols, target_col,
        lambda y: (y >= VAL_LATE_YEAR_MIN) & (y <= VAL_LATE_YEAR_MAX), "Val late"
    )
    del df_train
    gc.collect()
    print("  Memory cleaned up after val_late extraction")
    
    # =========================================================================
    # STEP C: Train early model, evaluate on val_late, compute metrics
    # =========================================================================
    print("\n" + "=" * 70)
    print("STEP C: TRAIN EARLY MODEL AND EVALUATE ON VAL_LATE")
    print("=" * 70)
    
    print(f"\nTemporal split summary:")
    print(f"  train_early (≤{TRAIN_EARLY_YEAR_MAX}): {len(X_train_early):,} rows, {y_train_early.sum():,} positives ({y_train_early.mean()*100:.3f}%)")
    print(f"  val_late ({VAL_LATE_YEAR_MIN}-{VAL_LATE_YEAR_MAX}): {len(X_val_late):,} rows, {y_val_late.sum():,} positives ({y_val_late.mean()*100:.3f}%)")
    
    print(f"\nTraining LightGBM on train_early ({len(X_train_early):,} samples)...")
    
    train_early_start = time.time()
    lgb_model = LGBMClassifier(**best_params)
    lgb_model.fit(
        X_train_early,
        y_train_early,
        eval_set=[(X_val_late, y_val_late)],
        eval_metric="average_precision",
        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(100)],
    )
    train_early_time = time.time() - train_early_start
    
    print(f"\nTraining completed in {train_early_time:.1f}s ({train_early_time/60:.1f} min)")
    
    # Evaluate on val_late
    print("\nEvaluating on val_late...")
    val_start = time.time()
    y_val_proba = lgb_model.predict_proba(X_val_late)[:, 1]
    val_time = time.time() - val_start
    
    validation_metrics = compute_metrics(y_val_late.values, y_val_proba)
    print(f"\n" + "-" * 40)
    print("Validation Set (val_late) Performance:")
    print("-" * 40)
    print(f"ROC-AUC: {validation_metrics['roc_auc']:.4f}")
    print(f"PR-AUC:  {validation_metrics['pr_auc']:.4f}")
    print(f"Precision @ top 1%:  {validation_metrics['precision_at_1pct']:.4f}")
    print(f"Precision @ top 5%:  {validation_metrics['precision_at_5pct']:.4f}")
    print(f"Precision @ top 10%: {validation_metrics['precision_at_10pct']:.4f}")
    print(f"Baseline rate: {validation_metrics['baseline_rate']:.4f}")
    print("-" * 40)
    
    # Log validation metrics to W&B
    if use_wandb:
        wandb.log({
            **{f"val/{k}": v for k, v in validation_metrics.items()},
            "val/n_samples": int(len(y_val_late)),
            "val/n_positives": int(y_val_late.sum()),
            "val/time_seconds": val_time,
        })
    
    # Free memory: delete all datasets from validation phase (keep lgb_model for best_iteration_)
    del X_train_early, y_train_early, X_val_late, y_val_late
    gc.collect()
    print("\n  Memory cleaned up after validation phase")
    
    # =========================================================================
    # STEP D: Load full training data (2000-2017) and train final model
    # =========================================================================
    print("\n" + "=" * 70)
    print("STEP D: LOAD FULL TRAINING DATA (2000-2017) AND TRAIN FINAL MODEL")
    print("=" * 70)
    
    df_train = load_and_preprocess(train_path, keep_cols_train, target_col, "train.parquet")
    y = (df_train[target_col] > 0).astype(np.int8)
    X = df_train[feature_cols].copy()
    train_pos, train_neg = int(y.sum()), int((y == 0).sum())
    train_years = sorted(df_train['year'].unique())
    n_train_full = len(df_train)
    del df_train
    gc.collect()
    
    print(f"\nFull training set distribution:")
    print(f"  No transition (0): {train_neg:>12,}  ({100 - train_pos/len(y)*100:.3f}%)")
    print(f"  Transition (0→1):  {train_pos:>12,}  ({train_pos/len(y)*100:.3f}%)")
    print(f"  Class ratio:       1 : {train_neg / max(train_pos, 1):.1f}")
    print(f"  Years: {train_years[0]}-{train_years[-1]}")
    
    train_stats = {
        "n_train": n_train_full, "train_positives": train_pos, "train_negatives": train_neg,
        "train_positive_pct": train_pos/len(y)*100, "train_years": f"{train_years[0]}-{train_years[-1]}",
    }
    
    # Log data statistics to wandb
    if use_wandb:
        wandb.log({
            "data/n_features": len(feature_cols),
            "data/n_train": n_train_full,
            "data/train_positives": train_pos,
            "data/train_negatives": train_neg,
            "data/train_positive_pct": train_pos/len(y)*100,
        })
    
    print(f"\nTraining LightGBM on full training dataset ({len(X):,} samples, 2000-2017)...")
    
    # Use best_iteration_ from temporal validation model to set n_estimators for final training
    n_estimators_full = getattr(lgb_model, "best_iteration_", None)
    if n_estimators_full is None:
        # Fallback to configured n_estimators if early stopping did not set best_iteration_
        n_estimators_full = best_params.get("n_estimators", 3000)
    final_train_params = best_params.copy()
    final_train_params["n_estimators"] = n_estimators_full
    
    train_start = time.time()
    final_model = LGBMClassifier(**final_train_params)
    final_model.fit(X, y)
    train_time = time.time() - train_start
    
    print(f"\nTraining completed in {train_time:.1f}s ({train_time/60:.1f} min)")
    
    # Save model
    model_path = model_dir / f"model1_lgbm_{timestamp}.pkl"
    with open(model_path, 'wb') as f:
        pickle.dump(final_model, f)
    print(f"\nModel saved to: {model_path}")
    
    # Log training time
    if use_wandb:
        wandb.log({
            "training/train_early_time_seconds": train_early_time,
            "training/train_early_time_minutes": train_early_time / 60,
            "training/final_train_time_seconds": train_time,
            "training/final_train_time_minutes": train_time / 60,
        })
    
    # Cleanup: delete X, y, models
    del X, y, final_model, lgb_model
    gc.collect()
    
    print("\nPhase 1 completed. Memory cleaned up.")
    
    return (model_path, validation_metrics, train_stats, train_early_time, 
            val_time, train_time, train_pos, train_neg, n_train_full)


def phase2_testing(
    test_path: Path,
    model_path: Path,
    feature_cols: list,
    keep_cols_test: list,
    timestamp: str,
    output_dir: Path,
    use_wandb: bool,
    target_col: str,
    validation_metrics: Dict[str, Any],
    train_stats: Dict[str, Any],
    train_early_time: float,
    val_time: float,
    train_time: float,
    train_pos: int,
    train_neg: int,
    n_train_full: int,
    best_params: Dict[str, Any],
    start_time: float,
) -> None:
    """Phase 2: Testing phase - loads test.parquet in batches, loads model, evaluates."""
    print("\n" + "=" * 70)
    print("PHASE 2: TESTING (BATCH PROCESSING)")
    print("=" * 70)
    
    # -------------------------------------------------------------------------
    # Load model
    # -------------------------------------------------------------------------
    print(f"\nLoading saved model from: {model_path}")
    with open(model_path, 'rb') as f:
        final_model = pickle.load(f)
    
    # -------------------------------------------------------------------------
    # Batch processing for inference
    # -------------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("BATCH PROCESSING FOR INFERENCE")
    print("=" * 70)
    
    batch_size = 100_000
    print(f"\nProcessing test.parquet in batches of {batch_size:,} rows...")
    
    # Open parquet file for batch iteration
    parquet_file = pq.ParquetFile(test_path)
    
    # Accumulators for results and statistics
    scored_results = []  # List of DataFrames with (row, col, year, y_true, y_pred_proba, x, y)
    y_true_all = []  # Accumulate all y_true for metrics
    y_proba_all = []  # Accumulate all y_proba for metrics
    test_pos = 0
    test_neg = 0
    test_years_set = set()
    n_test_total = 0
    
    pred_start = time.time()
    batch_num = 0
    
    # Iterate over batches
    for batch in parquet_file.iter_batches(batch_size=batch_size, columns=keep_cols_test):
        batch_num += 1
        batch_df = batch.to_pandas()
        
        # Drop rows with missing target
        batch_df = batch_df.dropna(subset=[target_col])
        if len(batch_df) == 0:
            continue
        
        # Downcast numeric dtypes to reduce memory
        downcast_numeric_dtypes(batch_df)
        
        # Extract target and features
        y_batch = (batch_df[target_col] > 0).astype(np.int8)
        X_batch = batch_df[feature_cols].copy()
        
        # Make predictions
        y_proba_batch = final_model.predict_proba(X_batch)[:, 1]
        
        # Accumulate for metrics
        y_true_all.append(y_batch.values)
        y_proba_all.append(y_proba_batch)
        test_pos += int(y_batch.sum())
        test_neg += int((y_batch == 0).sum())
        test_years_set.update(batch_df['year'].unique())
        n_test_total += len(y_batch)
        
        # Create lightweight result DataFrame with only necessary columns
        result_dict = {
            'row': batch_df['row'].values,
            'col': batch_df['col'].values,
            'year': batch_df['year'].values,
            'y_true': y_batch.values,
            'y_pred_proba': y_proba_batch,
        }
        # Add optional coordinate columns if present
        for col in ['x', 'y']:
            if col in batch_df.columns:
                result_dict[col] = batch_df[col].values
        
        scored_results.append(pd.DataFrame(result_dict))
        
        # Free memory immediately
        del batch_df, X_batch, y_batch, y_proba_batch
        gc.collect()
        
        if batch_num % 10 == 0:
            print(f"  Processed {batch_num} batches ({n_test_total:,} rows so far)...")
    
    pred_time = time.time() - pred_start
    print(f"\n  Completed {batch_num} batches in {pred_time:.1f}s")
    print(f"  Total rows processed: {n_test_total:,}")
    
    # Concatenate all results into single DataFrame
    print("\nConcatenating batch results...")
    scored_df = pd.concat(scored_results, ignore_index=True)
    del scored_results
    gc.collect()
    print(f"  Concatenated DataFrame: {len(scored_df):,} rows")
    
    # Concatenate y_true and y_proba for metrics
    y_test = np.concatenate(y_true_all)
    y_proba = np.concatenate(y_proba_all)
    del y_true_all, y_proba_all
    gc.collect()
    
    test_years = sorted(test_years_set)
    
    print(f"\nTest set distribution:")
    print(f"  No transition (0): {test_neg:>12,}  ({100 - test_pos/n_test_total*100:.3f}%)")
    print(f"  Transition (0→1):  {test_pos:>12,}  ({test_pos/n_test_total*100:.3f}%)")
    print(f"  Class ratio:       1 : {test_neg / max(test_pos, 1):.1f}")
    print(f"  Years: {test_years[0]}-{test_years[-1]}")
    
    # Log data statistics to wandb
    if use_wandb:
        wandb.log({
            "data/n_test": int(n_test_total),
            "data/test_positives": test_pos,
            "data/test_negatives": test_neg,
            "data/test_positive_pct": test_pos/n_test_total*100,
        })
    
    # -------------------------------------------------------------------------
    # Compute metrics
    # -------------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("EVALUATE ON TEST SET (2018-2024)")
    print("=" * 70)
    
    print("\nComputing metrics...")
    test_metrics = compute_metrics(y_test, y_proba)
    baseline_rate = test_metrics['baseline_rate']
    prec_at_k = {k: test_metrics[f'precision_at_{k}pct'] for k in [1, 5, 10]}
    
    print(f"\n" + "-" * 40)
    print("Area Under Curve Metrics:")
    print("-" * 40)
    print(f"ROC-AUC: {test_metrics['roc_auc']:.4f}")
    print(f"PR-AUC:  {test_metrics['pr_auc']:.4f}")
    
    print(f"\n" + "-" * 40)
    print("Precision @ Top-K Predictions:")
    print("-" * 40)
    for k in [1, 5, 10]:
        n_top_k = max(1, int(len(y_test) * k / 100))
        n_protected_in_top_k = int(n_top_k * prec_at_k[k])
        print(f"  Precision @ top {k:>2}%:  {prec_at_k[k]:.4f}  "
              f"({n_protected_in_top_k:>7,} transitions in top {n_top_k:>9,})")
    print(f"\n  Baseline (random):    {baseline_rate:.4f}")
    print("\nLift over baseline:")
    for k in [1, 5, 10]:
        print(f"  Top {k:>2}%: {test_metrics[f'lift_at_{k}pct']:.2f}x")
    
    if use_wandb:
        wandb.log({**{f"test/{k}": v for k, v in test_metrics.items()}})
    
    print("\n" + "-" * 40)
    print("Top 20 Most Important Features:")
    print("-" * 40)
    feature_importance = pd.DataFrame({
        'feature': feature_cols, 'importance': final_model.feature_importances_
    }).sort_values('importance', ascending=False).head(20)
    print(feature_importance.to_string(index=False))
    
    # -------------------------------------------------------------------------
    # Save predictions and metrics
    # -------------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("SAVE PREDICTIONS AND METRICS")
    print("=" * 70)
    
    # Save scored outputs
    scored_path = output_dir / f"model1_lgbm_scored_{timestamp}.parquet"
    scored_df.to_parquet(scored_path, index=False)
    print(f"\nScored outputs saved to: {scored_path}")
    
    # Compile metrics
    metrics = {
        "metadata": {
            "timestamp": timestamp,
            "model": "LightGBM",
            "task": "transition_01_prediction",
            "random_state": RANDOM_STATE,
            "n_features": len(feature_cols),
            "features": feature_cols,
        },
        "data": {
            "n_train_full": n_train_full,
            "n_test": int(n_test_total),
            "train_positives": train_pos,
            "train_negatives": train_neg,
            "test_positives": test_pos,
            "test_negatives": test_neg,
        },
        "temporal_split": {
            "method": "strict_temporal_split",
            "train_early_max_year": TRAIN_EARLY_YEAR_MAX,
            "val_late_min_year": VAL_LATE_YEAR_MIN,
            "val_late_max_year": VAL_LATE_YEAR_MAX,
        },
        "validation_performance": validation_metrics,
        "model_parameters": best_params,
        "test_performance": test_metrics,
        "feature_importance": feature_importance.head(20).to_dict('records'),
        "timing": {
            "train_early_seconds": train_early_time,
            "validation_seconds": val_time,
            "final_training_seconds": train_time,
            "prediction_seconds": pred_time,
            "total_seconds": time.time() - start_time,
        },
    }
    
    # Convert NumPy types to native Python types
    metrics = convert_numpy_types(metrics)
    
    # Save metrics
    metrics_path = output_dir / f"model1_lgbm_metrics_{timestamp}.json"
    with open(metrics_path, 'w') as f:
        json.dump(metrics, f, indent=2)
    print(f"Metrics saved to: {metrics_path}")
    
    # Free memory
    del scored_df, y_test, y_proba, final_model
    gc.collect()
    
    # -------------------------------------------------------------------------
    # Summary
    # -------------------------------------------------------------------------
    total_time = time.time() - start_time
    
    print("\n" + "=" * 70)
    print("SUMMARY")
    print("=" * 70)
    print(f"Model:                       LightGBM")
    print(f"Parameters:                  {best_params}")
    print(f"Features used:               {len(feature_cols)}")
    print(f"\nTemporal Split (Internal Validation):")
    print(f"  train_early (≤{TRAIN_EARLY_YEAR_MAX}) → val_late ({VAL_LATE_YEAR_MIN}-{VAL_LATE_YEAR_MAX})")
    print(f"  Validation PR-AUC:         {validation_metrics['pr_auc']:.4f}")
    print(f"  Validation ROC-AUC:        {validation_metrics['roc_auc']:.4f}")
    print(f"\nTest Set Performance (2018-2024):")
    print(f"  Samples:                   {n_test_total:,}")
    print(f"  Positive rate:             {test_pos/n_test_total*100:.3f}%")
    print(f"  ROC-AUC:                   {test_metrics['roc_auc']:.4f}")
    print(f"  PR-AUC:                    {test_metrics['pr_auc']:.4f}")
    print(f"  Precision @ top 1%:        {prec_at_k[1]:.4f} ({test_metrics['lift_at_1pct']:.1f}x lift)")
    print(f"  Precision @ top 5%:        {prec_at_k[5]:.4f} ({test_metrics['lift_at_5pct']:.1f}x lift)")
    print(f"  Precision @ top 10%:       {prec_at_k[10]:.4f} ({test_metrics['lift_at_10pct']:.1f}x lift)")
    print(f"\nTimings:")
    print(f"  Train early (≤{TRAIN_EARLY_YEAR_MAX}):     {train_early_time:.1f}s ({train_early_time/60:.1f} min)")
    print(f"  Validation (2015-2017):    {val_time:.1f}s")
    print(f"  Final training (2000-2017): {train_time:.1f}s ({train_time/60:.1f} min)")
    print(f"  Prediction (batch):        {pred_time:.1f}s ({pred_time/60:.1f} min)")
    print(f"  Total time:                {total_time:.1f}s ({total_time/60:.1f} min)")
    print("=" * 70)
    print("Done.")
    
    # Log final summary
    if use_wandb:
        wandb.log({
            "summary/total_time_seconds": total_time,
            "summary/total_time_minutes": total_time / 60,
            "status": "success"
        })
        wandb.finish()


def main() -> None:
    start_time = time.time()
    
    # -------------------------------------------------------------------------
    # Setup paths
    # -------------------------------------------------------------------------
    repo_root = Path(__file__).resolve().parents[3]
    
    # Input paths
    train_path = resolve_parquet_file("train.parquet")
    test_path = resolve_parquet_file("test.parquet")
    params_path = resolve_best_params_json()
    
    # Output directories
    output_dir = repo_root / "outputs/Results/ml_models"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    model_dir = repo_root / "data/ml/models"
    model_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    use_wandb = False
    try:
        print("Initializing Weights & Biases...")
        wandb.init(
            project="ml-training-transitions",
            entity=os.environ.get("WANDB_ENTITY"),
            name=f"model1_lgbm_{timestamp}",
            config={
                "model": "LightGBM", "task": "transition_01_prediction",
                "random_state": RANDOM_STATE,
                "temporal_split": {
                    "train_early_max_year": TRAIN_EARLY_YEAR_MAX,
                    "val_late_min_year": VAL_LATE_YEAR_MIN,
                    "val_late_max_year": VAL_LATE_YEAR_MAX,
                },
            },
        )
        use_wandb = True
        print("W&B connected\n")
    except Exception as err:
        print(f"W&B initialization failed: {err}\n")
    
    print("=" * 70)
    print("LGBM TRANSITION MODEL (0→1 PREDICTION)")
    print("=" * 70)
    print(f"\nTrain:  {train_path}")
    print(f"Test:   {test_path}")
    print(f"Params: {params_path}")
    print(f"Output: {output_dir}")
    
    # -------------------------------------------------------------------------
    # Step 1: Load best parameters
    # -------------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("STEP 1: LOAD BEST PARAMETERS")
    print("=" * 70)
    
    best_params, best_cv_score = load_best_params(params_path)
    print(f"\nLoaded parameters from: {params_path}")
    print(f"\nBest parameters:")
    for param, value in sorted(best_params.items()):
        if param != 'verbose':
            print(f"  {param}: {value}")
    if best_cv_score is not None:
        print(f"\nBest CV score (from tuning): {best_cv_score:.4f}")
    
    # -------------------------------------------------------------------------
    # Step 2: Schema checks (lazy loading - read schema only)
    # -------------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("STEP 2: SCHEMA CHECKS (LAZY LOADING)")
    print("=" * 70)
    
    print(f"\nReading column names from parquet files...")
    all_cols_train = pq.ParquetFile(train_path).schema_arrow.names
    all_cols_test = pq.ParquetFile(test_path).schema_arrow.names
    
    target_col = "transition_01"
    df_schema = pd.read_parquet(train_path, columns=all_cols_train).head(1)
    numeric_cols = df_schema.select_dtypes(include=['number']).columns.tolist()
    feature_cols = [col for col in numeric_cols if col.lower() not in {c.lower() for c in EXCLUDE_COLS}]
    
    required_cols = [target_col, 'year', 'row', 'col']
    optional_cols = [col for col in ['x', 'y'] if col in all_cols_train]
    keep_cols_train = feature_cols + required_cols + optional_cols
    keep_cols_test = feature_cols + required_cols + [col for col in ['x', 'y'] if col in all_cols_test]
    
    print(f"  Selected {len(feature_cols)} feature columns")
    print(f"  Will load {len(keep_cols_train)} columns from train, {len(keep_cols_test)} from test")
    del df_schema
    gc.collect()
    
    # -------------------------------------------------------------------------
    # Phase 1: Training
    # -------------------------------------------------------------------------
    (model_path, validation_metrics, train_stats, train_early_time, 
     val_time, train_time, train_pos, train_neg, n_train_full) = phase1_training(
        train_path=train_path,
        params_path=params_path,
        feature_cols=feature_cols,
        keep_cols_train=keep_cols_train,
        best_params=best_params,
        timestamp=timestamp,
        model_dir=model_dir,
        output_dir=output_dir,
        use_wandb=use_wandb,
        target_col=target_col,
    )
    
    # -------------------------------------------------------------------------
    # Phase 2: Testing
    # -------------------------------------------------------------------------
    phase2_testing(
        test_path=test_path,
        model_path=model_path,
        feature_cols=feature_cols,
        keep_cols_test=keep_cols_test,
        timestamp=timestamp,
        output_dir=output_dir,
        use_wandb=use_wandb,
        target_col=target_col,
        validation_metrics=validation_metrics,
        train_stats=train_stats,
        train_early_time=train_early_time,
        val_time=val_time,
        train_time=train_time,
        train_pos=train_pos,
        train_neg=train_neg,
        n_train_full=n_train_full,
        best_params=best_params,
        start_time=start_time,
    )


if __name__ == "__main__":
    # Set up output file capture
    repo_root = Path(__file__).resolve().parents[3]
    output_dir = repo_root / "outputs/Results/ml_models"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"model1_lgbm_{timestamp}.txt"
    
    tee = Tee(output_file)
    sys.stdout = tee
    
    try:
        main()
    finally:
        sys.stdout = tee.stdout
        tee.close()
        print(f"\nOutput saved to: {output_file}")
