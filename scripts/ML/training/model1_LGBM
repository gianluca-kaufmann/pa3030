#!/usr/bin/env python3
"""LGBM Transition Model (0→1 Prediction)
Train LightGBM on pre-split parquet files: train_early_sampled (2000-2014), 
val_late_full (2015-2017), test_full (2018-2024). Final model trained on 2000-2017,
evaluated on 2018-2024. Uses isotonic calibration fitted on val_late predictions.
"""

from __future__ import annotations

import gc
import json
import os
import sys
import time
import pickle
from datetime import datetime
from pathlib import Path
from typing import Dict, Any

import numpy as np
import pandas as pd
import wandb
import pyarrow as pa
import pyarrow.parquet as pq
import lightgbm as lgb
from sklearn.metrics import roc_auc_score, average_precision_score
from sklearn.calibration import CalibratedClassifierCV


class Tee:
    """Write to both stdout and a file."""
    def __init__(self, file_path: Path):
        self.file = open(file_path, 'w', encoding='utf-8')
        self.stdout = sys.stdout
    def write(self, text: str) -> None:
        self.stdout.write(text)
        self.file.write(text)
        self.stdout.flush()
        self.file.flush()
    def flush(self) -> None:
        self.stdout.flush()
        self.file.flush()
    def close(self) -> None:
        self.file.close()


class LGBMBoosterWrapper:
    """Wrapper to make LightGBM Booster sklearn-compatible for calibration."""
    def __init__(self, booster: lgb.Booster):
        self.booster = booster
    
    def predict_proba(self, X: np.ndarray | pd.DataFrame) -> np.ndarray:
        """Return probabilities in sklearn format [n_samples, 2]."""
        proba_positive = self.booster.predict(X)
        proba_positive = np.clip(proba_positive, 0.0, 1.0)  # Ensure valid probabilities
        proba_negative = 1.0 - proba_positive
        return np.column_stack([proba_negative, proba_positive])

# Configuration
RANDOM_STATE = 42
EXCLUDE_COLS = {'transition_01', 'WDPA_b1', 'WDPA_prev', 'x', 'y', 'row', 'col'}
TRAIN_EARLY_YEAR_MAX = 2014
VAL_LATE_YEAR_MIN = 2015
VAL_LATE_YEAR_MAX = 2017
FIXED_PARAMS = {'random_state': RANDOM_STATE, 'boosting_type': 'gbdt', 'objective': 'binary', 'verbose': -1}

# Utility Functions

def get_n_jobs() -> int:
    """Get number of CPUs (-1 for all available, may limit for memory efficiency)."""
    slurm_cpus = os.environ.get("SLURM_CPUS_PER_TASK")
    n_jobs = -1
    if slurm_cpus:
        try:
            n_jobs = int(slurm_cpus)
        except ValueError:
            n_jobs = -1
    
    # Check for memory-constrained mode
    memory_constrained = os.environ.get("MEMORY_CONSTRAINED", "").lower() in ("1", "true", "yes")
    if memory_constrained and n_jobs < 0:
        # Limit to fewer cores in memory-constrained environments
        try:
            import os as os_module
            n_jobs = max(1, os_module.cpu_count() // 2)
            print(f"  [MEMORY_CONSTRAINED mode] Limiting to {n_jobs} cores")
        except:
            n_jobs = 4
    
    return n_jobs


def report_memory_usage(label: str = "") -> None:
    """Report current memory usage."""
    try:
        import psutil
        process = psutil.Process()
        mem_info = process.memory_info()
        mem_gb = mem_info.rss / 1024**3
        print(f"  [Memory {label}] RSS: {mem_gb:.2f} GB")
    except ImportError:
        pass  # psutil not available


def convert_numpy_types(obj: Any) -> Any:
    """Recursively convert NumPy types to native Python types for JSON serialization."""
    if isinstance(obj, (np.integer, np.int_, np.intc, np.intp, np.int8, np.int16, np.int32, 
                        np.int64, np.uint8, np.uint16, np.uint32, np.uint64)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float_, np.float16, np.float32, np.float64)):
        return float(obj)
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [convert_numpy_types(item) for item in obj]
    elif obj is None or isinstance(obj, (str, int, float, bool)):
        return obj
    else:
        try:
            return obj.item() if hasattr(obj, 'item') else obj
        except (ValueError, TypeError, AttributeError):
            return str(obj)


def compute_precision_at_k(y_true: np.ndarray, y_proba: np.ndarray, k: float) -> float:
    """Compute precision at top k% of predictions."""
    n_top_k = max(1, int(len(y_true) * k / 100))
    top_k_idx = np.argsort(y_proba)[-n_top_k:]
    return y_true[top_k_idx].sum() / n_top_k


def resolve_parquet_file(filename: str) -> Path:
    """Locate parquet file (prefer $SCRATCH if present)."""
    repo_root = Path(__file__).resolve().parents[3]
    scratch_root = Path(os.environ["SCRATCH"]) if os.environ.get("SCRATCH") else None

    candidates = []
    if scratch_root is not None:
        candidates.append(scratch_root / f"data/ml/{filename}")
    candidates.append(repo_root / f"data/ml/{filename}")

    for cand in candidates:
        if cand.exists():
            return cand

    raise FileNotFoundError(f"{filename} not found in expected locations")


def resolve_best_params_json() -> Path:
    """Locate lgbm_best_params.json (in same directory as this script)."""
    script_dir = Path(__file__).resolve().parent
    params_path = script_dir / "lgbm_best_params.json"
    
    if not params_path.exists():
        raise FileNotFoundError(f"lgbm_best_params.json not found at {params_path}")
    
    return params_path


def resolve_train_metadata() -> Path:
    """Locate train_early_metadata.json (prefer $SCRATCH if present)."""
    repo_root = Path(__file__).resolve().parents[3]
    scratch_root = Path(os.environ["SCRATCH"]) if os.environ.get("SCRATCH") else None

    candidates = []
    if scratch_root is not None:
        candidates.append(scratch_root / "data/ml/train_early_metadata.json")
    candidates.append(repo_root / "data/ml/train_early_metadata.json")

    for cand in candidates:
        if cand.exists():
            return cand

    raise FileNotFoundError("train_early_metadata.json not found in expected locations")


def load_best_params(params_path: Path) -> tuple[Dict[str, Any], Any]:
    """Load and merge parameters: FIXED_PARAMS → json_fixed_params → best_params → guardrails."""
    with open(params_path, 'r') as f:
        params_data = json.load(f)
    final_params = {**FIXED_PARAMS, **params_data.get('fixed_params', {}), **params_data.get('best_params', {})}
    guardrail_params = {"max_depth": 8, "num_leaves": 255, "min_child_samples": 50, "subsample": 0.7,
                       "subsample_freq": 1, "colsample_bytree": 0.7, "learning_rate": 0.03, "n_estimators": 3000,
                       "reg_alpha": 1.0, "reg_lambda": 1.0, "metric": "average_precision", "n_jobs": get_n_jobs(), "verbose": -1}
    final_params.update(guardrail_params)
    return final_params, params_data.get('best_cv_score', None)


def downcast_numeric_dtypes(df: pd.DataFrame) -> None:
    """Downcast numeric dtypes to reduce memory usage (in-place)."""
    for col in df.select_dtypes(include=['float64']).columns:
        df[col] = df[col].astype('float32', copy=False)
    for col in df.select_dtypes(include=['int64']).columns:
        df[col] = df[col].astype('int32', copy=False)


def load_and_preprocess(df_path: Path, columns: list, target_col: str, name: str = "") -> pd.DataFrame:
    """Load, downcast, and preprocess DataFrame."""
    print(f"\nLoading {name or df_path.name}...")
    load_start = time.time()
    df = pd.read_parquet(df_path, columns=columns)
    mem_before = df.memory_usage(deep=True).sum() / 1024**3
    print(f"  Loaded {len(df):,} rows in {time.time() - load_start:.1f}s ({mem_before:.2f} GB). Downcasting...")
    downcast_numeric_dtypes(df)
    mem_after = df.memory_usage(deep=True).sum() / 1024**3
    print(f"  After downcast: {mem_after:.2f} GB (saved {mem_before - mem_after:.2f} GB)")
    gc.collect()
    if target_col not in df.columns:
        raise ValueError(f"Target column '{target_col}' not found")
    df.dropna(subset=[target_col], inplace=True)
    print(f"  {len(df):,} rows after dropna")
    gc.collect()
    return df


def extract_split(df: pd.DataFrame, feature_cols: list, target_col: str, year_filter, split_name: str) -> tuple:
    """Extract X, y and stats from filtered DataFrame."""
    df_split = df[year_filter(df['year'])].copy()
    y = (df_split[target_col] > 0).astype(np.int8)
    X = df_split[feature_cols].copy()
    pos, neg = int(y.sum()), int((y == 0).sum())
    years = sorted(df_split['year'].unique())
    print(f"\n{split_name}: {neg:,} neg ({100-pos/len(y)*100:.3f}%), {pos:,} pos ({pos/len(y)*100:.3f}%), ratio 1:{neg/max(pos,1):.1f}, years {years[0]}-{years[-1]}")
    return X, y, pos, neg


def compute_metrics(y_true: np.ndarray, y_proba: np.ndarray) -> Dict[str, float]:
    """Compute all validation/test metrics."""
    roc_auc = roc_auc_score(y_true, y_proba)
    pr_auc = average_precision_score(y_true, y_proba)
    baseline_rate = y_true.mean()
    prec_at_k = {k: compute_precision_at_k(y_true, y_proba, k) for k in [1, 5, 10]}
    return {"roc_auc": float(roc_auc), "pr_auc": float(pr_auc), "precision_at_1pct": float(prec_at_k[1]),
            "precision_at_5pct": float(prec_at_k[5]), "precision_at_10pct": float(prec_at_k[10]),
            "baseline_rate": float(baseline_rate),
            "lift_at_1pct": float(prec_at_k[1] / baseline_rate) if baseline_rate > 0 else 0,
            "lift_at_5pct": float(prec_at_k[5] / baseline_rate) if baseline_rate > 0 else 0,
            "lift_at_10pct": float(prec_at_k[10] / baseline_rate) if baseline_rate > 0 else 0}


# =============================================================================
# Main Pipeline
# =============================================================================

def phase1_training(
    train_path: Path, val_path: Path, params_path: Path, feature_cols: list, keep_cols_train: list,
    best_params: Dict[str, Any], timestamp: str, model_dir: Path, output_dir: Path, use_wandb: bool, target_col: str,
) -> tuple[Path, Dict[str, Any], Dict[str, Any], float, float, float, float, int, int, Dict[str, Any]]:
    """Phase 1: Training - loads train/val splits, trains model, saves it."""
    print("\n" + "="*70 + "\nPHASE 1: TRAINING\n" + "="*70)
    report_memory_usage("start of Phase 1")
    print("\nSTEP A: LOAD TRAIN_EARLY (≤2014)")
    # Only load essential columns for initial training (no x, y coordinates needed)
    essential_cols = feature_cols + [target_col, 'year', 'row', 'col']
    df_train = load_and_preprocess(train_path, essential_cols, target_col, "train_early_sampled.parquet")
    X_train_early, y_train_early, train_early_pos, train_early_neg = extract_split(
        df_train, feature_cols, target_col, lambda y: y <= TRAIN_EARLY_YEAR_MAX, "Train early")
    del df_train; gc.collect()
    report_memory_usage("after STEP A")
    
    print("\nSTEP B: LOAD VAL_LATE (2015-2017)")
    df_val = load_and_preprocess(val_path, essential_cols, target_col, "val_late_full.parquet")
    X_val_late, y_val_late, val_late_pos, val_late_neg = extract_split(
        df_val, feature_cols, target_col, lambda y: (y >= VAL_LATE_YEAR_MIN) & (y <= VAL_LATE_YEAR_MAX), "Val late")
    del df_val; gc.collect()
    
    print(f"\nSTEP C: TRAIN & EVALUATE\nTemporal split: train_early (≤{TRAIN_EARLY_YEAR_MAX}): {len(X_train_early):,} rows, val_late ({VAL_LATE_YEAR_MIN}-{VAL_LATE_YEAR_MAX}): {len(X_val_late):,} rows")
    
    # Imbalance handling: load scale_pos_weight from preprocessing metadata
    metadata_path = resolve_train_metadata()
    with open(metadata_path, 'r') as f:
        train_metadata = json.load(f)
    scale_pos_weight = train_metadata["scale_pos_weight"]
    
    neg_full, pos_full = val_late_neg, val_late_pos
    neg_sampled, pos_sampled = train_early_neg, train_early_pos
    ratio_full = float("inf") if pos_full == 0 else neg_full / pos_full
    ratio_sampled = float("inf") if pos_sampled == 0 else neg_sampled / pos_sampled
    print(f"\nImbalance: sampled ratio {ratio_sampled:.3f}, full ratio {ratio_full:.3f}, scale_pos_weight {scale_pos_weight:.6f} (from preprocessing)")
    best_params["is_unbalance"] = False
    best_params["scale_pos_weight"] = scale_pos_weight
    imbalance_info = {"neg_full": int(neg_full), "pos_full": int(pos_full), "neg_sampled": int(neg_sampled),
                     "pos_sampled": int(pos_sampled), "ratio_full": float(ratio_full),
                     "ratio_sampled": float(ratio_sampled), "scale_pos_weight": float(scale_pos_weight)}
    if use_wandb:
        wandb.log({"imbalance/neg_pos_ratio_full": float(ratio_full), "imbalance/neg_pos_ratio_sampled": float(ratio_sampled),
                  "imbalance/scale_pos_weight": float(scale_pos_weight)})
    
    print(f"\nTraining on train_early ({len(X_train_early):,} samples)...")
    report_memory_usage("before training")
    train_early_start = time.time()
    train_params = best_params.copy()
    num_boost_round_val = train_params.pop("num_boost_round", train_params.pop("n_estimators", 3000))
    train_params.pop("n_estimators", None)
    
    # Add memory-efficient LightGBM parameters
    train_params['force_col_wise'] = True  # More memory efficient for many features
    train_params['max_bin'] = 255  # Reduce memory usage
    
    # Create datasets with free_raw_data=True to reduce memory usage
    # Keep validation data in memory for later prediction (don't copy to save memory)
    print(f"  Creating train dataset...")
    train_dataset = lgb.Dataset(X_train_early, label=y_train_early, free_raw_data=True)
    del X_train_early, y_train_early; gc.collect()
    report_memory_usage("after train dataset creation")
    
    print(f"  Creating validation dataset...")
    val_dataset = lgb.Dataset(X_val_late, label=y_val_late, free_raw_data=True, reference=train_dataset)
    # Don't delete X_val_late, y_val_late yet - needed for metrics after training
    report_memory_usage("after val dataset creation")
    
    print(f"  Starting training...")
    lgb_model = lgb.train(train_params, train_dataset, num_boost_round=num_boost_round_val, valid_sets=[val_dataset],
                         valid_names=["val_late"], callbacks=[lgb.early_stopping(100), lgb.log_evaluation(100)])
    train_early_time = time.time() - train_early_start
    best_iteration = lgb_model.best_iteration
    print(f"\nTraining done in {train_early_time:.1f}s. Best iteration: {best_iteration if best_iteration else 'N/A'}")
    
    # Free datasets after training
    del train_dataset, val_dataset; gc.collect()
    
    val_start = time.time()
    y_val_proba = lgb_model.predict(X_val_late, num_iteration=best_iteration if best_iteration else None)
    val_time = time.time() - val_start
    validation_metrics = compute_metrics(y_val_late.values, y_val_proba)
    print(f"\nValidation: ROC-AUC {validation_metrics['roc_auc']:.4f}, PR-AUC {validation_metrics['pr_auc']:.4f}, "
          f"P@1% {validation_metrics['precision_at_1pct']:.4f}, P@5% {validation_metrics['precision_at_5pct']:.4f}, "
          f"P@10% {validation_metrics['precision_at_10pct']:.4f}")
    
    if use_wandb:
        wandb.log({**{f"val/{k}": v for k, v in validation_metrics.items()}, "val/n_samples": int(len(y_val_late)),
                  "val/n_positives": int(y_val_late.sum()), "val/time_seconds": val_time})
    
    # Now free validation data and predictions
    del X_val_late, y_val_late, y_val_proba; gc.collect()
    
    print("\nSTEP D: LOAD & TRAIN FINAL MODEL (2000-2017)")
    
    # First, load val_late to extract calibration data
    print("  Loading val_late for calibration data...")
    df_val_full = load_and_preprocess(val_path, keep_cols_train, target_col, "val_late_full.parquet")
    df_val_late_for_cal = df_val_full[df_val_full['year'].between(VAL_LATE_YEAR_MIN, VAL_LATE_YEAR_MAX)].copy()
    y_val_late_for_cal = (df_val_late_for_cal[target_col] > 0).astype(np.int8)
    X_val_late_for_cal = df_val_late_for_cal[feature_cols].copy()
    del df_val_late_for_cal; gc.collect()
    
    # Extract val features and targets, then free dataframe
    val_full_len = len(df_val_full)
    val_years = df_val_full['year'].unique().tolist()
    y_val_full = (df_val_full[target_col] > 0).astype(np.int8)
    X_val_full = df_val_full[feature_cols].copy()
    del df_val_full; gc.collect()
    print(f"  Val data extracted: {val_full_len:,} rows, {X_val_full.memory_usage(deep=True).sum() / 1024**3:.2f} GB")
    
    # Now load train data
    print("  Loading train_early...")
    df_train_full = load_and_preprocess(train_path, keep_cols_train, target_col, "train_early_sampled.parquet")
    train_full_len = len(df_train_full)
    train_years = df_train_full['year'].unique().tolist()
    
    # Extract train features and targets, then free dataframe immediately
    y_train_full = (df_train_full[target_col] > 0).astype(np.int8)
    X_train_full = df_train_full[feature_cols].copy()
    del df_train_full; gc.collect()
    print(f"  Train data extracted: {train_full_len:,} rows, {X_train_full.memory_usage(deep=True).sum() / 1024**3:.2f} GB")
    
    # Combine years info
    train_years = sorted(set(train_years + val_years))
    del val_years; gc.collect()
    
    print(f"  Combining train_early ({train_full_len:,} rows) + val_late ({val_full_len:,} rows)...")
    combine_start = time.time()
    
    # Concatenate features and targets
    X = pd.concat([X_train_full, X_val_full], ignore_index=True)
    del X_train_full, X_val_full; gc.collect()
    print(f"  X combined, memory: {X.memory_usage(deep=True).sum() / 1024**3:.2f} GB")
    
    y = pd.concat([y_train_full, y_val_full], ignore_index=True)
    del y_train_full, y_val_full; gc.collect()
    
    train_pos, train_neg = int(y.sum()), int((y == 0).sum())
    n_train_full = len(X)
    
    print(f"  Combined in {time.time() - combine_start:.1f}s")
    
    # Report memory usage before creating LightGBM dataset
    mem_X = X.memory_usage(deep=True).sum() / 1024**3
    mem_y = y.memory_usage(deep=True) / 1024**3 if hasattr(y, 'memory_usage') else (y.nbytes / 1024**3)
    print(f"  Combined dataset memory: X={mem_X:.2f} GB, y={mem_y:.3f} GB, total={mem_X + mem_y:.2f} GB")
    print(f"\nFull training: {train_neg:,} neg, {train_pos:,} pos, ratio 1:{train_neg/max(train_pos,1):.1f}, years {train_years[0]}-{train_years[-1]}")
    train_stats = {"n_train": n_train_full, "train_positives": train_pos, "train_negatives": train_neg,
                  "train_positive_pct": train_pos/len(y)*100, "train_years": f"{train_years[0]}-{train_years[-1]}"}
    if use_wandb:
        wandb.log({"data/n_features": len(feature_cols), "data/n_train": n_train_full, "data/train_positives": train_pos,
                  "data/train_negatives": train_neg, "data/train_positive_pct": train_pos/len(y)*100})
    
    num_boost_round = int(best_iteration * 1.2) if best_iteration else int(best_params.get("n_estimators", 3000))
    print(f"Training final model ({len(X):,} samples, num_boost_round={num_boost_round})...")
    report_memory_usage("before final training")
    
    final_train_params = best_params.copy()
    final_train_params.pop("n_estimators", None)
    
    # Ensure memory-efficient LightGBM settings for large datasets
    final_train_params['max_bin'] = 255  # Reduce memory usage
    final_train_params['force_col_wise'] = True  # More memory efficient
    
    train_start = time.time()
    print(f"  Creating LightGBM dataset...")
    dataset_start = time.time()
    # Use free_raw_data=True to free memory after dataset construction
    train_dataset = lgb.Dataset(X, label=y, free_raw_data=True)
    print(f"  Dataset created in {time.time() - dataset_start:.1f}s")
    report_memory_usage("after dataset creation")
    
    # Free X and y after dataset creation
    del X, y; gc.collect()
    report_memory_usage("after freeing X, y")
    
    print(f"  Training...")
    
    final_model = lgb.train(final_train_params, train_dataset, num_boost_round=num_boost_round, valid_sets=None)
    train_time = time.time() - train_start
    print(f"Training done in {train_time:.1f}s ({train_time/60:.1f} min)")
    report_memory_usage("after training")
    
    # Free dataset after training
    del train_dataset; gc.collect()
    report_memory_usage("after freeing dataset")
    
    model_path = model_dir / f"model1_lgbm_{timestamp}.pkl"
    with open(model_path, 'wb') as f:
        pickle.dump(final_model, f)
    print(f"Model saved: {model_path}")
    gc.collect()
    
    # Fit isotonic calibration on val_late predictions
    print(f"\nSTEP E: FITTING ISOTONIC CALIBRATION ON VAL_LATE")
    print(f"  Calibrating on {len(X_val_late_for_cal):,} val_late samples...")
    cal_start = time.time()
    wrapped_model = LGBMBoosterWrapper(final_model)
    calibrated_model = CalibratedClassifierCV(wrapped_model, method='isotonic', cv='prefit')
    calibrated_model.fit(X_val_late_for_cal, y_val_late_for_cal)
    cal_time = time.time() - cal_start
    print(f"  Calibration done in {cal_time:.1f}s")
    
    calibrated_model_path = model_dir / f"model1_lgbm_calibrated_{timestamp}.pkl"
    with open(calibrated_model_path, 'wb') as f:
        pickle.dump(calibrated_model, f)
    print(f"  Calibrated model saved: {calibrated_model_path}")
    
    del X_val_late_for_cal, y_val_late_for_cal, wrapped_model; gc.collect()
    report_memory_usage("after calibration")
    
    if use_wandb:
        wandb.log({"training/train_early_time_seconds": train_early_time, "training/train_early_time_minutes": train_early_time / 60,
                  "training/final_train_time_seconds": train_time, "training/final_train_time_minutes": train_time / 60,
                  "training/final_num_boost_round": num_boost_round,
                  "calibration/time_seconds": cal_time})
    
    print("Phase 1 completed.")
    return (calibrated_model_path, validation_metrics, train_stats, train_early_time, val_time, train_time, train_pos, train_neg, n_train_full, imbalance_info)


def phase2_testing(
    test_path: Path, model_path: Path, feature_cols: list, keep_cols_test: list, timestamp: str, output_dir: Path,
    use_wandb: bool, target_col: str, validation_metrics: Dict[str, Any], train_stats: Dict[str, Any],
    train_early_time: float, val_time: float, train_time: float, train_pos: int, train_neg: int, n_train_full: int,
    best_params: Dict[str, Any], start_time: float, imbalance_info: Dict[str, Any],
) -> None:
    """Phase 2: Testing - loads test in batches, loads calibrated model, evaluates."""
    print("\n" + "="*70 + "\nPHASE 2: TESTING\n" + "="*70)
    report_memory_usage("start of Phase 2")
    print(f"\nLoading calibrated model from {model_path}")
    with open(model_path, 'rb') as f:
        calibrated_model = pickle.load(f)
    report_memory_usage("after loading model")
    
    # Use smaller batch size to reduce memory pressure
    batch_size = 50_000
    print(f"Processing test in batches of {batch_size:,}...")
    parquet_file = pq.ParquetFile(test_path)
    scored_path = output_dir / f"model1_lgbm_scored_{timestamp}.parquet"
    writer = None
    y_true_all, y_proba_all = [], []
    test_pos = test_neg = n_test_total = batch_num = 0
    test_years_set = set()
    pred_start = time.time()
    
    try:
        for batch in parquet_file.iter_batches(batch_size=batch_size, columns=keep_cols_test):
            batch_num += 1
            batch_df = batch.to_pandas().dropna(subset=[target_col])
            if len(batch_df) == 0:
                continue
            downcast_numeric_dtypes(batch_df)
            y_batch = (batch_df[target_col] > 0).astype(np.int8)
            X_batch = batch_df[feature_cols].copy()
            # Calibrated model returns [n_samples, 2] with [neg_prob, pos_prob], extract pos_prob (column 1)
            y_proba_batch = calibrated_model.predict_proba(X_batch)[:, 1].astype(np.float32)
            y_true_all.append(y_batch.values)
            y_proba_all.append(y_proba_batch)
            test_pos += int(y_batch.sum())
            test_neg += int((y_batch == 0).sum())
            test_years_set.update(batch_df['year'].unique())
            n_test_total += len(y_batch)
            result_dict = {'row': batch_df['row'].values, 'col': batch_df['col'].values, 'year': batch_df['year'].values,
                          'y_true': y_batch.values, 'y_pred_proba': y_proba_batch}
            for col in ['x', 'y']:
                if col in batch_df.columns:
                    result_dict[col] = batch_df[col].values
            result_df = pd.DataFrame(result_dict)
            result_table = pa.Table.from_pandas(result_df, preserve_index=False)
            if writer is None:
                writer = pq.ParquetWriter(scored_path, result_table.schema)
                print(f"Writing to {scored_path}")
            writer.write_table(result_table)
            del batch_df, X_batch, y_batch, y_proba_batch, result_df, result_table, batch
            # More aggressive garbage collection during batch processing
            gc.collect()
            if batch_num % 10 == 0:
                print(f"  {batch_num} batches, {n_test_total:,} rows")
                report_memory_usage(f"batch {batch_num}")
            elif batch_num % 50 == 0:
                # Extra cleanup every 50 batches
                gc.collect()
    finally:
        if writer is not None:
            writer.close()
        del parquet_file; gc.collect()
    
    pred_time = time.time() - pred_start
    print(f"Completed {batch_num} batches in {pred_time:.1f}s, {n_test_total:,} rows")
    report_memory_usage("after all predictions")
    
    # Concatenate predictions with memory-efficient dtypes
    print("  Concatenating predictions...")
    y_test = np.concatenate(y_true_all).astype(np.int8)
    y_proba = np.concatenate(y_proba_all).astype(np.float32)
    del y_true_all, y_proba_all; gc.collect()
    test_years = sorted(test_years_set)
    del test_years_set; gc.collect()
    report_memory_usage("after concatenation")
    print(f"\nTest set: {test_neg:,} neg, {test_pos:,} pos, ratio 1:{test_neg/max(test_pos,1):.1f}, years {test_years[0]}-{test_years[-1]}")
    
    if use_wandb:
        wandb.log({"data/n_test": int(n_test_total), "data/test_positives": test_pos, "data/test_negatives": test_neg,
                  "data/test_positive_pct": test_pos/n_test_total*100})
    
    test_metrics = compute_metrics(y_test, y_proba)
    baseline_rate = test_metrics['baseline_rate']
    prec_at_k = {k: test_metrics[f'precision_at_{k}pct'] for k in [1, 5, 10]}
    print(f"\nTest metrics (using calibrated probabilities): ROC-AUC {test_metrics['roc_auc']:.4f}, PR-AUC {test_metrics['pr_auc']:.4f}")
    for k in [1, 5, 10]:
        n_top_k = max(1, int(len(y_test) * k / 100))
        n_protected = int(n_top_k * prec_at_k[k])
        print(f"  P@{k}%: {prec_at_k[k]:.4f} ({n_protected:,}/{n_top_k:,}), lift {test_metrics[f'lift_at_{k}pct']:.2f}x")
    
    if use_wandb:
        wandb.log({**{f"test/{k}": v for k, v in test_metrics.items()}})
    
    # Get feature importance from the underlying LightGBM model
    # The calibrated model wraps the original model, so we need to access it
    try:
        underlying_model = calibrated_model.calibrated_classifiers_[0].estimator.booster
        feature_importance = pd.DataFrame({'feature': feature_cols, 'importance': underlying_model.feature_importance()}).sort_values('importance', ascending=False).head(20)
        print("\nTop 20 features:\n" + feature_importance.to_string(index=False))
    except (AttributeError, IndexError) as e:
        print(f"\nWarning: Could not extract feature importance from calibrated model: {e}")
        feature_importance = pd.DataFrame({'feature': feature_cols, 'importance': [0] * len(feature_cols)}).head(20)
    
    # Free test data after metrics computation
    del y_test, y_proba; gc.collect()
    
    metrics = {"metadata": {"timestamp": timestamp, "model": "LightGBM", "task": "transition_01_prediction",
                           "random_state": RANDOM_STATE, "n_features": len(feature_cols), "features": feature_cols,
                           "calibration": "isotonic", "calibration_method": "CalibratedClassifierCV with cv='prefit'",
                           "calibration_data": "val_late (2015-2017)", "test_probabilities": "calibrated"},
              "data": {"n_train_full": n_train_full, "n_test": int(n_test_total), "train_positives": train_pos,
                      "train_negatives": train_neg, "test_positives": test_pos, "test_negatives": test_neg},
              "temporal_split": {"method": "strict_temporal_split", "train_early_max_year": TRAIN_EARLY_YEAR_MAX,
                                "val_late_min_year": VAL_LATE_YEAR_MIN, "val_late_max_year": VAL_LATE_YEAR_MAX},
              "validation_performance": validation_metrics, "model_parameters": best_params,
              "imbalance_handling": {"method": "scale_pos_weight", "is_unbalance": False,
                                    "scale_pos_weight": imbalance_info.get("scale_pos_weight"),
                                    "neg_pos_ratio_full": imbalance_info.get("ratio_full"),
                                    "neg_pos_ratio_sampled": imbalance_info.get("ratio_sampled"),
                                    "neg_full": imbalance_info.get("neg_full"), "pos_full": imbalance_info.get("pos_full"),
                                    "neg_sampled": imbalance_info.get("neg_sampled"), "pos_sampled": imbalance_info.get("pos_sampled")},
              "test_performance": test_metrics, "feature_importance": feature_importance.head(20).to_dict('records'),
              "timing": {"train_early_seconds": train_early_time, "validation_seconds": val_time,
                        "final_training_seconds": train_time, "prediction_seconds": pred_time, "total_seconds": time.time() - start_time}}
    metrics = convert_numpy_types(metrics)
    metrics_path = output_dir / f"model1_lgbm_metrics_{timestamp}.json"
    with open(metrics_path, 'w') as f:
        json.dump(metrics, f, indent=2)
    print(f"\nMetrics saved: {metrics_path}")
    del calibrated_model; gc.collect()
    
    total_time = time.time() - start_time
    print("\n" + "="*70 + "\nSUMMARY\n" + "="*70)
    print(f"Model: LightGBM (with isotonic calibration), {len(feature_cols)} features\nValidation (≤{TRAIN_EARLY_YEAR_MAX}→{VAL_LATE_YEAR_MIN}-{VAL_LATE_YEAR_MAX}): PR-AUC {validation_metrics['pr_auc']:.4f}, ROC-AUC {validation_metrics['roc_auc']:.4f}")
    print(f"Test (2018-2024, {n_test_total:,} samples, {test_pos/n_test_total*100:.3f}% pos, calibrated probabilities): ROC-AUC {test_metrics['roc_auc']:.4f}, PR-AUC {test_metrics['pr_auc']:.4f}")
    print(f"  P@1%: {prec_at_k[1]:.4f} ({test_metrics['lift_at_1pct']:.1f}x), P@5%: {prec_at_k[5]:.4f} ({test_metrics['lift_at_5pct']:.1f}x), P@10%: {prec_at_k[10]:.4f} ({test_metrics['lift_at_10pct']:.1f}x)")
    print(f"Timings: train_early {train_early_time:.1f}s, val {val_time:.1f}s, final_train {train_time:.1f}s, pred {pred_time:.1f}s, total {total_time:.1f}s ({total_time/60:.1f}m)")
    print("="*70 + "\nDone.")
    
    if use_wandb:
        wandb.log({"summary/total_time_seconds": total_time, "summary/total_time_minutes": total_time / 60, "status": "success"})
        wandb.finish()


def main() -> None:
    start_time = time.time()
    repo_root = Path(__file__).resolve().parents[3]
    train_path = resolve_parquet_file("train_early_sampled.parquet")
    val_path = resolve_parquet_file("val_late_full.parquet")
    test_path = resolve_parquet_file("test_full.parquet")
    params_path = resolve_best_params_json()
    output_dir = repo_root / "outputs/Results/ml_models"
    output_dir.mkdir(parents=True, exist_ok=True)
    model_dir = repo_root / "data/ml/models"
    model_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    use_wandb = False
    try:
        wandb.init(project="ml-training-transitions", entity=os.environ.get("WANDB_ENTITY"), name=f"model1_lgbm_{timestamp}",
                  config={"model": "LightGBM", "task": "transition_01_prediction", "random_state": RANDOM_STATE,
                         "temporal_split": {"train_early_max_year": TRAIN_EARLY_YEAR_MAX, "val_late_min_year": VAL_LATE_YEAR_MIN,
                                           "val_late_max_year": VAL_LATE_YEAR_MAX}})
        use_wandb = True
        print("W&B connected")
    except Exception as err:
        print(f"W&B failed: {err}")
    
    print("="*70 + "\nLGBM TRANSITION MODEL\n" + "="*70)
    print(f"Train: {train_path}\nVal: {val_path}\nTest: {test_path}\nParams: {params_path}\nOutput: {output_dir}")
    
    best_params, best_cv_score = load_best_params(params_path)
    print(f"\nLoaded params from {params_path}")
    if best_cv_score:
        print(f"Best CV score: {best_cv_score:.4f}")
    
    all_cols_train = pq.ParquetFile(train_path).schema_arrow.names
    all_cols_test = pq.ParquetFile(test_path).schema_arrow.names
    target_col = "transition_01"
    # Build feature columns from PyArrow schema only (avoid loading full parquet into pandas)
    schema = pq.ParquetFile(train_path).schema_arrow
    numeric_cols = [
        name
        for name, field in zip(schema.names, schema)
        if pa.types.is_integer(field.type) or pa.types.is_floating(field.type)
    ]
    # Exclude target, coordinates, and year from features (keep year in required_cols)
    feature_cols = [c for c in numeric_cols if c not in EXCLUDE_COLS and c not in {'year'}]
    required_cols = [target_col, 'year', 'row', 'col']
    optional_cols = [col for col in ['x', 'y'] if col in all_cols_train]
    keep_cols_train = feature_cols + required_cols + optional_cols
    keep_cols_test = feature_cols + required_cols + [col for col in ['x', 'y'] if col in all_cols_test]
    print(f"Selected {len(feature_cols)} features, loading {len(keep_cols_train)} cols from train, {len(keep_cols_test)} from test")
    
    (model_path, validation_metrics, train_stats, train_early_time, val_time, train_time, train_pos, train_neg, n_train_full, imbalance_info) = phase1_training(
        train_path, val_path, params_path, feature_cols, keep_cols_train, best_params, timestamp, model_dir, output_dir, use_wandb, target_col)
    
    phase2_testing(test_path, model_path, feature_cols, keep_cols_test, timestamp, output_dir, use_wandb, target_col,
                  validation_metrics, train_stats, train_early_time, val_time, train_time, train_pos, train_neg, n_train_full,
                  best_params, start_time, imbalance_info)


if __name__ == "__main__":
    repo_root = Path(__file__).resolve().parents[3]
    output_dir = repo_root / "outputs/Results/ml_models"
    output_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"model1_lgbm_{timestamp}.txt"
    tee = Tee(output_file)
    sys.stdout = tee
    try:
        main()
    finally:
        sys.stdout = tee.stdout
        tee.close()
        print(f"\nOutput saved to: {output_file}")
