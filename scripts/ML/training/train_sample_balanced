#!/usr/bin/env python3

"""Transition Prediction Training Script

Purpose: train a model to predict new WDPA establishment (0→1 transitions).
Input:  `data/ml/sample_training_balanced.parquet` (generated by sample_preprocessing).
Process: use transition_01 as target, exclude identity/leakage columns, perform
         80/20 stratified split, then fit a RandomForest classifier.
Output: prints a classification report and the 20 most important features.
"""

from __future__ import annotations

import os
import sys
import time
from datetime import datetime
from pathlib import Path

import pandas as pd
import wandb
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, average_precision_score
from sklearn.model_selection import train_test_split


class Tee:
    """Write to both stdout and a file."""
    def __init__(self, file_path: Path):
        self.file = open(file_path, 'w', encoding='utf-8')
        self.stdout = sys.stdout
    
    def write(self, text: str) -> None:
        self.stdout.write(text)
        self.file.write(text)
        self.file.flush()
    
    def flush(self) -> None:
        self.stdout.flush()
        self.file.flush()
    
    def close(self) -> None:
        self.file.close()

RANDOM_STATE = 42
N_ESTIMATORS = 100
TEST_SIZE = 0.2
MAX_SAMPLE_SIZE = 2_000_000  # Subsample if dataset is larger to avoid memory issues

# Columns to exclude from features (to prevent data leakage or overfitting)
EXCLUDE_COLS = {
    "transition_01",  # Target variable
    "WDPA_b1",        # Direct leakage of target
    "WDPA_prev",      # Direct leakage of target
    "row",            # Spatial identifier
    "col",            # Spatial identifier
    "x",              # Spatial coordinate
    "y",              # Spatial coordinate
    "year",           # Temporal identifier
}


def main() -> None:
    start_time = time.time()
    
    # Initialize W&B
    wandb_api_key = os.environ.get("WANDB_API_KEY")
    wandb_entity = os.environ.get("WANDB_ENTITY")
    
    if not wandb_api_key:
        print("Warning: WANDB_API_KEY not found in environment")
    if not wandb_entity:
        print("Warning: WANDB_ENTITY not found in environment")
    
    print("Initializing Weights & Biases...")
    wandb.init(
        project="ml-training-balanced",
        entity=wandb_entity,
        name=f"train_balanced_{time.strftime('%Y%m%d_%H%M%S')}",
        config={
            "model": "RandomForest",
            "n_estimators": N_ESTIMATORS,
            "test_size": TEST_SIZE,
            "random_state": RANDOM_STATE,
            "max_sample_size": MAX_SAMPLE_SIZE,
            "target": "transition_01",
        },
    )
    print("W&B connected\n")
    
    repo_root = Path(__file__).resolve().parents[3]
    input_path = repo_root / "data/ml/sample_training_balanced.parquet"

    print(f"Loading data from {input_path} …")
    df = pd.read_parquet(input_path)
    print(f"Loaded {len(df):,} rows with {len(df.columns)} columns.")

    target_col = "transition_01"
    if target_col not in df.columns:
        raise ValueError(
            f"Target column '{target_col}' not found in data. "
            "Ensure sample_preprocessing has been run with the transition logic."
        )

    print(f"\nUsing '{target_col}' as target variable.")

    # Drop rows with missing target
    df_clean = df.dropna(subset=[target_col])
    dropped = len(df) - len(df_clean)
    if dropped > 0:
        print(f"Dropped {dropped:,} rows with missing target.")
    print(f"Remaining: {len(df_clean):,} rows.")

    if len(df_clean) == 0:
        raise ValueError("No valid rows remaining after dropping missing targets.")

    # Subsample if dataset is too large
    if len(df_clean) > MAX_SAMPLE_SIZE:
        print(
            f"\nDataset has {len(df_clean):,} rows. "
            f"Subsampling to {MAX_SAMPLE_SIZE:,} for memory efficiency …"
        )
        df_clean = df_clean.sample(
            n=MAX_SAMPLE_SIZE,
            random_state=RANDOM_STATE,
            stratify=df_clean[target_col],  # Maintain class balance
        )
        print(f"Subsampled to {len(df_clean):,} rows.")

    y = df_clean[target_col]

    # Select numeric features, excluding identity and leakage columns
    numeric_cols = df_clean.select_dtypes(include=["number"]).columns.tolist()
    feature_cols = [
        col for col in numeric_cols 
        if col.lower() not in {c.lower() for c in EXCLUDE_COLS}
    ]

    if not feature_cols:
        raise ValueError("No valid numeric feature columns available for training.")

    X = df_clean[feature_cols]

    print(f"\nUsing {len(feature_cols)} numeric features.")
    print(f"Excluded columns: {sorted(EXCLUDE_COLS & set(numeric_cols))}")
    
    # Print target distribution
    transition_counts = y.value_counts().sort_index()
    n_negatives = transition_counts.get(0, 0)
    n_positives = transition_counts.get(1, 0)
    positive_ratio = n_positives / len(y) * 100
    
    print(f"\nTarget distribution:")
    print(f"  No transition (0): {n_negatives:,}")
    print(f"  New WDPA (0→1):    {n_positives:,}")
    print(f"  Class balance:     {positive_ratio:.1f}% positive")
    
    wandb.log({
        "data/n_features": len(feature_cols),
        "data/n_samples": len(df_clean),
        "data/n_positives": n_positives,
        "data/n_negatives": n_negatives,
        "data/positive_ratio": positive_ratio / 100,
    })

    # Train/test split with stratification
    print(
        f"\nSplitting into train/test "
        f"({int((1-TEST_SIZE)*100)}/{int(TEST_SIZE*100)}) with stratification …"
    )
    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y,
        test_size=TEST_SIZE,
        stratify=y,
        random_state=RANDOM_STATE,
    )
    
    # Print training set statistics
    train_transition_counts = y_train.value_counts().sort_index()
    print(f"\nTraining set: {len(X_train):,} samples")
    print(f"  No transition (0): {train_transition_counts.get(0, 0):,}")
    print(f"  New WDPA (0→1):    {train_transition_counts.get(1, 0):,}")
    
    print(f"\nTest set: {len(X_test):,} samples")
    test_transition_counts = y_test.value_counts().sort_index()
    print(f"  No transition (0): {test_transition_counts.get(0, 0):,}")
    print(f"  New WDPA (0→1):    {test_transition_counts.get(1, 0):,}")

    # Train Random Forest
    print(f"\nTraining RandomForestClassifier with {N_ESTIMATORS} trees …")
    rf = RandomForestClassifier(
        n_estimators=N_ESTIMATORS,
        random_state=RANDOM_STATE,
        n_jobs=-1,
        verbose=1,
    )
    rf.fit(X_train, y_train)

    # Evaluate
    print("\nEvaluating on test set …")
    y_pred = rf.predict(X_test)
    y_proba = rf.predict_proba(X_test)[:, 1]

    print("\n" + "=" * 60)
    print("CLASSIFICATION REPORT")
    print("=" * 60)
    print(classification_report(y_test, y_pred))
    
    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()
    
    print("\nConfusion Matrix:")
    print(f"True Negatives:  {tn:,}")
    print(f"False Positives: {fp:,}")
    print(f"False Negatives: {fn:,}")
    print(f"True Positives:  {tp:,}")
    
    # Compute AUC metrics
    roc_auc = roc_auc_score(y_test, y_proba)
    pr_auc = average_precision_score(y_test, y_proba)
    
    print(f"\nROC-AUC: {roc_auc:.4f}")
    print(f"PR-AUC:  {pr_auc:.4f}")

    # Feature importance
    print("\n" + "=" * 60)
    print("TOP 20 MOST IMPORTANT FEATURES")
    print("=" * 60)
    feature_importance = pd.DataFrame(
        {"feature": feature_cols, "importance": rf.feature_importances_}
    ).sort_values("importance", ascending=False)

    print(feature_importance.head(20).to_string(index=False))
    
    # Log to W&B
    total_time = time.time() - start_time
    
    wandb.log({
        "metrics/true_negatives": int(tn),
        "metrics/false_positives": int(fp),
        "metrics/false_negatives": int(fn),
        "metrics/true_positives": int(tp),
        "metrics/accuracy": (tp + tn) / (tp + tn + fp + fn),
        "metrics/precision": tp / (tp + fp) if (tp + fp) > 0 else 0,
        "metrics/recall": tp / (tp + fn) if (tp + fn) > 0 else 0,
        "metrics/f1": 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0,
        "metrics/roc_auc": roc_auc,
        "metrics/pr_auc": pr_auc,
        "timing/total_seconds": total_time,
        "timing/total_minutes": total_time / 60,
        "status": "success"
    })
    
    print(f"\nTotal time: {total_time:.1f}s ({total_time/60:.1f} min)")
    print("Done.")
    
    wandb.finish()


if __name__ == "__main__":
    # Set up output file capture
    repo_root = Path(__file__).resolve().parents[3]
    output_dir = repo_root / "outputs/results/ml_baselines"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"train_sample_balanced_{timestamp}.txt"
    
    tee = Tee(output_file)
    sys.stdout = tee
    
    try:
        main()
    finally:
        sys.stdout = tee.stdout
        tee.close()
        print(f"\nOutput saved to: {output_file}")
