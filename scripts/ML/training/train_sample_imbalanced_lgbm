#!/usr/bin/env python3

"""Imbalanced Transition Prediction Training Script (LightGBM)

Purpose: Train a LightGBM model to predict new WDPA establishment (0→1 transitions) on
         realistic imbalanced data with comprehensive evaluation metrics.
Input:  `data/ml/sample_training_imbalanced.parquet` (realistic class distribution).
Process: Use transition_01 as target, perform 80/20 stratified split, fit a
         LightGBM classifier with scale_pos_weight, and evaluate with multiple metrics.
Output: Classification report, ROC-AUC, PR-AUC, precision@k, confusion matrix,
        and feature importance.
"""

from __future__ import annotations

import os
import sys
import time
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import lightgbm as lgb
import wandb
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    roc_auc_score,
    average_precision_score,
    precision_recall_curve,
    roc_curve,
)
from sklearn.model_selection import train_test_split


class Tee:
    """Write to both stdout and a file."""
    def __init__(self, file_path: Path):
        self.file = open(file_path, 'w', encoding='utf-8')
        self.stdout = sys.stdout
    
    def write(self, text: str) -> None:
        self.stdout.write(text)
        self.file.write(text)
        self.file.flush()
    
    def flush(self) -> None:
        self.stdout.flush()
        self.file.flush()
    
    def close(self) -> None:
        self.file.close()

RANDOM_STATE = 42
VAL_SIZE = 0.1    # 10% for validation (from original data)
TEST_SIZE = 0.2   # 20% for test (from original data)
# Training will be 70% (remaining after val+test split)

# Scale pos weight values to test
SCALE_POS_WEIGHTS = [40, 60]

# Columns to exclude from features (to prevent data leakage or overfitting)
EXCLUDE_COLS = {
    "transition_01",  # Target variable
    "WDPA_b1",        # Direct leakage of target
    "WDPA_prev",      # Direct leakage of target
    "row",            # Spatial identifier
    "col",            # Spatial identifier
    "x",              # Spatial coordinate
    "y",              # Spatial coordinate
    "year",           # Temporal identifier
}


def compute_precision_at_k(y_true: np.ndarray, y_proba: np.ndarray, k: float) -> float:
    """Compute precision at top k% of predictions.
    
    Args:
        y_true: True labels (0 or 1)
        y_proba: Predicted probabilities for positive class
        k: Percentage (e.g., 1 for top 1%, 5 for top 5%)
    
    Returns:
        Precision among top k% predictions
    """
    n_samples = len(y_true)
    n_top_k = max(1, int(n_samples * k / 100))
    
    # Get indices of top k% predictions
    top_k_idx = np.argsort(y_proba)[-n_top_k:]
    
    # Calculate precision among top k%
    precision = y_true[top_k_idx].sum() / n_top_k
    
    return precision


def main(scale_pos_weight: float) -> None:
    start_time = time.time()
    
    # Initialize W&B for this specific run
    wandb_api_key = os.environ.get("WANDB_API_KEY")
    wandb_entity = os.environ.get("WANDB_ENTITY")
    
    if not wandb_api_key:
        print("Warning: WANDB_API_KEY not found in environment")
    if not wandb_entity:
        print("Warning: WANDB_ENTITY not found in environment")
    
    print("Initializing Weights & Biases...")
    wandb.init(
        project="ml-training-imbalanced",
        entity=wandb_entity,
        name=f"train_imbalanced_lgbm_spw{scale_pos_weight}_{time.strftime('%Y%m%d_%H%M%S')}",
        config={
            "model": "LightGBM",
            "scale_pos_weight": scale_pos_weight,
            "val_size": VAL_SIZE,
            "test_size": TEST_SIZE,
            "random_state": RANDOM_STATE,
            "target": "transition_01",
        },
    )
    print("W&B connected\n")
    
    repo_root = Path(__file__).resolve().parents[3]
    input_path = repo_root / "data/ml/sample_training_imbalanced.parquet"

    print("=" * 70)
    print("IMBALANCED SAMPLE TRAINING WDPA TRANSITION PREDICTION (LightGBM)")
    print("=" * 70)
    
    print(f"\nLoading data from {input_path} …")
    df = pd.read_parquet(input_path)
    print(f"Loaded {len(df):,} rows with {len(df.columns)} columns.")

    target_col = "transition_01"
    if target_col not in df.columns:
        raise ValueError(
            f"Target column '{target_col}' not found in data. "
            "Ensure preprocessing has been run with the transition logic."
        )

    print(f"\nUsing '{target_col}' as target variable.")

    # Drop rows with missing target
    df_clean = df.dropna(subset=[target_col])
    dropped = len(df) - len(df_clean)
    if dropped > 0:
        print(f"Dropped {dropped:,} rows with missing target.")
    print(f"Remaining: {len(df_clean):,} rows.")

    if len(df_clean) == 0:
        raise ValueError("No valid rows remaining after dropping missing targets.")

    # No subsampling - use full dataset
    print(f"\nUsing full dataset: {len(df_clean):,} rows.")

    y = df_clean[target_col]

    # Select numeric features, excluding identity and leakage columns
    numeric_cols = df_clean.select_dtypes(include=["number"]).columns.tolist()
    feature_cols = [
        col for col in numeric_cols 
        if col.lower() not in {c.lower() for c in EXCLUDE_COLS}
    ]

    if not feature_cols:
        raise ValueError("No valid numeric feature columns available for training.")

    X = df_clean[feature_cols]

    print(f"\nUsing {len(feature_cols)} numeric features.")
    print(f"Excluded columns: {sorted(EXCLUDE_COLS & set(numeric_cols))}")
    
    # Print target distribution
    transition_counts = y.value_counts().sort_index()
    n_negatives = transition_counts.get(0, 0)
    n_positives = transition_counts.get(1, 0)
    positive_ratio = n_positives / len(y) * 100
    
    print(f"\n" + "=" * 70)
    print("TARGET DISTRIBUTION (IMBALANCED)")
    print("=" * 70)
    print(f"  No transition (0): {n_negatives:>12,}  ({100 - positive_ratio:>5.3f}%)")
    print(f"  New WDPA (0→1):    {n_positives:>12,}  ({positive_ratio:>5.3f}%)")
    print(f"  Class ratio:       1 : {n_negatives / max(n_positives, 1):.1f}")

    # Calculate scale_pos_weight for LightGBM
    auto_scale_pos_weight = n_negatives / max(n_positives, 1)
    print(f"\nUsing scale_pos_weight: {scale_pos_weight:.2f}")
    print(f"(Automatic value based on class ratio: {auto_scale_pos_weight:.2f})")
    
    wandb.log({
        "data/n_features": len(feature_cols),
        "data/n_samples": len(df_clean),
        "data/n_positives": n_positives,
        "data/n_negatives": n_negatives,
        "data/positive_ratio": positive_ratio / 100,
        "data/class_ratio": n_negatives / max(n_positives, 1),
        "data/auto_scale_pos_weight": auto_scale_pos_weight,
    })

    # Train/val/test split with stratification (70/10/20)
    print(f"\n" + "=" * 70)
    print("TRAIN/VAL/TEST SPLIT")
    print("=" * 70)
    print(f"Splitting into train/val/test (70/10/20) with stratification …")
    
    # First split: separate test set (20%)
    X_temp, X_test, y_temp, y_test = train_test_split(
        X,
        y,
        test_size=TEST_SIZE,
        stratify=y,
        random_state=RANDOM_STATE,
    )
    
    # Second split: separate validation from remaining (10% of original = 12.5% of temp)
    val_size_adjusted = VAL_SIZE / (1 - TEST_SIZE)  # 0.1 / 0.8 = 0.125
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp,
        y_temp,
        test_size=val_size_adjusted,
        stratify=y_temp,
        random_state=RANDOM_STATE,
    )
    
    # Print training set statistics
    train_transition_counts = y_train.value_counts().sort_index()
    train_pos = train_transition_counts.get(1, 0)
    train_neg = train_transition_counts.get(0, 0)
    print(f"\nTraining set: {len(X_train):,} samples ({len(X_train)/len(df_clean)*100:.1f}%)")
    print(f"  No transition (0): {train_neg:,}")
    print(f"  New WDPA (0→1):    {train_pos:,}")
    
    val_transition_counts = y_val.value_counts().sort_index()
    val_pos = val_transition_counts.get(1, 0)
    val_neg = val_transition_counts.get(0, 0)
    print(f"\nValidation set: {len(X_val):,} samples ({len(X_val)/len(df_clean)*100:.1f}%)")
    print(f"  No transition (0): {val_neg:,}")
    print(f"  New WDPA (0→1):    {val_pos:,}")
    
    test_transition_counts = y_test.value_counts().sort_index()
    test_pos = test_transition_counts.get(1, 0)
    test_neg = test_transition_counts.get(0, 0)
    print(f"\nTest set: {len(X_test):,} samples ({len(X_test)/len(df_clean)*100:.1f}%)")
    print(f"  No transition (0): {test_neg:,}")
    print(f"  New WDPA (0→1):    {test_pos:,}")

    # Train LightGBM with scale_pos_weight
    print(f"\n" + "=" * 70)
    print("MODEL TRAINING")
    print("=" * 70)
    print(f"Training LightGBM classifier …")
    print(f"Using scale_pos_weight={scale_pos_weight:.2f} to handle imbalanced data …")
    
    # LightGBM parameters optimized for imbalanced binary classification
    lgbm_params = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting_type': 'gbdt',
        'num_leaves': 255,
        'learning_rate': 0.05,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'max_depth': 16,
        'min_child_samples': 100,
        'scale_pos_weight': scale_pos_weight,
        'random_state': RANDOM_STATE,
        'n_jobs': -1,
        'verbose': -1,
    }
    
    print("\nModel parameters:")
    for param, value in lgbm_params.items():
        if param != 'verbose':
            print(f"  {param}: {value}")
    
    # Create LightGBM classifier
    lgbm = lgb.LGBMClassifier(
        n_estimators=500,
        **lgbm_params
    )
    
    # Fit with early stopping using validation set
    print(f"\nTraining with early stopping on validation set (patience=50) …")
    lgbm.fit(
        X_train, 
        y_train,
        eval_set=[(X_val, y_val)],
        eval_metric='auc',
        callbacks=[
            lgb.early_stopping(stopping_rounds=50, verbose=False),
            lgb.log_evaluation(period=100)
        ]
    )
    
    print(f"\nTraining complete. Best iteration: {lgbm.best_iteration_}")
    print(f"Best validation AUC: {lgbm.best_score_['valid_0']['auc']:.4f}")

    # Predict probabilities and classes
    print("\n" + "=" * 70)
    print("EVALUATION ON TEST SET")
    print("=" * 70)
    print("Computing predictions …")
    y_pred = lgbm.predict(X_test)
    y_proba = lgbm.predict_proba(X_test)[:, 1]  # Probability of positive class

    # Classification report
    print("\n" + "=" * 70)
    print("CLASSIFICATION REPORT")
    print("=" * 70)
    print(classification_report(y_test, y_pred, digits=4))

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()
    
    print("\n" + "=" * 70)
    print("CONFUSION MATRIX")
    print("=" * 70)
    print(f"                 Predicted Negative    Predicted Positive")
    print(f"Actual Negative  {tn:>18,}    {fp:>18,}")
    print(f"Actual Positive  {fn:>18,}    {tp:>18,}")
    print(f"\nTrue Negatives:  {tn:,}")
    print(f"False Positives: {fp:,}")
    print(f"False Negatives: {fn:,}")
    print(f"True Positives:  {tp:,}")

    # ROC-AUC and PR-AUC
    print("\n" + "=" * 70)
    print("AREA UNDER CURVE METRICS")
    print("=" * 70)
    
    roc_auc = roc_auc_score(y_test, y_proba)
    pr_auc = average_precision_score(y_test, y_proba)
    
    print(f"ROC-AUC:  {roc_auc:.4f}")
    print(f"PR-AUC:   {pr_auc:.4f}")
    
    # Precision at top k%
    print("\n" + "=" * 70)
    print("PRECISION @ TOP-K PREDICTIONS")
    print("=" * 70)
    print("(Precision among highest-confidence predictions)")
    print()
    
    for k in [1, 5, 10]:
        prec_at_k = compute_precision_at_k(y_test.values, y_proba, k)
        n_top_k = max(1, int(len(y_test) * k / 100))
        expected_positives = int(n_top_k * prec_at_k)
        print(f"Precision @ top {k:>2}%:  {prec_at_k:>7.4f}  "
              f"({expected_positives:>6,} positives in top {n_top_k:>7,} predictions)")
    
    # Baseline precision (random ranking)
    baseline_precision = test_pos / len(y_test)
    print(f"\nBaseline (random): {baseline_precision:>7.4f}  "
          f"(overall positive rate in test set)")

    # Feature importance
    print("\n" + "=" * 70)
    print("TOP 20 MOST IMPORTANT FEATURES")
    print("=" * 70)
    feature_importance = pd.DataFrame(
        {"feature": feature_cols, "importance": lgbm.feature_importances_}
    ).sort_values("importance", ascending=False)

    print(feature_importance.head(20).to_string(index=False))
    
    # Summary
    prec_at_1 = compute_precision_at_k(y_test.values, y_proba, 1)
    prec_at_5 = compute_precision_at_k(y_test.values, y_proba, 5)
    prec_at_10 = compute_precision_at_k(y_test.values, y_proba, 10)
    
    print("\n" + "=" * 70)
    print("SUMMARY")
    print("=" * 70)
    print(f"Model:                LightGBM ({lgbm.best_iteration_} iterations, scale_pos_weight={scale_pos_weight:.2f})")
    print(f"Test samples:         {len(y_test):,}")
    print(f"Positive rate:        {positive_ratio:.3f}%")
    print(f"ROC-AUC:              {roc_auc:.4f}")
    print(f"PR-AUC:               {pr_auc:.4f}")
    print(f"Precision @ top 1%:   {prec_at_1:.4f}")
    print(f"Precision @ top 5%:   {prec_at_5:.4f}")
    print(f"Precision @ top 10%:  {prec_at_10:.4f}")
    print("=" * 70)
    
    # Log to W&B
    total_time = time.time() - start_time
    
    wandb.log({
        "metrics/true_negatives": int(tn),
        "metrics/false_positives": int(fp),
        "metrics/false_negatives": int(fn),
        "metrics/true_positives": int(tp),
        "metrics/accuracy": (tp + tn) / (tp + tn + fp + fn),
        "metrics/precision": tp / (tp + fp) if (tp + fp) > 0 else 0,
        "metrics/recall": tp / (tp + fn) if (tp + fn) > 0 else 0,
        "metrics/f1": 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0,
        "metrics/roc_auc": roc_auc,
        "metrics/pr_auc": pr_auc,
        "metrics/precision_at_1": prec_at_1,
        "metrics/precision_at_5": prec_at_5,
        "metrics/precision_at_10": prec_at_10,
        "metrics/best_iteration": lgbm.best_iteration_,
        "metrics/best_val_auc": lgbm.best_score_['valid_0']['auc'],
        "timing/total_seconds": total_time,
        "timing/total_minutes": total_time / 60,
        "status": "success"
    })
    
    print(f"\nTotal time: {total_time:.1f}s ({total_time/60:.1f} min)")
    print("All done.")
    
    wandb.finish()


if __name__ == "__main__":
    repo_root = Path(__file__).resolve().parents[3]
    output_dir = repo_root / "outputs/results/ml_baselines"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    print("=" * 70)
    print(f"RUNNING {len(SCALE_POS_WEIGHTS)} EXPERIMENTS WITH DIFFERENT SCALE_POS_WEIGHTS")
    print("=" * 70)
    print(f"Values to test: {SCALE_POS_WEIGHTS}")
    print("=" * 70)
    
    for idx, spw in enumerate(SCALE_POS_WEIGHTS, start=1):
        print(f"\n{'#' * 70}")
        print(f"# EXPERIMENT {idx}/{len(SCALE_POS_WEIGHTS)}: scale_pos_weight = {spw}")
        print(f"{'#' * 70}\n")
        
        output_file = output_dir / f"train_sample_imbalanced_lgbm_spw{spw}_{timestamp}.txt"
        tee = Tee(output_file)
        original_stdout = sys.stdout
        sys.stdout = tee
        
        try:
            main(scale_pos_weight=spw)
        finally:
            sys.stdout = original_stdout
            tee.close()
            print(f"\nExperiment {idx}/{len(SCALE_POS_WEIGHTS)} complete. Output saved to: {output_file}")
    
    print(f"\n{'=' * 70}")
    print("ALL EXPERIMENTS COMPLETE")
    print("=" * 70)
    print(f"Results saved in: {output_dir}")
    print(f"Timestamp: {timestamp}")