#!/usr/bin/env python3

"""Baseline Training Script

Purpose: benchmark model performance on a lightweight 2020 subset.
Input:  `data/ml/sample_2020_100k.parquet` created by baseline preprocessing.
Process: drop rows with missing `WDPA_b1`, keep numeric features, perform an
         80/20 stratified split, then fit a RandomForest classifier.
Output: prints a classification report and the 20 most important features.
"""

from __future__ import annotations

import os
import time
from pathlib import Path

import pandas as pd
import wandb
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
from sklearn.model_selection import train_test_split

RANDOM_STATE = 42
N_ESTIMATORS = 100
TEST_SIZE = 0.2


def main() -> None:
    start_time = time.time()
    
    # Initialize W&B
    wandb_api_key = os.environ.get("WANDB_API_KEY")
    wandb_entity = os.environ.get("WANDB_ENTITY")
    
    if not wandb_api_key:
        print("Warning: WANDB_API_KEY not found in environment")
    if not wandb_entity:
        print("Warning: WANDB_ENTITY not found in environment")
    
    print("Initializing Weights & Biases...")
    wandb.init(
        project="ml-training-baselines",
        entity=wandb_entity,
        name=f"train_baseline_{time.strftime('%Y%m%d_%H%M%S')}",
        config={
            "model": "RandomForest",
            "n_estimators": N_ESTIMATORS,
            "test_size": TEST_SIZE,
            "random_state": RANDOM_STATE,
        },
    )
    print("W&B connected\n")
    
    repo_root = Path(__file__).resolve().parents[3]
    input_path = repo_root / "data/ml/sample_2020_100k.parquet"

    print(f"Loading data from {input_path} …")
    df = pd.read_parquet(input_path)
    print(f"Loaded {len(df):,} rows with {len(df.columns)} columns.")

    # Target and features
    target_col = "WDPA_b1"
    if target_col not in df.columns:
        raise ValueError(f"Target column '{target_col}' not found in data.")

    print(f"\nUsing '{target_col}' as target variable.")

    # Drop rows with missing target
    df_clean = df.dropna(subset=[target_col])
    print(f"Dropped {len(df) - len(df_clean):,} rows with missing target.")
    print(f"Remaining: {len(df_clean):,} rows.")

    if len(df_clean) == 0:
        raise ValueError("No valid rows remaining after dropping missing targets.")

    # Separate features and target
    y = df_clean[target_col]
    
    # Select only numeric columns for features (exclude target)
    numeric_cols = df_clean.select_dtypes(include=["number"]).columns.tolist()
    if target_col in numeric_cols:
        numeric_cols.remove(target_col)
    
    X = df_clean[numeric_cols]
    
    print(f"\nUsing {len(numeric_cols)} numeric features.")
    target_dist = y.value_counts().sort_index()
    print(f"Target distribution:\n{target_dist}")
    
    wandb.log({
        "data/n_features": len(numeric_cols),
        "data/n_samples": len(df_clean),
        "data/n_positives": target_dist.get(1, 0),
        "data/n_negatives": target_dist.get(0, 0),
    })

    # Split train/test with stratification
    print(f"\nSplitting into train/test ({int((1-TEST_SIZE)*100)}/{int(TEST_SIZE*100)}) with stratification …")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=TEST_SIZE, 
        stratify=y, 
        random_state=RANDOM_STATE
    )
    print(f"Train: {len(X_train):,} samples")
    print(f"Test:  {len(X_test):,} samples")

    # Train RandomForest
    print(f"\nTraining RandomForestClassifier with {N_ESTIMATORS} trees …")
    rf = RandomForestClassifier(
        n_estimators=N_ESTIMATORS,
        random_state=RANDOM_STATE,
        n_jobs=-1,
        verbose=1
    )
    rf.fit(X_train, y_train)

    # Predict and evaluate
    print("\nEvaluating on test set …")
    y_pred = rf.predict(X_test)
    y_proba = rf.predict_proba(X_test)
    
    # Calculate metrics
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()
    
    # Try to compute ROC-AUC (only if both classes present)
    try:
        roc_auc = roc_auc_score(y_test, y_proba[:, 1])
    except:
        roc_auc = None
    
    print("\n" + "="*60)
    print("CLASSIFICATION REPORT")
    print("="*60)
    print(classification_report(y_test, y_pred))
    
    # Confusion Matrix
    print("\n" + "="*60)
    print("CONFUSION MATRIX")
    print("="*60)
    print(f"True Negatives:  {tn:,}")
    print(f"False Positives: {fp:,}")
    print(f"False Negatives: {fn:,}")
    print(f"True Positives:  {tp:,}")
    
    if roc_auc:
        print(f"\nROC-AUC: {roc_auc:.4f}")

    # Feature importance
    print("\n" + "="*60)
    print("TOP 20 MOST IMPORTANT FEATURES")
    print("="*60)
    feature_importance = pd.DataFrame({
        "feature": numeric_cols,
        "importance": rf.feature_importances_
    }).sort_values("importance", ascending=False)
    
    print(feature_importance.head(20).to_string(index=False))
    
    # Log to W&B
    total_time = time.time() - start_time
    
    log_dict = {
        "metrics/true_negatives": int(tn),
        "metrics/false_positives": int(fp),
        "metrics/false_negatives": int(fn),
        "metrics/true_positives": int(tp),
        "metrics/accuracy": (tp + tn) / (tp + tn + fp + fn),
        "metrics/precision": tp / (tp + fp) if (tp + fp) > 0 else 0,
        "metrics/recall": tp / (tp + fn) if (tp + fn) > 0 else 0,
        "timing/total_seconds": total_time,
        "timing/total_minutes": total_time / 60,
        "status": "success"
    }
    
    if roc_auc:
        log_dict["metrics/roc_auc"] = roc_auc
    
    wandb.log(log_dict)
    
    print(f"\nTotal time: {total_time:.1f}s ({total_time/60:.1f} min)")
    print("Done.")
    
    wandb.finish()


if __name__ == "__main__":
    main()

