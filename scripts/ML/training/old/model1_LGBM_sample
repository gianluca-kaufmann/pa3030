#!/usr/bin/env python3

"""LGBM Transition Model - Stratified Sample (0→1 Prediction)

Purpose: Train a LightGBM model to predict transitions to protected areas (0→1).
         Uses a stratified sample of 10M rows from training years (2000-2017) 
         with GroupKFold cross-validation for hyperparameter tuning.
         Only performs tuning, does not train final model.

Input:   `merged_panel_final.parquet`
Output:  - Best parameters and CV score (.json)
"""

from __future__ import annotations

import gc
import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any

import numpy as np
import pandas as pd
from lightgbm import LGBMClassifier
from sklearn.model_selection import GroupKFold, GridSearchCV


class Tee:
    """Write to both stdout and a file."""
    def __init__(self, file_path: Path):
        self.file = open(file_path, 'w', encoding='utf-8')
        self.stdout = sys.stdout
    
    def write(self, text: str) -> None:
        self.stdout.write(text)
        self.file.write(text)
        self.file.flush()
    
    def flush(self) -> None:
        self.stdout.flush()
        self.file.flush()
    
    def close(self) -> None:
        self.file.close()


# =============================================================================
# Configuration
# =============================================================================
RANDOM_STATE = 42

# Temporal split - only training years
TRAIN_YEARS = range(2000, 2018)  # 2000-2017

# Sample size
SAMPLE_SIZE = 10_000_000  # 10 million rows

# Columns to exclude from features
EXCLUDE_COLS = {
    'transition_01',  # Target variable
    'WDPA_b1',        # Leakage
    'WDPA_prev',      # Leakage
    'x',              # Coordinate
    'y',              # Coordinate
    'row',            # Identifier
    'col',            # Identifier
    'year',           # Temporal identifier
}

# GroupKFold configuration
N_SPLITS = 5

# Hyperparameter search space (kept compact to limit combinations)
PARAM_GRID = {
    'n_estimators': [200, 500],
    'learning_rate': [0.05, 0.1],
    'num_leaves': [31, 63],
    'max_depth': [-1, 10, 20],
    'min_child_samples': [20, 100],
    'subsample': [0.7, 1.0],
    'colsample_bytree': [0.7, 1.0],
}

# Fixed parameters
FIXED_PARAMS = {
    'random_state': RANDOM_STATE,
    'n_jobs': -1,
    'boosting_type': 'gbdt',
    'objective': 'binary',
    'class_weight': 'balanced',
    'verbose': -1,
}


# =============================================================================
# Utility Functions
# =============================================================================

def convert_numpy_types(obj: Any) -> Any:
    """Recursively convert NumPy types to native Python types for JSON serialization."""
    if isinstance(obj, (np.integer, np.int_, np.intc, np.intp, np.int8,
                        np.int16, np.int32, np.int64, np.uint8, np.uint16,
                        np.uint32, np.uint64)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float_, np.float16, np.float32, np.float64)):
        return float(obj)
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [convert_numpy_types(item) for item in obj]
    elif obj is None or isinstance(obj, (str, int, float, bool)):
        return obj
    else:
        try:
            if hasattr(obj, 'item'):
                return obj.item()
            else:
                return obj
        except (ValueError, TypeError, AttributeError):
            return str(obj)


def load_data_year(parquet_path: Path, year: int, columns: Optional[list] = None) -> pd.DataFrame:
    """Load data for a single year from parquet file efficiently."""
    filters = [('year', '==', year)]
    df = pd.read_parquet(parquet_path, filters=filters, columns=columns)
    return df


def stratified_sample_chunked(
    parquet_path: Path,
    years: range,
    n_samples: int,
    stratify_cols: list,
    target_col: str,
    random_state: int = 42
) -> pd.DataFrame:
    """
    Draw a stratified sample by reading year-by-year, preserving distribution.
    Uses chunked reading to avoid loading all years into memory at once.
    
    Args:
        parquet_path: Path to parquet file
        years: Range of years to process
        n_samples: Number of samples to draw
        stratify_cols: List of column names to preserve distribution for
        target_col: Target column name
        random_state: Random seed
    
    Returns:
        Sampled DataFrame
    """
    print(f"\nDrawing stratified sample of {n_samples:,} rows (chunked by year)...")
    print(f"  Preserving distribution of: {', '.join(stratify_cols)}")
    print(f"  Processing years {years.start}-{years.stop-1} year-by-year")
    
    # First pass: Count rows per stratum across all years
    print("\n  First pass: Counting rows per stratum...")
    stratum_counts = {}
    total_rows = 0
    
    for year in years:
        df_year = load_data_year(parquet_path, year)
        if len(df_year) == 0:
            continue
        
        if target_col not in df_year.columns:
            del df_year
            gc.collect()
            continue
        
        df_year_clean = df_year.dropna(subset=[target_col])
        if len(df_year_clean) == 0:
            del df_year
            gc.collect()
            continue
        
        df_year_clean = downcast_float64_to_float32(df_year_clean)
        
        # Create binary target for stratification
        df_year_clean['transition_01_binary'] = (df_year_clean[target_col] > 0).astype(int)
        
        # Count per stratum (year, transition_01_binary) - vectorized
        stratum_counts_year = df_year_clean.groupby(['year', 'transition_01_binary']).size()
        for (y, trans), count in stratum_counts_year.items():
            stratum_key = (int(y), int(trans))
            stratum_counts[stratum_key] = stratum_counts.get(stratum_key, 0) + count
            total_rows += count
        
        del df_year, df_year_clean
        gc.collect()
    
    if total_rows == 0:
        raise ValueError("No data found in specified years")
    
    if total_rows <= n_samples:
        print(f"  Dataset has {total_rows:,} rows <= {n_samples:,}, loading full dataset")
        # Load all data if it fits
        sampled_dfs = []
        for year in years:
            df_year = load_data_year(parquet_path, year)
            if len(df_year) == 0:
                continue
            if target_col not in df_year.columns:
                continue
            df_year_clean = df_year.dropna(subset=[target_col])
            if len(df_year_clean) > 0:
                sampled_dfs.append(df_year_clean)
            del df_year
            gc.collect()
        return pd.concat(sampled_dfs, ignore_index=True)
    
    n_strata = len(stratum_counts)
    print(f"  Found {n_strata} unique strata across {total_rows:,} total rows")
    
    # Calculate sampling fraction per stratum
    sampling_fraction = n_samples / total_rows
    stratum_sample_sizes = {
        stratum_key: max(1, int(count * sampling_fraction))
        for stratum_key, count in stratum_counts.items()
    }
    
    # Second pass: Sample from each year proportionally
    print("\n  Second pass: Sampling from each year...")
    sampled_dfs = []
    np.random.seed(random_state)
    
    for year in years:
        df_year = load_data_year(parquet_path, year)
        if len(df_year) == 0:
            continue
        
        if target_col not in df_year.columns:
            del df_year
            gc.collect()
            continue
        
        df_year_clean = df_year.dropna(subset=[target_col])
        if len(df_year_clean) == 0:
            del df_year
            gc.collect()
            continue
        
        df_year_clean = downcast_float64_to_float32(df_year_clean)
        df_year_clean['transition_01_binary'] = (df_year_clean[target_col] > 0).astype(int)
        
        # Sample from each stratum in this year
        year_samples = []
        for stratum_key, n_sample in stratum_sample_sizes.items():
            stratum_year, stratum_transition = stratum_key
            if stratum_year != year:
                continue
            
            stratum_df = df_year_clean[
                df_year_clean['transition_01_binary'] == stratum_transition
            ]
            
            if len(stratum_df) == 0:
                continue
            
            if n_sample >= len(stratum_df):
                year_samples.append(stratum_df)
            else:
                year_samples.append(
                    stratum_df.sample(n=n_sample, random_state=random_state)
                )
        
        if year_samples:
            sampled_dfs.append(pd.concat(year_samples, ignore_index=True))
        
        del df_year, df_year_clean
        gc.collect()
    
    # Combine all samples
    if not sampled_dfs:
        raise ValueError("No samples collected")
    
    sampled_df = pd.concat(sampled_dfs, ignore_index=True)
    
    # If we got more than needed, randomly sample down
    if len(sampled_df) > n_samples:
        sampled_df = sampled_df.sample(n=n_samples, random_state=random_state).reset_index(drop=True)
    elif len(sampled_df) < n_samples:
        print(f"  Warning: Only sampled {len(sampled_df):,} rows (requested {n_samples:,})")
    
    # Drop temporary column
    if 'transition_01_binary' in sampled_df.columns:
        sampled_df = sampled_df.drop(columns=['transition_01_binary'])
    
    print(f"  Sampled {len(sampled_df):,} rows")
    
    return sampled_df


def get_feature_columns(df: pd.DataFrame) -> list:
    """Get valid feature columns, excluding identifiers and leakage columns."""
    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
    feature_cols = [
        col for col in numeric_cols
        if col.lower() not in {c.lower() for c in EXCLUDE_COLS}
    ]
    return feature_cols


def downcast_float64_to_float32(df: pd.DataFrame) -> pd.DataFrame:
    """Downcast float64 columns to float32 to reduce memory footprint."""
    float64_cols = df.select_dtypes(include=["float64"]).columns
    if len(float64_cols) > 0:
        print(f"  Downcasting {len(float64_cols)} float64 columns to float32")
        df[float64_cols] = df[float64_cols].astype(np.float32)
    return df


# =============================================================================
# Main Pipeline
# =============================================================================

def main() -> None:
    start_time = time.time()
    
    # -------------------------------------------------------------------------
    # Setup paths
    # -------------------------------------------------------------------------
    repo_root = Path(__file__).resolve().parents[3]
    scratch_root = Path(os.environ["SCRATCH"]) if os.environ.get("SCRATCH") else None
    
    # Input path (prefer SCRATCH on Euler; check multiple possible locations)
    candidate_inputs = []
    if scratch_root is not None and scratch_root.exists():
        # Euler cluster: Check common locations
        candidate_inputs.append(scratch_root / "outputs/Results/merged_panel_final.parquet")
        candidate_inputs.append(scratch_root / "data/ml/merged_panel_final.parquet")
    # Local fallback
    candidate_inputs.append(repo_root / "data/ml/merged_panel_final.parquet")
    
    input_path = None
    for cand in candidate_inputs:
        if cand.exists():
            input_path = cand
            break
    
    if input_path is None:
        error_msg = (
            "Could not find merged_panel_final.parquet in any expected location.\n"
            f"Checked:\n"
        )
        for cand in candidate_inputs:
            error_msg += f"  - {cand}\n"
        raise FileNotFoundError(error_msg)
    
    # Output directories
    output_dir = repo_root / "outputs/Results/ml_models"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    print("=" * 70)
    print("LGBM TRANSITION MODEL - STRATIFIED SAMPLE (0→1 PREDICTION)")
    print("=" * 70)
    print(f"\nInput:  {input_path}")
    print(f"Output: {output_dir}")
    
    # -------------------------------------------------------------------------
    # Step 1: Chunked stratified sampling (year-by-year)
    # -------------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("STEP 1: CHUNKED STRATIFIED SAMPLING")
    print("=" * 70)
    
    target_col = "transition_01"
    
    # Draw stratified sample preserving year and transition_01 distribution
    # This reads year-by-year to avoid OOM - never loads all 18 years at once
    df_sample = stratified_sample_chunked(
        parquet_path=input_path,
        years=TRAIN_YEARS,
        n_samples=SAMPLE_SIZE,
        stratify_cols=['year', 'transition_01_binary'],
        target_col=target_col,
        random_state=RANDOM_STATE
    )
    
    # Downcast to save memory
    df_sample = downcast_float64_to_float32(df_sample)
    
    # Get feature columns from sample
    feature_cols = get_feature_columns(df_sample)
    excluded_cols = sorted(EXCLUDE_COLS & set(df_sample.columns))
    
    print(f"\nUsing {len(feature_cols)} features")
    print(f"Excluded columns ({len(excluded_cols)}): {excluded_cols}")
    
    # Show sample distribution
    sample_pos = (df_sample[target_col] > 0).sum()
    sample_neg = (df_sample[target_col] == 0).sum()
    sample_pos_pct = sample_pos / len(df_sample) * 100
    
    # Calculate class ratio and add scale_pos_weight to parameter grid
    ratio = sample_neg / max(sample_pos, 1)
    PARAM_GRID['scale_pos_weight'] = [0.5 * ratio, ratio, 2 * ratio]
    
    print(f"\n" + "-" * 40)
    print("Sampled set distribution:")
    print(f"  No transition (0): {sample_neg:>12,}  ({100 - sample_pos_pct:.3f}%)")
    print(f"  Transition (0→1):  {sample_pos:>12,}  ({sample_pos_pct:.3f}%)")
    print(f"  Class ratio:       1 : {sample_neg / max(sample_pos, 1):.1f}")
    print("-" * 40)
    
    gc.collect()
    
    # -------------------------------------------------------------------------
    # Step 2: Prepare features and target
    # -------------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("STEP 2: PREPARE FEATURES AND TARGET")
    print("=" * 70)
    
    # Prepare target and features
    y_sample = (df_sample[target_col] > 0).astype(np.int8)
    X_sample = df_sample[feature_cols]
    groups_sample = df_sample['year'].values
    
    print(f"Feature matrix shape: {X_sample.shape}")
    print(f"Target shape: {y_sample.shape}")
    
    # Check for missing values
    missing_sample = X_sample.isnull().sum()
    cols_with_missing = missing_sample[missing_sample > 0]
    if len(cols_with_missing) > 0:
        print(f"\nWarning: {len(cols_with_missing)} columns have missing values")
        print("  Top 5:")
        for col, count in cols_with_missing.head().items():
            print(f"    {col}: {count:,} ({count/len(X_sample)*100:.2f}%)")
    
    # -------------------------------------------------------------------------
    # Step 3: Hyperparameter tuning with GroupKFold
    # -------------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("STEP 3: HYPERPARAMETER TUNING WITH GROUPKFOLD")
    print("=" * 70)
    
    print(f"\nUsing GroupKFold with {N_SPLITS} splits (grouped by year)")
    
    # Calculate number of parameter combinations
    n_combinations = 1
    for values in PARAM_GRID.values():
        n_combinations *= len(values)
    print(f"Searching over {n_combinations} hyperparameter combinations")
    
    # Create base model
    base_model = LGBMClassifier(**FIXED_PARAMS)
    
    # Create GroupKFold
    group_kfold = GroupKFold(n_splits=N_SPLITS)
    
    print("\nParameter grid:")
    for param, values in PARAM_GRID.items():
        print(f"  {param}: {values}")
    
    print("\nFixed parameters:")
    for param, value in FIXED_PARAMS.items():
        print(f"  {param}: {value}")
    
    # Grid search
    print(f"\nStarting grid search...")
    print(f"This may take a while...\n")
    
    gc.collect()
    grid_search = GridSearchCV(
        estimator=base_model,
        param_grid=PARAM_GRID,
        cv=group_kfold,
        scoring='average_precision',  # PR-AUC (better for imbalanced data)
        n_jobs=-1,
        verbose=2,
        return_train_score=True,
    )
    
    tune_start = time.time()
    grid_search.fit(X_sample, y_sample, groups=groups_sample)
    tune_time = time.time() - tune_start
    
    print(f"\nGrid search completed in {tune_time:.1f}s ({tune_time/60:.1f} min)")
    
    # Extract best CV score and params
    best_cv_score = float(grid_search.best_score_)
    best_cv_params = grid_search.best_params_.copy()
    
    # -------------------------------------------------------------------------
    # Step 4: Save results
    # -------------------------------------------------------------------------
    print("\n" + "=" * 70)
    print("STEP 4: SAVE RESULTS")
    print("=" * 70)
    
    # Print results clearly
    print("\n" + "=" * 70)
    print("HYPERPARAMETER TUNING RESULTS")
    print("=" * 70)
    print(f"\nBest CV Score (PR-AUC): {best_cv_score:.6f}")
    print(f"\nBest Parameters:")
    for param, value in sorted(best_cv_params.items()):
        print(f"  {param:25s}: {value}")
    print("=" * 70)
    
    # Prepare results dictionary
    results = {
        "metadata": {
            "timestamp": timestamp,
            "model": "LightGBM",
            "task": "transition_01_prediction",
            "random_state": RANDOM_STATE,
            "train_years": f"{TRAIN_YEARS.start}-{TRAIN_YEARS.stop-1}",
            "sample_size": SAMPLE_SIZE,
            "n_features": len(feature_cols),
        },
        "hyperparameter_tuning": {
            "method": "GridSearchCV",
            "cv_strategy": f"GroupKFold(n_splits={N_SPLITS})",
            "scoring": "average_precision",
            "best_score": best_cv_score,
            "best_params": best_cv_params,
            "tuning_time_seconds": tune_time,
            "n_combinations": n_combinations,
        },
        "data": {
            "n_samples": int(len(X_sample)),
            "sample_positives": int(sample_pos),
            "sample_negatives": int(sample_neg),
            "sample_positive_pct": float(sample_pos_pct),
        },
    }
    
    # Convert NumPy types to native Python types
    results = convert_numpy_types(results)
    
    # Save results
    results_path = output_dir / f"model1_lgbm_sample_results_{timestamp}.json"
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"\nResults saved to: {results_path}")
    
    # -------------------------------------------------------------------------
    # Summary
    # -------------------------------------------------------------------------
    total_time = time.time() - start_time
    
    print("\n" + "=" * 70)
    print("SUMMARY")
    print("=" * 70)
    print(f"Model:                       LightGBM")
    print(f"Sample size:                  {len(X_sample):,}")
    print(f"Best CV score (PR-AUC):       {best_cv_score:.6f}")
    print(f"Best parameters:              {best_cv_params}")
    print(f"Features used:                {len(feature_cols)}")
    print(f"\nTimings:")
    print(f"  Hyperparameter tuning:      {tune_time:.1f}s ({tune_time/60:.1f} min)")
    print(f"  Total time:                 {total_time:.1f}s ({total_time/60:.1f} min)")
    print("=" * 70)
    print("Done.")


if __name__ == "__main__":
    # Set up output file capture
    repo_root = Path(__file__).resolve().parents[3]
    output_dir = repo_root / "outputs/Results/ml_models"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"model1_lgbm_sample_{timestamp}.txt"
    
    tee = Tee(output_file)
    sys.stdout = tee
    
    try:
        main()
    finally:
        sys.stdout = tee.stdout
        tee.close()
        print(f"\nOutput saved to: {output_file}")
