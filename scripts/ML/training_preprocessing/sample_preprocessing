#!/usr/bin/env python3

"""Temporal Transition Sampling Script

Purpose: build a balanced dataset for predicting new WDPA establishment.
Input:  `data/ml/merged_panel_2000_2024.parquet`.
Process: load multiple years, use self-joins to create lagged WDPA column,
         identify 0→1 transitions, and sample up to 2M rows with balanced classes.
Output: `data/ml/sample_training.parquet`.
"""

from __future__ import annotations

from pathlib import Path

import duckdb
import os
import time
import pyarrow.parquet as pq
import wandb

# Configuration
SAMPLE_YEARS = (2000, 2024)  # Inclusive range - expanded for more transitions
TOTAL_SAMPLE_SIZE = 2_000_000
TRANSITION_LIMIT = 1_000_000  # Max positive transitions (0→1)
RANDOM_STATE = 42

# W&B configuration
WANDB_PROJECT = "ml-training-preprocessing"
WANDB_ENTITY = os.environ.get("WANDB_ENTITY", "gianlukauff")


def main() -> None:
    start_time = time.time()
    
    repo_root = Path(__file__).resolve().parents[3]
    input_path = repo_root / "data/ml/merged_panel_2000_2024.parquet"
    output_path = repo_root / "data/ml/sample_training.parquet"
    
    # Use temporary file for intermediate results
    temp_dir = repo_root / "data/ml/temp"
    temp_dir.mkdir(parents=True, exist_ok=True)

    # Initialize Weights & Biases
    year_min, year_max = SAMPLE_YEARS
    wandb.init(
        project=WANDB_PROJECT,
        entity=WANDB_ENTITY,
        name=f"sample_preprocessing_{year_min}_{year_max}",
        config={
            "sample_years": SAMPLE_YEARS,
            "total_sample_size": TOTAL_SAMPLE_SIZE,
            "transition_limit": TRANSITION_LIMIT,
            "random_state": RANDOM_STATE,
            "input_path": str(input_path),
            "output_path": str(output_path),
        },
        save_code=True,
    )
    print("Weights & Biases connected successfully!")
    wandb.log({"status": "started", "start_time": start_time})

    print(f"Loading data from {input_path} …")
    if not input_path.exists():
        wandb.log({"status": "failed", "error": "input_file_not_found"})
        wandb.finish()
        raise FileNotFoundError(f"Missing source file: {input_path}")

    output_path.parent.mkdir(parents=True, exist_ok=True)

    escaped_in = str(input_path).replace("'", "''")
    con = duckdb.connect()
    
    # Configure DuckDB for memory efficiency and large datasets
    # Use more threads if on Euler (check if SCRATCH exists), otherwise use 2 for laptop
    num_threads = 8 if os.environ.get("SCRATCH") else 2
    con.execute(f"SET threads={num_threads}")
    con.execute("SET preserve_insertion_order=false")
    
    # Set temp directory to $SCRATCH if available (more space on Euler)
    temp_dir_env = os.environ.get("SCRATCH") or str(temp_dir)
    con.execute(f"SET temp_directory='{temp_dir_env}/duckdb_temp'")
    # Increase temp directory size limit (100GB on Euler, 20GB on laptop)
    max_temp_size = "100GB" if os.environ.get("SCRATCH") else "20GB"
    con.execute(f"PRAGMA max_temp_directory_size='{max_temp_size}'")
    
    print(f"DuckDB configured: {num_threads} threads, temp_dir='{temp_dir_env}/duckdb_temp', max_temp={max_temp_size}")

    # Verify WDPA_b1 column exists
    schema_df = con.execute(
        f"DESCRIBE SELECT * FROM read_parquet('{escaped_in}')"
    ).df()
    has_wdpa = schema_df["column_name"].str.lower().eq("wdpa_b1").any()

    if not has_wdpa:
        wandb.log({"status": "failed", "error": "WDPA_b1_column_missing"})
        wandb.finish()
        raise ValueError(
            "'WDPA_b1' column does not exist in the merged panel; "
            "cannot derive protected labels."
        )

    print(f"Processing years {year_min}–{year_max} …")
    wandb.log({"processing/year_range": f"{year_min}-{year_max}"})

    # Step 1: Create transition data using self-join approach (memory-efficient)
    # For each year Y, join with year Y-1 to get previous WDPA status
    temp_transitions = temp_dir / "temp_transitions.parquet"
    
    print("Computing transitions using year-by-year self-joins …")
    print("  This avoids expensive window functions and processes incrementally.")
    
    # Build the query using self-joins for each consecutive year pair
    # This is much more memory-efficient than window functions
    escaped_temp = str(temp_transitions).replace("'", "''")
    
    transition_query = f"""
        WITH base AS (
            SELECT *
            FROM read_parquet('{escaped_in}')
            WHERE year >= {year_min} AND year <= {year_max}
        )
        SELECT 
            curr.*,
            COALESCE(prev.WDPA_b1, 0) AS WDPA_prev,
            CASE 
                WHEN COALESCE(prev.WDPA_b1, 0) = 0 AND COALESCE(curr.WDPA_b1, 0) = 1 THEN 1
                ELSE 0
            END AS transition_01
        FROM base AS curr
        LEFT JOIN base AS prev
            ON curr.row = prev.row 
            AND curr.col = prev.col 
            AND prev.year = curr.year - 1
        WHERE prev.year IS NOT NULL
    """
    
    print(f"  Writing transitions to temporary file …")
    materialize_start = time.time()
    con.execute(
        f"""
        COPY ({transition_query})
        TO '{escaped_temp}'
        (FORMAT PARQUET)
        """
    )
    materialize_time = time.time() - materialize_start
    print(f"  Materialized transitions to temporary file in {materialize_time/60:.1f} minutes.")
    wandb.log({"processing/materialize_time_minutes": materialize_time / 60})

    # Step 2: Count available transitions from the temporary file
    print("Counting transitions …")
    count_query = f"""
        SELECT 
            SUM(CASE WHEN transition_01 = 1 THEN 1 ELSE 0 END) AS positive_transitions,
            SUM(CASE WHEN transition_01 = 0 THEN 1 ELSE 0 END) AS negative_transitions,
            COUNT(*) AS total_rows
        FROM read_parquet('{escaped_temp}')
    """
    
    counts = con.execute(count_query).fetchone()
    pos_available, neg_available, total_available = counts
    
    print(f"  Total rows with lagged data: {total_available:,}")
    print(f"  Positive transitions (0→1): {pos_available:,}")
    print(f"  Negative transitions (other): {neg_available:,}")
    
    wandb.log({
        "data/total_rows": total_available,
        "data/positive_transitions": pos_available,
        "data/negative_transitions": neg_available,
        "data/positive_ratio": pos_available / total_available if total_available > 0 else 0,
    })
    
    # Diagnostic: Check WDPA distribution to understand why there are so few transitions
    print("\nDiagnostic: Checking WDPA_b1 value distribution …")
    diag_query = f"""
        SELECT 
            MIN(WDPA_b1) AS min_wdpa,
            MAX(WDPA_b1) AS max_wdpa,
            AVG(WDPA_b1) AS avg_wdpa,
            SUM(CASE WHEN WDPA_b1 > 0 THEN 1 ELSE 0 END) AS protected_pixels,
            COUNT(*) AS total_pixels
        FROM read_parquet('{escaped_temp}')
    """
    diag_result = con.execute(diag_query).fetchone()
    min_wdpa, max_wdpa, avg_wdpa, protected_pixels, total_pixels = diag_result
    
    print(f"  WDPA_b1 range: [{min_wdpa}, {max_wdpa}]")
    print(f"  WDPA_b1 average: {avg_wdpa:.4f}")
    print(f"  Protected pixels: {protected_pixels:,} / {total_pixels:,} ({protected_pixels/total_pixels*100:.1f}%)")
    
    wandb.log({
        "diagnostics/wdpa_min": min_wdpa,
        "diagnostics/wdpa_max": max_wdpa,
        "diagnostics/wdpa_avg": avg_wdpa,
        "diagnostics/protected_pixels": protected_pixels,
        "diagnostics/protected_ratio": protected_pixels / total_pixels if total_pixels > 0 else 0,
    })
    
    if pos_available == 0:
        temp_transitions.unlink(missing_ok=True)
        wandb.log({"status": "failed", "error": "no_positive_transitions"})
        wandb.finish()
        raise ValueError(
            f"No positive transitions found in years {year_min}–{year_max}. "
            "Cannot build training sample."
        )
    
    if pos_available < 10000:
        print(f"\n⚠️  WARNING: Only {pos_available:,} positive transitions available!")
        print(f"  This will result in severe class imbalance and poor model performance.")
        print(f"  Consider:")
        print(f"    - Expanding year range (currently {year_min}-{year_max})")
        print(f"    - Checking if WDPA_b1 is binary or needs thresholding")
        print(f"    - Investigating the data source for WDPA establishment dates")
        wandb.log({"warning": "very_few_positive_transitions"})

    # Step 3: Determine sampling strategy
    # Goal: Create a balanced dataset, but adapt if there are too few positives
    sample_pos = min(TRANSITION_LIMIT, pos_available)  # Use all positives if fewer than limit
    
    # For balanced sampling: match negatives to positives
    # But cap total sample size at TOTAL_SAMPLE_SIZE
    if sample_pos * 2 <= TOTAL_SAMPLE_SIZE:
        # We can do 50/50 balanced sampling
        sample_neg = sample_pos
        print(f"\nSampling strategy: BALANCED (50/50)")
    else:
        # Too many positives for balanced within total budget
        sample_pos = TOTAL_SAMPLE_SIZE // 2
        sample_neg = TOTAL_SAMPLE_SIZE // 2
        print(f"\nSampling strategy: BALANCED with cap at {TOTAL_SAMPLE_SIZE:,} total")
    
    sample_neg = min(sample_neg, neg_available)  # Cap at available negatives
    
    print(f"  Positive transitions: {sample_pos:,} of {pos_available:,}")
    print(f"  Negative transitions: {sample_neg:,} of {neg_available:,}")
    print(f"  Total sample size: {sample_pos + sample_neg:,}")
    print(f"  Class balance: {sample_pos/(sample_pos+sample_neg)*100:.1f}% positive")
    
    wandb.log({
        "sampling/positive_sample": sample_pos,
        "sampling/negative_sample": sample_neg,
        "sampling/total_sample": sample_pos + sample_neg,
        "sampling/class_balance": sample_pos / (sample_pos + sample_neg),
    })

    # Step 4: Sample and write directly from temporary file
    print(f"\nWriting sample to {output_path} …")
    sample_start = time.time()
    
    # Combine both samples in one query from the materialized file
    escaped_out = str(output_path).replace("'", "''")
    con.execute(
        f"""
        COPY (
            SELECT * FROM (
                SELECT *
                FROM read_parquet('{escaped_temp}')
                WHERE transition_01 = 1
                USING SAMPLE reservoir({sample_pos} ROWS) REPEATABLE ({RANDOM_STATE})
            )
            UNION ALL
            SELECT * FROM (
                SELECT *
                FROM read_parquet('{escaped_temp}')
                WHERE transition_01 = 0
                USING SAMPLE reservoir({sample_neg} ROWS) REPEATABLE ({RANDOM_STATE})
            )
        )
        TO '{escaped_out}'
        (FORMAT PARQUET)
        """
    )
    sample_time = time.time() - sample_start
    print(f"  Sampled and wrote in {sample_time:.1f} seconds.")
    wandb.log({"processing/sample_time_seconds": sample_time})
    
    # Clean up temporary file
    temp_transitions.unlink(missing_ok=True)
    print("  Cleaned up temporary files.")

    # Step 5: Verify output
    print("\nVerifying output …")
    output_counts = con.execute(
        f"""
        SELECT 
            SUM(CASE WHEN transition_01 = 1 THEN 1 ELSE 0 END) AS positive,
            SUM(CASE WHEN transition_01 = 0 THEN 1 ELSE 0 END) AS negative,
            COUNT(*) AS total,
            COUNT(DISTINCT year) AS n_years,
            MIN(year) AS min_year,
            MAX(year) AS max_year
        FROM read_parquet('{escaped_out}')
        """
    ).fetchone()

    pos_written, neg_written, total_written, n_years, min_year, max_year = output_counts
    
    # Calculate file size
    parquet_size_mb = output_path.stat().st_size / (1024 * 1024) if output_path.exists() else 0
    
    # Calculate total time
    total_time = time.time() - start_time

    print(f"\n{'='*60}")
    print(f"Successfully wrote {total_written:,} rows to {output_path}")
    print(f"{'='*60}")
    print(f"  Positive transitions (0→1): {pos_written:,}")
    print(f"  Negative transitions:       {neg_written:,}")
    print(f"  Years covered:              {n_years} ({min_year}–{max_year})")
    print(f"  Class balance:              {pos_written / total_written * 100:.1f}% positive")
    print(f"  File size:                  {parquet_size_mb:.1f} MB")
    print(f"  Total time:                 {total_time/60:.1f} minutes")
    print(f"{'='*60}")
    
    # Log final results to W&B
    wandb.log({
        "output/positive_count": pos_written,
        "output/negative_count": neg_written,
        "output/total_rows": total_written,
        "output/years_covered": n_years,
        "output/year_min": min_year,
        "output/year_max": max_year,
        "output/class_balance": pos_written / total_written if total_written > 0 else 0,
        "output/file_size_mb": parquet_size_mb,
        "summary/total_time_minutes": total_time / 60,
        "summary/total_time_hours": total_time / 3600,
        "summary/status": "completed",
    })
    
    # Warn if class balance is poor
    class_balance = pos_written / total_written if total_written > 0 else 0
    if class_balance < 0.3 or class_balance > 0.7:
        print(f"\n⚠️  WARNING: Poor class balance ({class_balance*100:.1f}% positive)")
        print(f"  This may lead to biased model predictions.")
        print(f"  Consider rerunning with adjusted sampling parameters.")
        wandb.alert(
            title="Poor Class Balance",
            text=f"Class balance is {class_balance*100:.1f}% positive (target: 50%)",
            level=wandb.AlertLevel.WARN
        )
    
    wandb.finish()
    print("\nPreprocessing complete!")


if __name__ == "__main__":
    main()
