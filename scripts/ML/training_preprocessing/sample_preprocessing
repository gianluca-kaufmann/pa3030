#!/usr/bin/env python3

"""Temporal Transition Sampling Script

Purpose: build a balanced dataset for predicting new WDPA establishment.
Input:  `data/ml/merged_panel_2000_2024.parquet`.
Process: load multiple years, use self-joins to create lagged WDPA column,
         identify 0→1 transitions, and sample up to 2M rows with balanced classes.
Output: `data/ml/sample_training.parquet`.
"""

from __future__ import annotations

from pathlib import Path

import duckdb
import os
import pyarrow.parquet as pq

# Configuration
SAMPLE_YEARS = (2000, 2024)  # Inclusive range - expanded for more transitions
TOTAL_SAMPLE_SIZE = 2_000_000
TRANSITION_LIMIT = 1_000_000  # Max positive transitions (0→1)
RANDOM_STATE = 42


def main() -> None:
    repo_root = Path(__file__).resolve().parents[3]
    input_path = repo_root / "data/ml/merged_panel_2000_2024.parquet"
    output_path = repo_root / "data/ml/sample_training.parquet"
    
    # Use temporary file for intermediate results
    temp_dir = repo_root / "data/ml/temp"
    temp_dir.mkdir(parents=True, exist_ok=True)

    print(f"Loading data from {input_path} …")
    if not input_path.exists():
        raise FileNotFoundError(f"Missing source file: {input_path}")

    output_path.parent.mkdir(parents=True, exist_ok=True)

    escaped_in = str(input_path).replace("'", "''")
    con = duckdb.connect()
    
    # Configure DuckDB for memory efficiency and large datasets
    # Use more threads if on Euler (check if SCRATCH exists), otherwise use 2 for laptop
    num_threads = 8 if os.environ.get("SCRATCH") else 2
    con.execute(f"SET threads={num_threads}")
    con.execute("SET preserve_insertion_order=false")
    
    # Set temp directory to $SCRATCH if available (more space on Euler)
    temp_dir_env = os.environ.get("SCRATCH") or str(temp_dir)
    con.execute(f"SET temp_directory='{temp_dir_env}/duckdb_temp'")
    # Increase temp directory size limit (100GB on Euler, 20GB on laptop)
    max_temp_size = "100GB" if os.environ.get("SCRATCH") else "20GB"
    con.execute(f"PRAGMA max_temp_directory_size='{max_temp_size}'")
    
    print(f"DuckDB configured: {num_threads} threads, temp_dir='{temp_dir_env}/duckdb_temp', max_temp={max_temp_size}")

    # Verify WDPA_b1 column exists
    schema_df = con.execute(
        f"DESCRIBE SELECT * FROM read_parquet('{escaped_in}')"
    ).df()
    has_wdpa = schema_df["column_name"].str.lower().eq("wdpa_b1").any()

    if not has_wdpa:
        raise ValueError(
            "'WDPA_b1' column does not exist in the merged panel; "
            "cannot derive protected labels."
        )

    year_min, year_max = SAMPLE_YEARS
    print(f"Processing years {year_min}–{year_max} …")

    # Step 1: Create transition data using self-join approach (memory-efficient)
    # For each year Y, join with year Y-1 to get previous WDPA status
    temp_transitions = temp_dir / "temp_transitions.parquet"
    
    print("Computing transitions using year-by-year self-joins …")
    print("  This avoids expensive window functions and processes incrementally.")
    
    # Build the query using self-joins for each consecutive year pair
    # This is much more memory-efficient than window functions
    escaped_temp = str(temp_transitions).replace("'", "''")
    
    transition_query = f"""
        WITH base AS (
            SELECT *
            FROM read_parquet('{escaped_in}')
            WHERE year >= {year_min} AND year <= {year_max}
        )
        SELECT 
            curr.*,
            COALESCE(prev.WDPA_b1, 0) AS WDPA_prev,
            CASE 
                WHEN COALESCE(prev.WDPA_b1, 0) = 0 AND COALESCE(curr.WDPA_b1, 0) = 1 THEN 1
                ELSE 0
            END AS transition_01
        FROM base AS curr
        LEFT JOIN base AS prev
            ON curr.row = prev.row 
            AND curr.col = prev.col 
            AND prev.year = curr.year - 1
        WHERE prev.year IS NOT NULL
    """
    
    print(f"  Writing transitions to temporary file …")
    con.execute(
        f"""
        COPY ({transition_query})
        TO '{escaped_temp}'
        (FORMAT PARQUET)
        """
    )
    print(f"  Materialized transitions to temporary file.")

    # Step 2: Count available transitions from the temporary file
    print("Counting transitions …")
    count_query = f"""
        SELECT 
            SUM(CASE WHEN transition_01 = 1 THEN 1 ELSE 0 END) AS positive_transitions,
            SUM(CASE WHEN transition_01 = 0 THEN 1 ELSE 0 END) AS negative_transitions,
            COUNT(*) AS total_rows
        FROM read_parquet('{escaped_temp}')
    """
    
    counts = con.execute(count_query).fetchone()
    pos_available, neg_available, total_available = counts
    
    print(f"  Total rows with lagged data: {total_available:,}")
    print(f"  Positive transitions (0→1): {pos_available:,}")
    print(f"  Negative transitions (other): {neg_available:,}")

    if pos_available == 0:
        temp_transitions.unlink(missing_ok=True)
        raise ValueError(
            f"No positive transitions found in years {year_min}–{year_max}. "
            "Cannot build training sample."
        )

    # Step 3: Determine sampling strategy
    # We want to balance positive and negative transitions
    sample_pos = min(TRANSITION_LIMIT, pos_available)
    sample_neg = min(sample_pos, neg_available, TOTAL_SAMPLE_SIZE - sample_pos)
    
    print(f"\nSampling strategy:")
    print(f"  Positive transitions: {sample_pos:,} of {pos_available:,}")
    print(f"  Negative transitions: {sample_neg:,} of {neg_available:,}")
    print(f"  Total sample size: {sample_pos + sample_neg:,}")

    # Step 4: Sample and write directly from temporary file
    print(f"\nWriting sample to {output_path} …")
    
    # Combine both samples in one query from the materialized file
    escaped_out = str(output_path).replace("'", "''")
    con.execute(
        f"""
        COPY (
            SELECT * FROM (
                SELECT *
                FROM read_parquet('{escaped_temp}')
                WHERE transition_01 = 1
                USING SAMPLE reservoir({sample_pos} ROWS) REPEATABLE ({RANDOM_STATE})
            )
            UNION ALL
            SELECT * FROM (
                SELECT *
                FROM read_parquet('{escaped_temp}')
                WHERE transition_01 = 0
                USING SAMPLE reservoir({sample_neg} ROWS) REPEATABLE ({RANDOM_STATE})
            )
        )
        TO '{escaped_out}'
        (FORMAT PARQUET)
        """
    )
    
    # Clean up temporary file
    temp_transitions.unlink(missing_ok=True)
    print("  Cleaned up temporary files.")

    # Step 5: Verify output
    print("\nVerifying output …")
    output_counts = con.execute(
        f"""
        SELECT 
            SUM(CASE WHEN transition_01 = 1 THEN 1 ELSE 0 END) AS positive,
            SUM(CASE WHEN transition_01 = 0 THEN 1 ELSE 0 END) AS negative,
            COUNT(*) AS total,
            COUNT(DISTINCT year) AS n_years,
            MIN(year) AS min_year,
            MAX(year) AS max_year
        FROM read_parquet('{escaped_out}')
        """
    ).fetchone()

    pos_written, neg_written, total_written, n_years, min_year, max_year = output_counts

    print(f"\n{'='*60}")
    print(f"Successfully wrote {total_written:,} rows to {output_path}")
    print(f"{'='*60}")
    print(f"  Positive transitions (0→1): {pos_written:,}")
    print(f"  Negative transitions:       {neg_written:,}")
    print(f"  Years covered:              {n_years} ({min_year}–{max_year})")
    print(f"  Class balance:              {pos_written / total_written * 100:.1f}% positive")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()
