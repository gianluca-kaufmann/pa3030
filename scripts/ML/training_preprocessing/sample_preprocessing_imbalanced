#!/usr/bin/env python3

"""Realistic Ratio Sampling Script

Purpose: Build an evaluation dataset that preserves true class distribution.
Input:  merged_panel_final.parquet
Process: Create lagged WDPA column, identify 0→1 transitions, sample ~10M rows 
         with realistic class ratio, keep only numeric features.
Output: sample_training_imbalanced.parquet
"""

from __future__ import annotations

import os
import sys
import time
from pathlib import Path

import duckdb
import wandb

# Add project root to path for config import
sys.path.insert(0, str(Path(__file__).resolve().parents[3]))
import config


# Configuration
TARGET_SAMPLE_SIZE = 10_000_000  # Target ~10M rows
RANDOM_STATE = 42


def main() -> None:
    start_time = time.time()
    
    # Get W&B credentials from environment
    wandb_api_key = os.environ.get("WANDB_API_KEY")
    wandb_entity = os.environ.get("WANDB_ENTITY")
    
    if not wandb_api_key:
        print("Warning: WANDB_API_KEY not found in environment")
    if not wandb_entity:
        print("Warning: WANDB_ENTITY not found in environment")
    
    # Initialize W&B
    print("Initializing Weights & Biases...")
    wandb.init(
        project="ml-training-preprocessing",
        entity=wandb_entity,
        name=f"sample_preprocessing_imbalanced_{time.strftime('%Y%m%d_%H%M%S')}",
        config={
            "target_sample_size": TARGET_SAMPLE_SIZE,
            "random_state": RANDOM_STATE,
            "sampling_strategy": "realistic_ratio",
        },
    )
    print("W&B connected")
    
    # Setup paths using config
    input_path = config._join_path(config.get_ml_dir(), "merged_panel_final.parquet")
    output_path = config._join_path(config.get_ml_dir(), "sample_training_imbalanced.parquet")
    
    # For temp files, use local filesystem
    repo_root = Path(__file__).resolve().parents[3]
    temp_dir = repo_root / "data/ml/temp"
    temp_transitions = temp_dir / "temp_transitions_imbalanced.parquet"
    
    try:
        
        temp_dir.mkdir(parents=True, exist_ok=True)
        
        # Ensure output directory exists (handle both local and GCS paths)
        if config.is_gcs_path(output_path):
            print(f"Output will be written to GCS: {output_path}")
            # GCS directories don't need to be created, but we need local temp
            temp_dir.mkdir(parents=True, exist_ok=True)
        else:
            Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        
        print(f"\nInput: {input_path}")
        print(f"Output: {output_path}")
        
        # Check if input exists
        if config.is_gcs_path(input_path):
            # For GCS paths, DuckDB can read directly
            print("Input is on GCS - DuckDB will read directly")
        elif not Path(input_path).exists():
            raise FileNotFoundError(f"Input file not found: {input_path}")
        
        # Initialize DuckDB with memory-efficient settings
        con = duckdb.connect()
        
        # Set up GCS authentication if using GCS
        if config.is_gcs_path(input_path):
            print("Setting up GCS authentication for DuckDB...")
            con = config.setup_duckdb_gcs_auth(con)
        
        num_threads = 8 if os.environ.get("SCRATCH") else 4
        con.execute(f"SET threads={num_threads}")
        con.execute("SET preserve_insertion_order=false")
        con.execute("SET memory_limit='80GB'")  # Leave headroom on 128GB system
        
        # Use SCRATCH temp directory if on Euler
        temp_dir_env = os.environ.get("SCRATCH") or str(temp_dir)
        con.execute(f"SET temp_directory='{temp_dir_env}/duckdb_temp'")
        con.execute("PRAGMA max_temp_directory_size='100GB'")
        
        print(f"DuckDB configured: {num_threads} threads, memory_limit=80GB")
        
        wandb.log({"config/threads": num_threads, "config/memory_limit_gb": 80})
        
        # Escape paths for SQL
        escaped_in = str(input_path).replace("'", "''")
        escaped_out = str(output_path).replace("'", "''")
        escaped_temp = str(temp_transitions).replace("'", "''")
        
        # Step 1: Get column information - identify numeric columns
        print("\nStep 1: Identifying numeric columns...")
        step1_start = time.time()
        
        # Get all columns and their types
        schema = con.execute(f"DESCRIBE SELECT * FROM read_parquet('{escaped_in}')").fetchall()
        
        # Always keep these columns
        core_columns = {'row', 'col', 'year'}
        
        # Identify numeric columns (excluding core columns which we handle separately)
        numeric_types = {'BIGINT', 'INTEGER', 'DOUBLE', 'FLOAT', 'DECIMAL', 'HUGEINT', 'UBIGINT'}
        numeric_columns = []
        
        for col_name, col_type, *_ in schema:
            if col_name in core_columns:
                continue  # Handle these separately
            # Check if column type contains numeric type keywords
            col_type_upper = col_type.upper()
            if any(num_type in col_type_upper for num_type in numeric_types):
                numeric_columns.append(col_name)
        
        print(f"   Found {len(numeric_columns)} numeric feature columns")
        print(f"   Core columns: {', '.join(sorted(core_columns))}")
        
        wandb.log({
            "data/n_numeric_columns": len(numeric_columns),
            "data/n_core_columns": len(core_columns),
        })
        
        step1_time = time.time() - step1_start
        print(f"Completed in {step1_time:.1f}s")
        
        # Step 2: Create lagged WDPA and compute transitions
        print("\nStep 2: Computing lagged WDPA and transitions (self-join)...")
        print("   Using self-join instead of window functions for memory efficiency")
        step2_start = time.time()
        
        # Build SELECT clause for numeric columns only
        numeric_select = ", ".join([f"curr.{col}" for col in numeric_columns])
        
        # Self-join approach: much more memory-efficient than LAG()
        transition_query = f"""
        SELECT 
            curr.row,
            curr.col,
            curr.year,
            {numeric_select},
            COALESCE(prev.WDPA_b1, 0) AS WDPA_prev,
            CASE 
                WHEN COALESCE(prev.WDPA_b1, 0) = 0 AND COALESCE(curr.WDPA_b1, 0) = 1 THEN 1
                ELSE 0
            END AS transition_01
        FROM read_parquet('{escaped_in}') AS curr
        LEFT JOIN read_parquet('{escaped_in}') AS prev
            ON curr.row = prev.row 
            AND curr.col = prev.col 
            AND prev.year = curr.year - 1
        WHERE prev.year IS NOT NULL
        """
        
        # Materialize to temporary file to avoid memory overflow
        print(f"   Materializing to temporary file: {temp_transitions}")
        con.execute(f"""
            COPY ({transition_query})
            TO '{escaped_temp}'
            (FORMAT PARQUET, COMPRESSION ZSTD)
        """)
        
        step2_time = time.time() - step2_start
        print(f"Completed in {step2_time:.1f}s ({step2_time/60:.1f} min)")
        wandb.log({"timing/step2_seconds": step2_time, "timing/step2_minutes": step2_time/60})
        
        # Step 3: Count positives and negatives from temporary file
        print("\nStep 3: Analyzing class distribution...")
        step3_start = time.time()
        
        counts = con.execute(f"""
            SELECT 
                SUM(transition_01) AS positives,
                SUM(CASE WHEN transition_01 = 0 THEN 1 ELSE 0 END) AS negatives,
                COUNT(*) AS total,
                MIN(year) AS year_min,
                MAX(year) AS year_max
            FROM read_parquet('{escaped_temp}')
        """).fetchone()
        
        positives, negatives, total, year_min, year_max = counts
        true_positive_ratio = positives / total if total > 0 else 0
        
        print(f"   Total rows (with lagged data): {total:,}")
        print(f"   Positive transitions (0→1):    {positives:,} ({true_positive_ratio*100:.4f}%)")
        print(f"   Negative transitions:          {negatives:,} ({(1-true_positive_ratio)*100:.4f}%)")
        print(f"   Year range:                    {year_min}–{year_max}")
        
        step3_time = time.time() - step3_start
        wandb.log({
            "data/total_rows": total,
            "data/positives": positives,
            "data/negatives": negatives,
            "data/true_positive_ratio": true_positive_ratio,
            "data/year_min": year_min,
            "data/year_max": year_max,
            "timing/step3_seconds": step3_time,
        })
        
        if positives == 0:
            raise ValueError("No positive transitions found! Cannot build training sample.")
        
        # Step 4: Determine sampling strategy (preserve ratio)
        print("\nStep 4: Determining sampling strategy...")
        
        # Cap sample size at available data
        actual_sample_size = min(TARGET_SAMPLE_SIZE, total)
        
        # Calculate samples per class to preserve ratio
        sample_positives = int(actual_sample_size * true_positive_ratio)
        sample_negatives = actual_sample_size - sample_positives
        
        # Ensure we don't sample more than available
        sample_positives = min(sample_positives, positives)
        sample_negatives = min(sample_negatives, negatives)
        total_sample = sample_positives + sample_negatives
        
        print(f"   Target sample size:   {TARGET_SAMPLE_SIZE:,}")
        print(f"   Actual sample size:   {total_sample:,}")
        print(f"   Sampling {sample_positives:,} positives (of {positives:,} available)")
        print(f"   Sampling {sample_negatives:,} negatives (of {negatives:,} available)")
        print(f"   Preserved ratio:      {sample_positives/total_sample*100:.4f}% positive")
        
        wandb.log({
            "sampling/positives": sample_positives,
            "sampling/negatives": sample_negatives,
            "sampling/total": total_sample,
            "sampling/preserved_ratio": sample_positives / total_sample if total_sample > 0 else 0,
        })
        
        # Step 5: Sample and write output (separate sampling for each class)
        print("\nStep 5: Sampling and combining with shuffle...")
        step5_start = time.time()
        
        # Create temporary files for each class
        temp_positives = temp_dir / "temp_positives_imbalanced.parquet"
        temp_negatives = temp_dir / "temp_negatives_imbalanced.parquet"
        escaped_pos = str(temp_positives).replace("'", "''")
        escaped_neg = str(temp_negatives).replace("'", "''")
        
        # Sample positives first
        print(f"   Sampling {sample_positives:,} positive transitions...")
        con.execute(f"""
            COPY (
                SELECT *
                FROM (
                    SELECT *
                    FROM read_parquet('{escaped_temp}')
                    WHERE transition_01 = 1
                )
                USING SAMPLE reservoir({sample_positives} ROWS) REPEATABLE ({RANDOM_STATE})
            )
            TO '{escaped_pos}'
            (FORMAT PARQUET, COMPRESSION ZSTD)
        """)
        
        # Sample negatives
        print(f"   Sampling {sample_negatives:,} negative transitions...")
        con.execute(f"""
            COPY (
                SELECT *
                FROM (
                    SELECT *
                    FROM read_parquet('{escaped_temp}')
                    WHERE transition_01 = 0
                )
                USING SAMPLE reservoir({sample_negatives} ROWS) REPEATABLE ({RANDOM_STATE})
            )
            TO '{escaped_neg}'
            (FORMAT PARQUET, COMPRESSION ZSTD)
        """)
        
        # Combine both samples with shuffle and write to final output
        print(f"   Combining samples with shuffle into final output...")
        con.execute(f"""
            COPY (
                SELECT * FROM (
                    SELECT * FROM read_parquet('{escaped_pos}')
                    UNION ALL
                    SELECT * FROM read_parquet('{escaped_neg}')
                )
                ORDER BY random()
            )
            TO '{escaped_out}'
            (FORMAT PARQUET, COMPRESSION ZSTD)
        """)
        
        step5_time = time.time() - step5_start
        print(f"Written to {output_path} in {step5_time:.1f}s ({step5_time/60:.1f} min)")
        wandb.log({"timing/step5_seconds": step5_time, "timing/step5_minutes": step5_time/60})
        
        # Clean up temporary files
        print("Cleaning up temporary files...")
        temp_transitions.unlink(missing_ok=True)
        temp_positives.unlink(missing_ok=True)
        temp_negatives.unlink(missing_ok=True)
        print("Temporary files removed")
        
        # Step 6: Verify output
        print("\nStep 6: Verifying output...")
        
        verify_counts = con.execute(f"""
            SELECT 
                SUM(transition_01) AS pos_written,
                SUM(CASE WHEN transition_01 = 0 THEN 1 ELSE 0 END) AS neg_written,
                COUNT(*) AS total_written,
                COUNT(DISTINCT year) AS n_years,
                MIN(year) AS min_year,
                MAX(year) AS max_year
            FROM read_parquet('{escaped_out}')
        """).fetchone()
        
        pos_written, neg_written, total_written, n_years, min_year, max_year = verify_counts
        file_size_mb = output_path.stat().st_size / (1024 * 1024)
        total_time = time.time() - start_time
        
        final_ratio = pos_written / total_written if total_written > 0 else 0
        
        # Print summary
        print("\n" + "="*70)
        print("REALISTIC RATIO SAMPLING COMPLETE")
        print("="*70)
        print(f"  Total rows:          {total_written:,}")
        print(f"  Positives (0→1):     {pos_written:,} ({final_ratio*100:.4f}%)")
        print(f"  Negatives:           {neg_written:,} ({(1-final_ratio)*100:.4f}%)")
        print(f"  True ratio:          {true_positive_ratio*100:.4f}%")
        print(f"  Ratio preserved:     {abs(final_ratio - true_positive_ratio) < 0.0001}")
        print(f"  Years covered:       {n_years} years ({min_year}–{max_year})")
        print(f"  Numeric features:    {len(numeric_columns)}")
        print(f"  File size:           {file_size_mb:.1f} MB")
        print(f"  Total time:          {total_time:.1f}s ({total_time/60:.1f} min)")
        print("="*70)
        
        # Log final results to W&B
        wandb.log({
            "output/total_rows": total_written,
            "output/positives": pos_written,
            "output/negatives": neg_written,
            "output/final_ratio": final_ratio,
            "output/true_ratio": true_positive_ratio,
            "output/ratio_preserved": abs(final_ratio - true_positive_ratio) < 0.0001,
            "output/n_years": n_years,
            "output/year_min": min_year,
            "output/year_max": max_year,
            "output/n_numeric_features": len(numeric_columns),
            "output/file_size_mb": file_size_mb,
            "timing/total_seconds": total_time,
            "timing/total_minutes": total_time / 60,
            "status": "success",
        })
        
        print("\nAll done! Evaluation dataset ready for testing.")
        
    except Exception as e:
        error_msg = f"{type(e).__name__}: {str(e)}"
        print(f"\nERROR: {error_msg}")
        wandb.log({
            "status": "failed",
            "error_type": type(e).__name__,
            "error_message": str(e),
        })
        raise
    
    finally:
        # Clean up temporary files in case of error
        try:
            cleanup_files = [
                temp_transitions,
                temp_dir / "temp_positives_imbalanced.parquet",
                temp_dir / "temp_negatives_imbalanced.parquet"
            ]
            cleaned = False
            for f in cleanup_files:
                if f.exists():
                    f.unlink()
                    cleaned = True
            if cleaned:
                print("Cleaned up temporary files")
        except:
            pass
        
        wandb.finish()
        print("W&B run finished")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\nScript failed: {e}", file=sys.stderr)
        sys.exit(1)

