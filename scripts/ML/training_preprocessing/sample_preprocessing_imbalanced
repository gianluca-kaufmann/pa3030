#!/usr/bin/env python3

"""Realistic Ratio Sampling Script (Optimized)

Purpose: Build an evaluation dataset that preserves true class distribution.
Input:  merged_panel_final.parquet (with transition_01 column pre-computed)
Process: Sample transitions on minimal columns, then read full features for sampled rows.
         Sample ~10M rows with realistic class ratio, keep only numeric features.
Output: sample_training_imbalanced.parquet

Key optimization: Work with minimal columns for sampling, then filter full features.
"""

from __future__ import annotations

import os
import sys
import time
from pathlib import Path

import duckdb
import wandb


# Configuration
TARGET_SAMPLE_SIZE = 10_000_000  # Target ~10M rows
RANDOM_STATE = 42


def main() -> None:
    start_time = time.time()
    
    # Get W&B credentials from environment
    wandb_api_key = os.environ.get("WANDB_API_KEY")
    wandb_entity = os.environ.get("WANDB_ENTITY")
    
    if not wandb_api_key:
        print("Warning: WANDB_API_KEY not found in environment")
    if not wandb_entity:
        print("Warning: WANDB_ENTITY not found in environment")
    
    # Initialize W&B
    print("Initializing Weights & Biases...")
    wandb.init(
        project="ml-training-preprocessing",
        entity=wandb_entity,
        name=f"sample_preprocessing_imbalanced_{time.strftime('%Y%m%d_%H%M%S')}",
        config={
            "target_sample_size": TARGET_SAMPLE_SIZE,
            "random_state": RANDOM_STATE,
            "sampling_strategy": "realistic_ratio",
        },
    )
    print("W&B connected")
    
    # Setup paths (outside try block for cleanup in finally)
    repo_root = Path(__file__).resolve().parents[3]
    input_path = repo_root / "data/ml/merged_panel_final.parquet"
    output_path = repo_root / "data/ml/sample_training_imbalanced.parquet"
    temp_dir = repo_root / "data/ml/temp"
    
    try:
        
        temp_dir.mkdir(parents=True, exist_ok=True)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        print(f"\nInput: {input_path}")
        print(f"Output: {output_path}")
        
        if not input_path.exists():
            raise FileNotFoundError(f"Input file not found: {input_path}")
        
        # Initialize DuckDB with memory-efficient settings
        con = duckdb.connect()
        
        # Euler cluster: 128GB nodes, use 48 cores and 100GB memory
        # Local: more conservative 4 cores and 16GB
        is_euler = bool(os.environ.get("SCRATCH"))
        num_threads = 48 if is_euler else 4
        memory_limit_gb = 100 if is_euler else 16
        
        con.execute(f"SET threads={num_threads}")
        con.execute("SET preserve_insertion_order=false")
        con.execute(f"SET memory_limit='{memory_limit_gb}GB'")
        
        # Use SCRATCH temp directory if on Euler, otherwise local temp dir
        temp_dir_env = os.environ.get("SCRATCH") or str(temp_dir)
        # Escape single quotes for SQL literal (handles paths like "Master's Thesis")
        temp_dir_sql = temp_dir_env.replace("'", "''")
        con.execute(f"SET temp_directory='{temp_dir_sql}/duckdb_temp'")
        
        if is_euler:
            con.execute("PRAGMA max_temp_directory_size='200GB'")
        
        print(f"DuckDB configured: {num_threads} threads, memory_limit={memory_limit_gb}GB")
        print(f"Running on: {'Euler cluster' if is_euler else 'local machine'}")
        
        wandb.log({
            "config/threads": num_threads, 
            "config/memory_limit_gb": memory_limit_gb,
            "config/is_euler": is_euler
        })
        
        # Escape paths for SQL
        escaped_in = str(input_path).replace("'", "''")
        escaped_out = str(output_path).replace("'", "''")
        
        # Step 1: Get column information - identify numeric columns
        print("\nStep 1: Identifying numeric columns...")
        step1_start = time.time()
        
        # Get all columns and their types
        schema = con.execute(f"DESCRIBE SELECT * FROM read_parquet('{escaped_in}')").fetchall()
        
        # Always keep these columns
        core_columns = {'row', 'col', 'year', 'transition_01'}
        
        # Identify numeric columns (excluding core columns which we handle separately)
        numeric_types = {'BIGINT', 'INTEGER', 'DOUBLE', 'FLOAT', 'DECIMAL', 'HUGEINT', 'UBIGINT'}
        numeric_columns = []
        
        for col_name, col_type, *_ in schema:
            if col_name in core_columns:
                continue  # Handle these separately
            # Check if column type contains numeric type keywords
            col_type_upper = col_type.upper()
            if any(num_type in col_type_upper for num_type in numeric_types):
                numeric_columns.append(col_name)
        
        print(f"   Found {len(numeric_columns)} numeric feature columns")
        print(f"   Core columns: {', '.join(sorted(core_columns))}")
        
        wandb.log({
            "data/n_numeric_columns": len(numeric_columns),
            "data/n_core_columns": len(core_columns),
        })
        
        step1_time = time.time() - step1_start
        print(f"Completed in {step1_time:.1f}s")
        
        # Step 2: Read minimal columns for transition analysis
        print("\nStep 2: Reading minimal columns (row, col, year, transition_01)...")
        print("   Avoiding full feature set (75GB+) for sampling decision")
        print("   Note: First year has transition_01=0 by design, included in negatives")
        step2_start = time.time()
        
        # Create temporary file with minimal columns only
        # No need to filter by year - transition_01 column already excludes first year's transitions
        temp_minimal = temp_dir / "temp_minimal_imbalanced.parquet"
        escaped_minimal = str(temp_minimal).replace("'", "''")
        
        con.execute(f"""
            COPY (
                SELECT "row", "col", "year", "transition_01"
                FROM read_parquet('{escaped_in}')
            )
            TO '{escaped_minimal}'
            (FORMAT PARQUET, COMPRESSION ZSTD)
        """)
        
        step2_time = time.time() - step2_start
        print(f"Completed in {step2_time:.1f}s ({step2_time/60:.1f} min)")
        wandb.log({"timing/step2_seconds": step2_time, "timing/step2_minutes": step2_time/60})
        
        # Step 3: Count positives and negatives from minimal columns
        print("\nStep 3: Analyzing class distribution from minimal columns...")
        step3_start = time.time()
        
        counts = con.execute(f"""
            SELECT 
                SUM("transition_01") AS positives,
                SUM(CASE WHEN "transition_01" = 0 THEN 1 ELSE 0 END) AS negatives,
                COUNT(*) AS total,
                MIN("year") AS year_min,
                MAX("year") AS year_max
            FROM read_parquet('{escaped_minimal}')
        """).fetchone()
        
        positives, negatives, total, year_min, year_max = counts
        true_positive_ratio = positives / total if total > 0 else 0
        
        print(f"   Total rows (with transition labels): {total:,}")
        print(f"   Positive transitions (0→1):          {positives:,} ({true_positive_ratio*100:.4f}%)")
        print(f"   Negative transitions:                {negatives:,} ({(1-true_positive_ratio)*100:.4f}%)")
        print(f"   Year range:                          {year_min}–{year_max}")
        
        step3_time = time.time() - step3_start
        wandb.log({
            "data/total_rows": total,
            "data/positives": positives,
            "data/negatives": negatives,
            "data/true_positive_ratio": true_positive_ratio,
            "data/year_min": year_min,
            "data/year_max": year_max,
            "timing/step3_seconds": step3_time,
        })
        
        if positives == 0:
            raise ValueError("No positive transitions found! Cannot build training sample.")
        
        # Step 4: Determine sampling strategy (preserve ratio)
        print("\nStep 4: Determining sampling strategy...")
        
        # Cap sample size at available data
        actual_sample_size = min(TARGET_SAMPLE_SIZE, total)
        
        # Calculate samples per class to preserve ratio
        sample_positives = int(actual_sample_size * true_positive_ratio)
        sample_negatives = actual_sample_size - sample_positives
        
        # Ensure we don't sample more than available
        sample_positives = min(sample_positives, positives)
        sample_negatives = min(sample_negatives, negatives)
        total_sample = sample_positives + sample_negatives
        
        print(f"   Target sample size:   {TARGET_SAMPLE_SIZE:,}")
        print(f"   Actual sample size:   {total_sample:,}")
        print(f"   Sampling {sample_positives:,} positives (of {positives:,} available)")
        print(f"   Sampling {sample_negatives:,} negatives (of {negatives:,} available)")
        print(f"   Preserved ratio:      {sample_positives/total_sample*100:.4f}% positive")
        
        wandb.log({
            "sampling/positives": sample_positives,
            "sampling/negatives": sample_negatives,
            "sampling/total": total_sample,
            "sampling/preserved_ratio": sample_positives / total_sample if total_sample > 0 else 0,
        })
        
        # Step 5: Sample on minimal columns to get (row, col, year) keys
        print("\nStep 5: Sampling on minimal columns to identify rows to keep...")
        step5_start = time.time()
        
        # Create temporary file for sampled keys
        temp_sampled_keys = temp_dir / "temp_sampled_keys_imbalanced.parquet"
        escaped_keys = str(temp_sampled_keys).replace("'", "''")
        
        # Sample positives and negatives, keep only keys (row, col, year)
        print(f"   Sampling {sample_positives:,} positives + {sample_negatives:,} negatives (keys only)...")
        con.execute(f"""
            COPY (
                SELECT "row", "col", "year"
                FROM (
                    SELECT "row", "col", "year"
                    FROM read_parquet('{escaped_minimal}')
                    WHERE "transition_01" = 1
                )
                USING SAMPLE reservoir({sample_positives} ROWS) REPEATABLE ({RANDOM_STATE})
                
                UNION ALL
                
                SELECT "row", "col", "year"
                FROM (
                    SELECT "row", "col", "year"
                    FROM read_parquet('{escaped_minimal}')
                    WHERE "transition_01" = 0
                )
                USING SAMPLE reservoir({sample_negatives} ROWS) REPEATABLE ({RANDOM_STATE})
            )
            TO '{escaped_keys}'
            (FORMAT PARQUET, COMPRESSION ZSTD)
        """)
        
        step5_time = time.time() - step5_start
        print(f"Sampled keys in {step5_time:.1f}s")
        wandb.log({"timing/step5_seconds": step5_time})
        
        # Step 6: Join sampled keys with full feature set (numeric columns only)
        print("\nStep 6: Reading numeric features for sampled rows only...")
        print(f"   This reads only ~{total_sample:,} rows from the full feature set")
        step6_start = time.time()
        
        # Build SELECT clause for numeric columns (without table alias prefix)
        core_cols = "row, col, year"
        numeric_select = ", ".join(numeric_columns)
        
        # Use SEMI JOIN pattern with WHERE IN to avoid table alias issues
        con.execute(f"""
            COPY (
                SELECT {core_cols}, {numeric_select}, transition_01
                FROM read_parquet('{escaped_in}')
                WHERE (row, col, year) IN (
                    SELECT row, col, year 
                    FROM read_parquet('{escaped_keys}')
                )
                ORDER BY random()
            )
            TO '{escaped_out}'
            (FORMAT PARQUET, COMPRESSION ZSTD)
        """)
        
        step6_time = time.time() - step6_start
        print(f"Written numeric features to {output_path} in {step6_time:.1f}s ({step6_time/60:.1f} min)")
        wandb.log({"timing/step6_seconds": step6_time, "timing/step6_minutes": step6_time/60})
        
        # Clean up temporary files
        print("\nCleaning up temporary files...")
        temp_minimal.unlink(missing_ok=True)
        temp_sampled_keys.unlink(missing_ok=True)
        print("Temporary files removed")
        
        # Step 7: Verify output
        print("\nStep 7: Verifying output...")
        
        verify_counts = con.execute(f"""
            SELECT 
                SUM("transition_01") AS pos_written,
                SUM(CASE WHEN "transition_01" = 0 THEN 1 ELSE 0 END) AS neg_written,
                COUNT(*) AS total_written,
                COUNT(DISTINCT "year") AS n_years,
                MIN("year") AS min_year,
                MAX("year") AS max_year
            FROM read_parquet('{escaped_out}')
        """).fetchone()
        
        pos_written, neg_written, total_written, n_years, min_year, max_year = verify_counts
        file_size_mb = output_path.stat().st_size / (1024 * 1024)
        total_time = time.time() - start_time
        
        final_ratio = pos_written / total_written if total_written > 0 else 0
        
        # Print summary
        print("\n" + "="*70)
        print("REALISTIC RATIO SAMPLING COMPLETE")
        print("="*70)
        print(f"  Total rows:          {total_written:,}")
        print(f"  Positives (0→1):     {pos_written:,} ({final_ratio*100:.4f}%)")
        print(f"  Negatives:           {neg_written:,} ({(1-final_ratio)*100:.4f}%)")
        print(f"  True ratio:          {true_positive_ratio*100:.4f}%")
        print(f"  Ratio preserved:     {abs(final_ratio - true_positive_ratio) < 0.0001}")
        print(f"  Years covered:       {n_years} years ({min_year}–{max_year})")
        print(f"  Numeric features:    {len(numeric_columns)}")
        print(f"  File size:           {file_size_mb:.1f} MB")
        print(f"  Total time:          {total_time:.1f}s ({total_time/60:.1f} min)")
        print("="*70)
        
        # Log final results to W&B
        wandb.log({
            "output/total_rows": total_written,
            "output/positives": pos_written,
            "output/negatives": neg_written,
            "output/final_ratio": final_ratio,
            "output/true_ratio": true_positive_ratio,
            "output/ratio_preserved": abs(final_ratio - true_positive_ratio) < 0.0001,
            "output/n_years": n_years,
            "output/year_min": min_year,
            "output/year_max": max_year,
            "output/n_numeric_features": len(numeric_columns),
            "output/file_size_mb": file_size_mb,
            "timing/total_seconds": total_time,
            "timing/total_minutes": total_time / 60,
            "status": "success",
        })
        
        print("\nAll done! Evaluation dataset ready for testing.")
        
    except Exception as e:
        error_msg = f"{type(e).__name__}: {str(e)}"
        print(f"\nERROR: {error_msg}")
        wandb.log({
            "status": "failed",
            "error_type": type(e).__name__,
            "error_message": str(e),
        })
        raise
    
    finally:
        # Clean up temporary files in case of error
        try:
            cleanup_files = [
                temp_dir / "temp_minimal_imbalanced.parquet",
                temp_dir / "temp_sampled_keys_imbalanced.parquet"
            ]
            cleaned = False
            for f in cleanup_files:
                if f.exists():
                    f.unlink()
                    cleaned = True
            if cleaned:
                print("Cleaned up temporary files")
        except:
            pass
        
        wandb.finish()
        print("W&B run finished")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\nScript failed: {e}", file=sys.stderr)
        sys.exit(1)

