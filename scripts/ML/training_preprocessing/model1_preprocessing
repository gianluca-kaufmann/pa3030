#!/usr/bin/env python3

"""
Sampling script for model1 training/validation/test splits.

Rules
-----
TRAIN_EARLY (2001–2014):
  - Keep all positives (transition_01 = 1)
  - Sample hard negatives at 1:100 ratio per year, weighted by exp(-dist_wdpa / k)
  - Year 2000 excluded to avoid treating existing PAs as transitions

VAL_LATE (2015–2017):
  - Full risk set (no sampling)

TEST (2018–2024):
  - Full risk set (no sampling)
"""

from __future__ import annotations

import os
import sys
import time
from pathlib import Path

import duckdb
import wandb

# Configuration
RANDOM_STATE = 42
# Split configuration
TRAIN_EARLY_YEARS = (2001, 2014)   # Sampled train with hard negatives (2000 excluded)
VAL_LATE_YEARS = (2015, 2017)      # Full validation
TEST_YEARS = (2018, 2024)          # Full test
WANDB_PROJECT = "ml-training-preprocessing"

# Hard negative sampling configuration
NEG_POS_RATIO = 100  # Target ratio of negatives to positives (exact per-year sampling)
DIST_WDPA_DECAY = 5.0  # Decay constant (km) for exponential weighting: exp(-dist_wdpa / k)
MAX_TRAIN_ROWS = 30_000_000  # Hard cap on TRAIN_EARLY total rows (never caps positives)


def resolve_input() -> Path:
    """Locate merged_panel_final.parquet (prefer $SCRATCH if present)."""
    repo_root = Path(__file__).resolve().parents[3]
    scratch_root = Path(os.environ["SCRATCH"]) if os.environ.get("SCRATCH") else None

    candidates = []
    if scratch_root is not None:
        candidates.append(scratch_root / "data/ml/merged_panel_final.parquet")
    candidates.append(repo_root / "data/ml/merged_panel_final.parquet")

    for cand in candidates:
        if cand.exists():
            return cand

    raise FileNotFoundError("merged_panel_final.parquet not found in expected locations")


def configure_duckdb(con: duckdb.DuckDBPyConnection) -> None:
    """Apply sensible local/Euler defaults."""
    is_euler = bool(os.environ.get("SCRATCH"))
    slurm_cpus = int(os.environ.get("SLURM_CPUS_PER_TASK", "0"))
    num_threads = slurm_cpus if slurm_cpus > 0 else (48 if is_euler else 4)

    # Match DuckDB memory to Slurm allocation when present
    slurm_mem_per_cpu_mb = os.environ.get("SLURM_MEM_PER_CPU")
    if slurm_mem_per_cpu_mb and slurm_cpus:
        total_mem_mb = int(slurm_mem_per_cpu_mb) * slurm_cpus
        memory_limit_gb = max(1, total_mem_mb // 1024)
    else:
        memory_limit_gb = 128 if is_euler else 16

    con.execute(f"SET threads={num_threads}")
    con.execute("SET preserve_insertion_order=false")
    con.execute(f"SET memory_limit='{memory_limit_gb}GB'")

    temp_dir = os.environ.get("SCRATCH") or (Path(__file__).parent / "temp")
    temp_dir_sql = str(temp_dir).replace("'", "''")
    con.execute(f"SET temp_directory='{temp_dir_sql}/duckdb_temp'")

    if is_euler:
        con.execute("PRAGMA max_temp_directory_size='200GB'")

    print(f"DuckDB configured: {num_threads} threads, {memory_limit_gb}GB memory")
    print(f"Running on: {'Euler cluster' if is_euler else 'local machine'}")


def log_year_counts(con: duckdb.DuckDBPyConnection, parquet_path: Path, label: str, use_wandb: bool = False) -> None:
    escaped = str(parquet_path).replace("'", "''")
    rows = con.execute(
        f"""
        SELECT
            year,
            SUM(transition_01) AS positives,
            SUM(CASE WHEN transition_01 = 0 THEN 1 ELSE 0 END) AS negatives,
            COUNT(*) AS total,
            CASE WHEN COUNT(*) > 0 THEN SUM(transition_01)::DOUBLE / COUNT(*) ELSE 0 END AS pos_ratio,
            CASE WHEN SUM(transition_01) > 0 
                 THEN SUM(CASE WHEN transition_01 = 0 THEN 1 ELSE 0 END)::DOUBLE / SUM(transition_01)
                 ELSE 0 END AS neg_pos_ratio
        FROM read_parquet('{escaped}')
        GROUP BY year
        ORDER BY year
        """
    ).fetchall()

    print(f"\n{label} per-year counts:")
    for year, pos, neg, total, pos_ratio, neg_pos_ratio in rows:
        print(f"  {year}: total={total:,} pos={pos:,} neg={neg:,} | pos_ratio={pos_ratio:.5f} neg/pos={neg_pos_ratio:.1f}")

    if use_wandb:
        wandb.log({f"{label.lower().replace(' ', '_')}/per_year": [
            dict(year=y, positives=p, negatives=n, total=t, pos_ratio=r, neg_pos_ratio=npr) 
            for y, p, n, t, r, npr in rows
        ]})


def main() -> None:
    start_time = time.time()

    wandb_api_key = os.environ.get("WANDB_API_KEY")
    wandb_entity = os.environ.get("WANDB_ENTITY")
    if not wandb_api_key:
        print("Warning: WANDB_API_KEY not found in environment")
    if not wandb_entity:
        print("Warning: WANDB_ENTITY not found in environment (will use default)")
    
    use_wandb = False
    try:
        print("Initializing Weights & Biases...")
        wandb.init(
            project=WANDB_PROJECT,
            entity=wandb_entity,
            name=f"model1_preprocessing_{time.strftime('%Y%m%d_%H%M%S')}",
            config={
                "random_state": RANDOM_STATE,
                "train_early_years": TRAIN_EARLY_YEARS,
                "val_late_years": VAL_LATE_YEARS,
                "test_years": TEST_YEARS,
                "neg_pos_ratio": NEG_POS_RATIO,
                "dist_wdpa_decay": DIST_WDPA_DECAY,
                "max_train_rows": MAX_TRAIN_ROWS,
                "sampling_method": "hard_negatives_exponential_capped",
            },
        )
        use_wandb = True
        print("W&B connected\n")
    except Exception as err:
        print(f"W&B initialization failed: {err}\n")

    input_path = resolve_input()
    base_dir = input_path.parent
    train_early_out = base_dir / "train_early_sampled.parquet"
    val_late_out = base_dir / "val_late_full.parquet"
    test_out = base_dir / "test_full.parquet"

    print(f"\nInput: {input_path}")
    print(f"TRAIN_EARLY (sampled) output: {train_early_out}")
    print(f"VAL_LATE_FULL output: {val_late_out}")
    print(f"TEST_FULL output: {test_out}")

    con = duckdb.connect()
    configure_duckdb(con)

    escaped_in = str(input_path).replace("'", "''")
    escaped_train_early = str(train_early_out).replace("'", "''")
    escaped_test = str(test_out).replace("'", "''")
    escaped_val_late = str(val_late_out).replace("'", "''")

    try:
        # TRAIN_EARLY: positives + hard negatives per year (2001–2014)
        # Hard negatives sampled exactly at 1:NEG_POS_RATIO per year, weighted by exp(-dist_wdpa / k)
        # Smart capping: never cap positives, adjust neg/pos ratio if needed to stay under MAX_TRAIN_ROWS
        train_early_sql = f"""
        COPY (
            WITH base AS (
                SELECT * FROM read_parquet('{escaped_in}')
                WHERE year BETWEEN {TRAIN_EARLY_YEARS[0]} AND {TRAIN_EARLY_YEARS[1]}
            ),
            year_counts AS (
                SELECT
                    year,
                    SUM(transition_01) AS pos_count,
                    SUM(CASE WHEN transition_01 = 0 THEN 1 ELSE 0 END) AS neg_count
                FROM base
                GROUP BY year
            ),
            -- Compute global positive count for capping logic
            global_stats AS (
                SELECT
                    SUM(pos_count) AS total_pos,
                    SUM(neg_count) AS total_neg
                FROM year_counts
            ),
            -- Compute per-year negative budget with smart capping
            neg_budget AS (
                SELECT
                    yc.year,
                    yc.pos_count,
                    yc.neg_count,
                    -- Maximum allowed negatives globally
                    LEAST(
                        gs.total_pos * {NEG_POS_RATIO},
                        {MAX_TRAIN_ROWS} - gs.total_pos
                    ) AS max_total_negs,
                    -- Per-year budget proportional to year's positive count
                    CAST(
                        yc.pos_count::DOUBLE / NULLIF(gs.total_pos, 0) * 
                        LEAST(
                            gs.total_pos * {NEG_POS_RATIO},
                            {MAX_TRAIN_ROWS} - gs.total_pos
                        ) AS BIGINT
                    ) AS year_neg_budget_raw,
                    -- Final per-year negative budget cannot exceed available negatives
                    LEAST(
                        yc.neg_count,
                        CAST(
                            yc.pos_count::DOUBLE / NULLIF(gs.total_pos, 0) * 
                            LEAST(
                                gs.total_pos * {NEG_POS_RATIO},
                                {MAX_TRAIN_ROWS} - gs.total_pos
                            ) AS BIGINT
                        )
                    ) AS year_neg_budget
                FROM year_counts yc
                CROSS JOIN global_stats gs
            ),
            positives AS (
                SELECT * FROM base WHERE transition_01 = 1
            ),
            -- Prepare negative pool with per-year budgets and a stable random key
            neg_candidates AS (
                SELECT
                    b.*,
                    nb.year_neg_budget,
                    nb.neg_count,
                    -- Split the yearly negative budget into 50% hard and 50% random
                    CAST(FLOOR(nb.year_neg_budget / 2.0) AS BIGINT) AS hard_budget,
                    nb.year_neg_budget - CAST(FLOOR(nb.year_neg_budget / 2.0) AS BIGINT) AS rand_budget,
                    COALESCE(b.dist_wdpa, 100.0) AS dist_for_rank,
                    random() AS u
                FROM base b
                LEFT JOIN neg_budget nb ON b.year = nb.year
                WHERE b.transition_01 = 0
                  AND nb.year_neg_budget > 0
            ),
            -- Rank for hard negatives: by distance to WDPA (closest first), tie-broken by u
            ranked_for_hard AS (
                SELECT
                    *,
                    ROW_NUMBER() OVER (
                        PARTITION BY year
                        ORDER BY dist_for_rank ASC, u
                    ) AS dist_rank
                FROM neg_candidates
            ),
            -- First half of the budget: hard negatives close to WDPA
            hard_negs AS (
                SELECT * EXCLUDE (
                    year_neg_budget,
                    neg_count,
                    hard_budget,
                    rand_budget,
                    dist_for_rank,
                    u,
                    dist_rank
                )
                FROM ranked_for_hard
                WHERE dist_rank <= hard_budget
                  AND hard_budget > 0
            ),
            -- Serialized selection:
            --   1) hard_negs as above (distance-ranked)
            --   2) leftover_pool = negatives that were NOT selected as hard_negs
            --   3) re-rank randomly only within leftover_pool to fill the random budget exactly
            leftover_pool AS (
                SELECT *
                FROM ranked_for_hard
                WHERE dist_rank > hard_budget
                  AND rand_budget > 0
            ),
            ranked_leftover AS (
                SELECT
                    *,
                    ROW_NUMBER() OVER (
                        PARTITION BY year
                        ORDER BY u
                    ) AS leftover_rand_rank
                FROM leftover_pool
            ),
            -- Second half of the budget: random negatives drawn only from leftover_pool
            random_negs AS (
                SELECT * EXCLUDE (
                    year_neg_budget,
                    neg_count,
                    hard_budget,
                    rand_budget,
                    dist_for_rank,
                    u,
                    dist_rank,
                    leftover_rand_rank
                )
                FROM ranked_leftover
                WHERE leftover_rand_rank <= rand_budget
            )
            SELECT * FROM positives
            UNION ALL
            SELECT * FROM hard_negs
            UNION ALL
            SELECT * FROM random_negs
        )
        TO '{escaped_train_early}' (FORMAT PARQUET, COMPRESSION ZSTD)
        """

        print(f"\nCreating TRAIN_EARLY split ({TRAIN_EARLY_YEARS[0]}–{TRAIN_EARLY_YEARS[1]}) with hard negatives...")
        print(f"  Target neg/pos ratio: {NEG_POS_RATIO}:1 per year")
        print(f"  Max total rows: {MAX_TRAIN_ROWS:,} (smart capping: never caps positives)")
        print(f"  Using exponential weighting: exp(-dist_wdpa / {DIST_WDPA_DECAY})")
        print(f"  Year 2000 excluded to avoid treating existing PAs as transitions")
        # Set random seed for reproducible sampling
        seed_value = RANDOM_STATE / 100.0
        con.execute(f"SELECT setseed({seed_value})")
        train_early_start = time.time()
        con.execute(train_early_sql)
        print(f"TRAIN_EARLY written in {time.time() - train_early_start:.1f}s")

        # VAL_LATE_FULL: full risk set 2015–2017
        val_late_sql = f"""
        COPY (
            SELECT * FROM read_parquet('{escaped_in}')
            WHERE year BETWEEN {VAL_LATE_YEARS[0]} AND {VAL_LATE_YEARS[1]}
        )
        TO '{escaped_val_late}' (FORMAT PARQUET, COMPRESSION ZSTD)
        """

        print(f"\nCreating VAL_LATE_FULL split ({VAL_LATE_YEARS[0]}–{VAL_LATE_YEARS[1]}, full risk set)...")
        val_late_start = time.time()
        con.execute(val_late_sql)
        print(f"VAL_LATE_FULL written in {time.time() - val_late_start:.1f}s")

        # TEST_FULL: full risk set 2018–2024
        test_sql = f"""
        COPY (
            SELECT * FROM read_parquet('{escaped_in}')
            WHERE year BETWEEN {TEST_YEARS[0]} AND {TEST_YEARS[1]}
        )
        TO '{escaped_test}' (FORMAT PARQUET, COMPRESSION ZSTD)
        """

        print(f"\nCreating TEST_FULL split ({TEST_YEARS[0]}–{TEST_YEARS[1]}, full risk set)...")
        test_start = time.time()
        con.execute(test_sql)
        print(f"TEST written in {time.time() - test_start:.1f}s")

        # Log counts and check achieved ratios
        log_year_counts(con, train_early_out, "TRAIN_EARLY", use_wandb)
        log_year_counts(con, val_late_out, "VAL_LATE_FULL", use_wandb)
        log_year_counts(con, test_out, "TEST_FULL", use_wandb)

        # Check total rows and capping status
        total_train_early_rows = con.execute(
            f"SELECT COUNT(*) FROM read_parquet('{escaped_train_early}')"
        ).fetchone()[0]
        
        capping_applied = total_train_early_rows >= MAX_TRAIN_ROWS * 0.95  # Within 5% threshold
        print(f"\nTotal TRAIN_EARLY rows: {total_train_early_rows:,} / {MAX_TRAIN_ROWS:,}")
        if capping_applied:
            print("  ⚠ Row limit active - negative sampling was capped")
        else:
            print("  ✓ Below row limit - full target ratio achieved")
        
        # Check for years where target ratio was infeasible
        print("\nChecking achieved neg/pos ratios vs target...")
        achieved_ratios = con.execute(
            f"""
            SELECT
                year,
                SUM(transition_01) AS pos_count,
                SUM(CASE WHEN transition_01 = 0 THEN 1 ELSE 0 END) AS neg_count,
                CASE WHEN SUM(transition_01) > 0 
                     THEN SUM(CASE WHEN transition_01 = 0 THEN 1 ELSE 0 END)::DOUBLE / SUM(transition_01)
                     ELSE 0 END AS achieved_ratio
            FROM read_parquet('{escaped_train_early}')
            GROUP BY year
            ORDER BY year
            """
        ).fetchall()
        
        for year, pos, neg, achieved in achieved_ratios:
            status = "✓" if abs(achieved - NEG_POS_RATIO) < 0.1 else "⚠"
            print(f"  {status} {year}: achieved {achieved:.1f}:1 (target {NEG_POS_RATIO}:1) | pos={pos:,} neg={neg:,}")
        
        # Compute scale_pos_weight from TRAIN_EARLY only
        train_stats = con.execute(
            f"""
            SELECT
                SUM(transition_01) AS total_pos,
                SUM(CASE WHEN transition_01 = 0 THEN 1 ELSE 0 END) AS total_neg
            FROM read_parquet('{escaped_train_early}')
            """
        ).fetchone()
        total_pos, total_neg = train_stats
        scale_pos_weight = total_neg / total_pos if total_pos > 0 else 1.0
        
        print(f"\nTRAIN_EARLY class balance:")
        print(f"  Positives: {total_pos:,}")
        print(f"  Negatives: {total_neg:,}")
        print(f"  Ratio (neg/pos): {scale_pos_weight:.2f}")
        print(f"  scale_pos_weight for LightGBM: {scale_pos_weight:.2f}")

        total_test = con.execute(
            f"SELECT COUNT(*) FROM read_parquet('{escaped_test}')"
        ).fetchone()[0]

        if use_wandb:
            wandb.log(
                {
                    "train_early/rows": total_train_early_rows,
                    "train_early/positives": total_pos,
                    "train_early/negatives": total_neg,
                    "train_early/scale_pos_weight": scale_pos_weight,
                    "train_early/capping_applied": capping_applied,
                    "train_early/max_rows": MAX_TRAIN_ROWS,
                    "val_late_full/rows": con.execute(
                        f"SELECT COUNT(*) FROM read_parquet('{escaped_val_late}')"
                    ).fetchone()[0],
                    "test_full/rows": total_test,
                    "timing/total_seconds": time.time() - start_time,
                }
            )

        print("\nDone.")

    except Exception as e:
        error_msg = f"{type(e).__name__}: {e}"
        print(f"\nERROR: {error_msg}")
        if use_wandb:
            wandb.log({"status": "failed", "error": error_msg})
        raise
    finally:
        if use_wandb:
            wandb.finish()


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\nScript failed: {e}", file=sys.stderr)
        sys.exit(1)
