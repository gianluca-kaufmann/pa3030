#!/usr/bin/env python3

"""Spatial Features Preprocessing Script

Purpose: Add spatial and transition features to the merged panel data with Risk Set filtering:
    1. Lag feature: WDPA_prev (1-year lag of WDPA_b1, default 0 for first year)
    2. Risk Set filter: Keep only rows where WDPA_prev == 0 (pixels not previously protected)
    3. Target variable: transition_01 = WDPA_b1 (simplified since WDPA_prev guaranteed to be 0)
    4. Distance features: distance to nearest WDPA protected area pixel
    5. Decay features: exp(-distance / 10000)
    6. Smoothed features: 4x4, 16x16, 64x64 mean for specified variables

Input:  data/ml/merged_panel_2000_2024.parquet
Output: data/ml/merged_panel_final.parquet (Risk Set only, with transition_01 column)

Processing: Year-by-year for memory efficiency with 500M+ rows.
Risk Set filtering significantly reduces output size by excluding already-protected pixels.
"""

from __future__ import annotations

import gc
import logging
import os
import time
import warnings
from pathlib import Path
from typing import Tuple

import numpy as np
import pandas as pd
import pyarrow.parquet as pq
import wandb
from scipy.ndimage import distance_transform_edt, uniform_filter

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')

DISTANCE_DECAY_SCALE = 10000
SMOOTH_VARIABLES = ['NDVI_b1', 'GPW_b1', 'HNTL_b1', 'gdp_b1', 'deforestation_b1', 
                     'wildfire_b1', 'GSN_b1', 'GSN_b2', 'GSN_b3', 'GSN_b4', 'GSN_b5']
SMOOTH_SIZES = [4, 16, 64]


def reconstruct_grid(df: pd.DataFrame, value_col: str) -> Tuple[np.ndarray, int, int, int, int]:
    """Reconstruct 2D grid from panel data."""
    min_row, max_row = df['row'].min(), df['row'].max()
    min_col, max_col = df['col'].min(), df['col'].max()
    
    grid = np.full((max_row - min_row + 1, max_col - min_col + 1), np.nan, dtype=np.float32)
    grid[(df['row'] - min_row).values, (df['col'] - min_col).values] = df[value_col].values
    
    return grid, min_row, max_row, min_col, max_col


def grid_to_dataframe(grid: np.ndarray, df: pd.DataFrame, min_row: int, min_col: int) -> pd.Series:
    """Convert grid back to DataFrame format."""
    values = grid[(df['row'] - min_row).values, (df['col'] - min_col).values]
    return pd.Series(values, index=df.index)


def compute_distance_features(df: pd.DataFrame) -> pd.DataFrame:
    """Compute distance to WDPA and decay features."""
    logger.info("  Computing distance features...")
    
    wdpa_grid, min_row, max_row, min_col, max_col = reconstruct_grid(df, 'WDPA_b1')
    wdpa_binary = np.where(np.isnan(wdpa_grid), 0, wdpa_grid > 0).astype(np.uint8)
    del wdpa_grid
    
    distance_grid = distance_transform_edt(1 - wdpa_binary) * 1000  # pixels to meters
    del wdpa_binary
    
    df['dist_wdpa'] = grid_to_dataframe(distance_grid, df, min_row, min_col)
    df['dist_wdpa_decay'] = np.exp(-df['dist_wdpa'] / DISTANCE_DECAY_SCALE)
    del distance_grid
    
    return df


def compute_smooth_feature(df: pd.DataFrame, variable: str, window_size: int) -> pd.Series:
    """Compute smoothed feature with given window size."""
    grid, min_row, max_row, min_col, max_col = reconstruct_grid(df, variable)
    valid_mask = ~np.isnan(grid)
    grid_filled = np.where(np.isnan(grid), 0, grid)
    del grid
    
    smoothed = uniform_filter(grid_filled, size=window_size, mode='constant')
    del grid_filled
    
    count = uniform_filter(valid_mask.astype(np.float32), size=window_size, mode='constant')
    del valid_mask
    
    smoothed = np.where(count > 0, smoothed, np.nan)
    del count
    
    result = grid_to_dataframe(smoothed, df, min_row, min_col)
    del smoothed
    
    return result


def compute_all_smooth_features(df: pd.DataFrame) -> pd.DataFrame:
    """Compute all smoothed features."""
    logger.info("  Computing smoothed features...")
    
    for variable in SMOOTH_VARIABLES:
        if variable not in df.columns:
            continue
        for window_size in SMOOTH_SIZES:
            df[f"{variable}_smooth{window_size}"] = compute_smooth_feature(df, variable, window_size)
            gc.collect()
    
    return df


def compute_lag_feature(input_path: Path, year: int) -> pd.DataFrame:
    """Compute WDPA_prev (1-year lag) for a given year by joining with previous year."""
    logger.info(f"  Computing lag feature (WDPA_prev) for year {year}...")
    
    # Read minimal columns for current year
    df_curr = pd.read_parquet(
        input_path, 
        columns=['row', 'col', 'year'],
        filters=[('year', '==', year)]
    )
    
    # Read minimal columns for previous year
    df_prev = pd.read_parquet(
        input_path,
        columns=['row', 'col', 'year', 'WDPA_b1'],
        filters=[('year', '==', year - 1)]
    )
    
    # Rename for join
    df_prev = df_prev.rename(columns={'WDPA_b1': 'WDPA_prev'})
    df_prev = df_prev.drop(columns=['year'])
    
    # Merge on spatial coordinates
    df_merged = df_curr.merge(df_prev, on=['row', 'col'], how='left')
    
    # Fill missing with 0 (pixels that didn't exist in previous year)
    df_merged['WDPA_prev'] = df_merged['WDPA_prev'].fillna(0)
    
    # Keep only the new column
    result = df_merged[['row', 'col', 'WDPA_prev']].copy()
    
    del df_curr, df_prev, df_merged
    gc.collect()
    
    return result


def process_year(input_path: Path, year: int, temp_dir: Path, is_first_year: bool) -> tuple[Path | None, dict]:
    """Process a single year and return temp file path + metrics."""
    start_time = time.time()
    logger.info(f"\nProcessing year {year}...")
    
    df_year = pd.read_parquet(input_path, filters=[('year', '==', year)], engine='pyarrow')
    n_rows_initial = len(df_year)
    logger.info(f"  Loaded {n_rows_initial:,} rows")
    
    if n_rows_initial == 0:
        return None, {}
    
    # Step 1: Add WDPA_prev (1-year lag)
    if not is_first_year:
        df_lag = compute_lag_feature(input_path, year)
        df_year = df_year.merge(df_lag, on=['row', 'col'], how='left')
        df_year['WDPA_prev'] = df_year['WDPA_prev'].fillna(0)  # Safety fallback
        del df_lag
        gc.collect()
    else:
        logger.info(f"  First year {year}: setting WDPA_prev = 0")
        df_year['WDPA_prev'] = 0
    
    # Step 2: Apply Risk Set filter (keep only WDPA_prev == 0)
    df_year = df_year[df_year['WDPA_prev'] == 0].copy()
    n_rows_filtered = len(df_year)
    logger.info(f"  Risk Set filter: {n_rows_filtered:,} rows (kept {100*n_rows_filtered/n_rows_initial:.1f}%)")
    
    if n_rows_filtered == 0:
        logger.warning(f"  No rows remaining after Risk Set filter for year {year}")
        return None, {}
    
    # Step 3: Create transition_01 = WDPA_b1 (since WDPA_prev guaranteed to be 0)
    df_year['transition_01'] = df_year['WDPA_b1'].astype(np.uint8)
    
    # Step 4: Compute spatial features
    df_year = compute_distance_features(df_year)
    df_year = compute_all_smooth_features(df_year)
    
    temp_file = temp_dir / f"year_{year}_processed.parquet"
    df_year.to_parquet(temp_file, engine='pyarrow', compression='snappy', index=False)
    
    del df_year
    gc.collect()
    
    elapsed = time.time() - start_time
    logger.info(f"  ✓ Year {year} complete in {elapsed:.1f}s")
    
    return temp_file, {
        "year": year, 
        "rows_initial": n_rows_initial,
        "rows_filtered": n_rows_filtered,
        "filter_ratio": n_rows_filtered / n_rows_initial,
        "time_seconds": elapsed
    }


def main() -> None:
    """Main processing function."""
    repo_root = Path(__file__).resolve().parents[3]
    scratch_root = Path(os.environ["SCRATCH"]) if "SCRATCH" in os.environ else None

    # Resolve input path with sensible defaults:
    #  1) Explicit override via env var SPATIAL_FEATURES_INPUT_PATH
    #  2) Common Euler location on $SCRATCH
    #  3) Fallback to local repo path (original behaviour)
    env_input = os.environ.get("SPATIAL_FEATURES_INPUT_PATH")
    if env_input:
        input_path = Path(env_input)
    else:
        candidate_paths = []
        if scratch_root is not None:
            candidate_paths.append(
                scratch_root / "outputs/Results/merged_panel_2000_2024.parquet"
            )
            candidate_paths.append(
                scratch_root / "data/ml/merged_panel_2000_2024.parquet"
            )
        candidate_paths.append(
            repo_root / "data/ml/merged_panel_2000_2024.parquet"
        )

        # Default to the last candidate (original local path) if none exist
        input_path = candidate_paths[-1]
        for cand in candidate_paths:
            if cand.exists():
                input_path = cand
                break

    # On clusters, write large outputs to $SCRATCH; locally keep them under the repo
    if scratch_root is not None:
        base_ml_dir = scratch_root / "data/ml"
    else:
        base_ml_dir = repo_root / "data/ml"

    output_path = base_ml_dir / "merged_panel_final.parquet"
    temp_dir = base_ml_dir / ".temp_spatial_features"
    
    # Initialize W&B
    wandb.init(
        project="thesis-spatial-features",
        name="spatial_features_preprocessing",
        config={
            "distance_decay_scale": DISTANCE_DECAY_SCALE,
            "smooth_variables": SMOOTH_VARIABLES,
            "smooth_sizes": SMOOTH_SIZES,
        }
    )
    
    logger.info("="*80)
    logger.info("SPATIAL FEATURES PREPROCESSING")
    logger.info("="*80)
    logger.info(f"Input:  {input_path}")
    logger.info(f"Output: {output_path}")
    
    if not input_path.exists():
        raise FileNotFoundError(f"Input file not found: {input_path}")
    
    temp_dir.mkdir(parents=True, exist_ok=True)
    
    # Get metadata
    logger.info("Reading metadata...")
    df_sample = pd.read_parquet(input_path, columns=['year'])
    years = sorted(df_sample['year'].unique())
    total_rows = len(df_sample)
    logger.info(f"Years: {years[0]}-{years[-1]} ({len(years)} years), Total rows: {total_rows:,}")
    del df_sample
    gc.collect()
    
    wandb.log({"total_years": len(years), "total_rows": total_rows})
    
    # Process year by year
    temp_files = []
    total_rows_initial = 0
    total_rows_filtered = 0
    
    for i, year in enumerate(years):
        try:
            is_first_year = (i == 0)
            temp_file, metrics = process_year(input_path, year, temp_dir, is_first_year)
            if temp_file is not None:
                temp_files.append(temp_file)
                wandb.log(metrics)
                if 'rows_initial' in metrics:
                    total_rows_initial += metrics['rows_initial']
                    total_rows_filtered += metrics['rows_filtered']
        except Exception as e:
            logger.error(f"Error processing year {year}: {e}")
            for tf in temp_files:
                if tf.exists():
                    tf.unlink()
            if temp_dir.exists() and not list(temp_dir.iterdir()):
                temp_dir.rmdir()
            wandb.finish(exit_code=1)
            raise
    
    # Combine all years using streaming
    logger.info("\n" + "="*80)
    logger.info(f"Combining {len(temp_files)} files (streaming)...")
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    writer = None
    total_rows_written = 0
    concat_start = time.time()
    
    for i, temp_file in enumerate(temp_files):
        logger.info(f"  [{i+1}/{len(temp_files)}] {temp_file.name}...")
        table = pq.read_table(temp_file)
        total_rows_written += len(table)
        
        if writer is None:
            logger.info(f"  Initializing output ({len(table.column_names)} columns)...")
            writer = pq.ParquetWriter(output_path, table.schema, compression='snappy')
        
        writer.write_table(table)
        del table
        gc.collect()
        
        wandb.log({"concatenation_progress": (i + 1) / len(temp_files)})
    
    if writer is None:
        raise RuntimeError("No data was written!")
    
    writer.close()
    concat_time = time.time() - concat_start
    logger.info(f"  ✓ Wrote {total_rows_written:,} rows in {concat_time:.1f}s")
    
    # Clean up temp files
    logger.info("\nCleaning up temp files...")
    for temp_file in temp_files:
        temp_file.unlink()
    if temp_dir.exists() and not list(temp_dir.iterdir()):
        temp_dir.rmdir()
    
    # Summary
    file_size_mb = output_path.stat().st_size / (1024**2)
    schema = pq.read_schema(output_path)
    new_cols = [c for c in schema.names if c.startswith('dist_') or '_smooth' in c]
    filter_ratio = total_rows_filtered / total_rows_initial if total_rows_initial > 0 else 0
    
    logger.info("\n" + "="*80)
    logger.info("SUMMARY")
    logger.info("="*80)
    logger.info(f"Initial rows:   {total_rows_initial:,}")
    logger.info(f"Filtered rows:  {total_rows_filtered:,} ({100*filter_ratio:.1f}% kept)")
    logger.info(f"Risk Set filter removed: {total_rows_initial - total_rows_filtered:,} already-protected pixels")
    logger.info(f"New columns: {len(new_cols)}")
    logger.info(f"File size: {file_size_mb:.1f} MB")
    logger.info("✓ Processing complete!")
    
    wandb.log({
        "initial_rows": total_rows_initial,
        "final_rows": total_rows_filtered,
        "filter_ratio": filter_ratio,
        "rows_removed": total_rows_initial - total_rows_filtered,
        "new_columns": len(new_cols),
        "file_size_mb": file_size_mb,
        "concatenation_time_seconds": concat_time
    })
    wandb.finish()


if __name__ == "__main__":
    main()

