#!/usr/bin/env python3

"""Spatial Features Preprocessing Script

Purpose: Add spatial and transition features to the merged panel data:
    1. Transition features: WDPA_prev and transition_01 (0→1 transitions)
       - For each year Y, join with year Y-1 to get previous WDPA status
       - Compute transition_01 = (WDPA_prev==0 & WDPA_b1==1)
       - Uses minimal columns (row, col, year, WDPA_b1) for memory efficiency
    2. Distance features: distance to nearest WDPA protected area pixel
    3. Decay features: exp(-distance / 10000)
    4. Smoothed features: 4x4, 16x16, 64x64 mean for specified variables

Input:  data/ml/merged_panel_2000_2024.parquet
Output: data/ml/merged_panel_final.parquet (with transition_01 column)

Processing: Year-by-year for memory efficiency with 500M+ rows.
Transition detection is done here (not in sampling scripts) to avoid heavy
joins on the full 75GB+ feature set later.
"""

from __future__ import annotations

import gc
import logging
import time
import warnings
from pathlib import Path
from typing import Tuple

import numpy as np
import pandas as pd
import pyarrow.parquet as pq
import wandb
from scipy.ndimage import distance_transform_edt, uniform_filter

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')

DISTANCE_DECAY_SCALE = 10000
SMOOTH_VARIABLES = ['NDVI_b1', 'GPW_b1', 'VIIRS_b1', 'gdp_b1', 'deforestation_b1', 
                     'wildfire_b1', 'GSN_b1', 'GSN_b2', 'GSN_b3', 'GSN_b4', 'GSN_b5']
SMOOTH_SIZES = [4, 16, 64]


def reconstruct_grid(df: pd.DataFrame, value_col: str) -> Tuple[np.ndarray, int, int, int, int]:
    """Reconstruct 2D grid from panel data."""
    min_row, max_row = df['row'].min(), df['row'].max()
    min_col, max_col = df['col'].min(), df['col'].max()
    
    grid = np.full((max_row - min_row + 1, max_col - min_col + 1), np.nan, dtype=np.float32)
    grid[(df['row'] - min_row).values, (df['col'] - min_col).values] = df[value_col].values
    
    return grid, min_row, max_row, min_col, max_col


def grid_to_dataframe(grid: np.ndarray, df: pd.DataFrame, min_row: int, min_col: int) -> pd.Series:
    """Convert grid back to DataFrame format."""
    values = grid[(df['row'] - min_row).values, (df['col'] - min_col).values]
    return pd.Series(values, index=df.index)


def compute_distance_features(df: pd.DataFrame) -> pd.DataFrame:
    """Compute distance to WDPA and decay features."""
    logger.info("  Computing distance features...")
    
    wdpa_grid, min_row, max_row, min_col, max_col = reconstruct_grid(df, 'WDPA_b1')
    wdpa_binary = np.where(np.isnan(wdpa_grid), 0, wdpa_grid > 0).astype(np.uint8)
    del wdpa_grid
    
    distance_grid = distance_transform_edt(1 - wdpa_binary) * 1000  # pixels to meters
    del wdpa_binary
    
    df['dist_wdpa'] = grid_to_dataframe(distance_grid, df, min_row, min_col)
    df['dist_wdpa_decay'] = np.exp(-df['dist_wdpa'] / DISTANCE_DECAY_SCALE)
    del distance_grid
    
    return df


def compute_smooth_feature(df: pd.DataFrame, variable: str, window_size: int) -> pd.Series:
    """Compute smoothed feature with given window size."""
    grid, min_row, max_row, min_col, max_col = reconstruct_grid(df, variable)
    valid_mask = ~np.isnan(grid)
    grid_filled = np.where(np.isnan(grid), 0, grid)
    del grid
    
    smoothed = uniform_filter(grid_filled, size=window_size, mode='constant')
    del grid_filled
    
    count = uniform_filter(valid_mask.astype(np.float32), size=window_size, mode='constant')
    del valid_mask
    
    smoothed = np.where(count > 0, smoothed, np.nan)
    del count
    
    result = grid_to_dataframe(smoothed, df, min_row, min_col)
    del smoothed
    
    return result


def compute_all_smooth_features(df: pd.DataFrame) -> pd.DataFrame:
    """Compute all smoothed features."""
    logger.info("  Computing smoothed features...")
    
    for variable in SMOOTH_VARIABLES:
        if variable not in df.columns:
            continue
        for window_size in SMOOTH_SIZES:
            df[f"{variable}_smooth{window_size}"] = compute_smooth_feature(df, variable, window_size)
            gc.collect()
    
    return df


def compute_transition_features(input_path: Path, year: int) -> pd.DataFrame:
    """Compute WDPA_prev and transition_01 for a given year by joining with previous year."""
    logger.info(f"  Computing transition features for year {year}...")
    
    # Read minimal columns for current year
    df_curr = pd.read_parquet(
        input_path, 
        columns=['row', 'col', 'year', 'WDPA_b1'],
        filters=[('year', '==', year)]
    )
    
    # Read minimal columns for previous year
    df_prev = pd.read_parquet(
        input_path,
        columns=['row', 'col', 'year', 'WDPA_b1'],
        filters=[('year', '==', year - 1)]
    )
    
    # Rename for join
    df_prev = df_prev.rename(columns={'WDPA_b1': 'WDPA_prev'})
    df_prev = df_prev.drop(columns=['year'])
    
    # Merge on spatial coordinates
    df_merged = df_curr.merge(df_prev, on=['row', 'col'], how='left')
    
    # Fill missing with 0 (pixels that didn't exist in previous year)
    df_merged['WDPA_prev'] = df_merged['WDPA_prev'].fillna(0)
    
    # Compute transition (0→1)
    df_merged['transition_01'] = (
        (df_merged['WDPA_prev'] == 0) & (df_merged['WDPA_b1'] == 1)
    ).astype(np.uint8)
    
    # Keep only the new columns
    result = df_merged[['row', 'col', 'WDPA_prev', 'transition_01']].copy()
    
    del df_curr, df_prev, df_merged
    gc.collect()
    
    return result


def process_year(input_path: Path, year: int, temp_dir: Path, is_first_year: bool) -> tuple[Path | None, dict]:
    """Process a single year and return temp file path + metrics."""
    start_time = time.time()
    logger.info(f"\nProcessing year {year}...")
    
    df_year = pd.read_parquet(input_path, filters=[('year', '==', year)], engine='pyarrow')
    n_rows = len(df_year)
    logger.info(f"  Loaded {n_rows:,} rows")
    
    if n_rows == 0:
        return None, {}
    
    # Add transition features (except for first year)
    if not is_first_year:
        df_transitions = compute_transition_features(input_path, year)
        df_year = df_year.merge(df_transitions, on=['row', 'col'], how='left')
        del df_transitions
        gc.collect()
    else:
        logger.info(f"  Skipping transition features for first year {year}")
        df_year['WDPA_prev'] = 0
        df_year['transition_01'] = 0
    
    df_year = compute_distance_features(df_year)
    df_year = compute_all_smooth_features(df_year)
    
    temp_file = temp_dir / f"year_{year}_processed.parquet"
    df_year.to_parquet(temp_file, engine='pyarrow', compression='snappy', index=False)
    
    del df_year
    gc.collect()
    
    elapsed = time.time() - start_time
    logger.info(f"  ✓ Year {year} complete in {elapsed:.1f}s")
    
    return temp_file, {"year": year, "rows": n_rows, "time_seconds": elapsed}


def main() -> None:
    """Main processing function."""
    repo_root = Path(__file__).resolve().parents[3]
    input_path = repo_root / "data/ml/merged_panel_2000_2024.parquet"
    output_path = repo_root / "data/ml/merged_panel_final.parquet"
    temp_dir = repo_root / "data/ml/.temp_spatial_features"
    
    # Initialize W&B
    wandb.init(
        project="thesis-spatial-features",
        name="spatial_features_preprocessing",
        config={
            "distance_decay_scale": DISTANCE_DECAY_SCALE,
            "smooth_variables": SMOOTH_VARIABLES,
            "smooth_sizes": SMOOTH_SIZES,
        }
    )
    
    logger.info("="*80)
    logger.info("SPATIAL FEATURES PREPROCESSING")
    logger.info("="*80)
    logger.info(f"Input:  {input_path}")
    logger.info(f"Output: {output_path}")
    
    if not input_path.exists():
        raise FileNotFoundError(f"Input file not found: {input_path}")
    
    temp_dir.mkdir(parents=True, exist_ok=True)
    
    # Get metadata
    logger.info("Reading metadata...")
    df_sample = pd.read_parquet(input_path, columns=['year'])
    years = sorted(df_sample['year'].unique())
    total_rows = len(df_sample)
    logger.info(f"Years: {years[0]}-{years[-1]} ({len(years)} years), Total rows: {total_rows:,}")
    del df_sample
    gc.collect()
    
    wandb.log({"total_years": len(years), "total_rows": total_rows})
    
    # Process year by year
    temp_files = []
    
    for i, year in enumerate(years):
        try:
            is_first_year = (i == 0)
            temp_file, metrics = process_year(input_path, year, temp_dir, is_first_year)
            if temp_file is not None:
                temp_files.append(temp_file)
                wandb.log(metrics)
        except Exception as e:
            logger.error(f"Error processing year {year}: {e}")
            for tf in temp_files:
                if tf.exists():
                    tf.unlink()
            if temp_dir.exists() and not list(temp_dir.iterdir()):
                temp_dir.rmdir()
            wandb.finish(exit_code=1)
            raise
    
    # Combine all years using streaming
    logger.info("\n" + "="*80)
    logger.info(f"Combining {len(temp_files)} files (streaming)...")
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    writer = None
    total_rows_written = 0
    concat_start = time.time()
    
    for i, temp_file in enumerate(temp_files):
        logger.info(f"  [{i+1}/{len(temp_files)}] {temp_file.name}...")
        table = pq.read_table(temp_file)
        total_rows_written += len(table)
        
        if writer is None:
            logger.info(f"  Initializing output ({len(table.column_names)} columns)...")
            writer = pq.ParquetWriter(output_path, table.schema, compression='snappy')
        
        writer.write_table(table)
        del table
        gc.collect()
        
        wandb.log({"concatenation_progress": (i + 1) / len(temp_files)})
    
    if writer is None:
        raise RuntimeError("No data was written!")
    
    writer.close()
    concat_time = time.time() - concat_start
    logger.info(f"  ✓ Wrote {total_rows_written:,} rows in {concat_time:.1f}s")
    
    # Clean up temp files
    logger.info("\nCleaning up temp files...")
    for temp_file in temp_files:
        temp_file.unlink()
    if temp_dir.exists() and not list(temp_dir.iterdir()):
        temp_dir.rmdir()
    
    # Summary
    file_size_mb = output_path.stat().st_size / (1024**2)
    schema = pq.read_schema(output_path)
    new_cols = [c for c in schema.names if c.startswith('dist_') or '_smooth' in c]
    
    logger.info("\n" + "="*80)
    logger.info("SUMMARY")
    logger.info("="*80)
    logger.info(f"Total rows: {total_rows_written:,}")
    logger.info(f"New columns: {len(new_cols)}")
    logger.info(f"File size: {file_size_mb:.1f} MB")
    logger.info("✓ Processing complete!")
    
    wandb.log({
        "final_rows": total_rows_written,
        "new_columns": len(new_cols),
        "file_size_mb": file_size_mb,
        "concatenation_time_seconds": concat_time
    })
    wandb.finish()


if __name__ == "__main__":
    main()

