#!/usr/bin/env python3

"""Spatial Features Preprocessing Script

Purpose: Add spatial features to the merged panel data:
    1. Distance features: distance to nearest WDPA protected area pixel
    2. Decay features: exp(-distance / 10000)
    3. Smoothed features: 4x4, 16x16, 64x64 mean for specified variables

Input:  data/ml/merged_panel_2000_2024.parquet
Output: data/ml/merged_panel_final.parquet

Processing: Year-by-year and chunked for memory efficiency with 500M+ rows
"""

from __future__ import annotations

import gc
import logging
import time
import warnings
from pathlib import Path
from typing import Tuple

import numpy as np
import pandas as pd
import pyarrow.parquet as pq
import wandb
from scipy.ndimage import distance_transform_edt, uniform_filter

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')

DISTANCE_DECAY_SCALE = 10000
SMOOTH_VARIABLES = ['NDVI_b1', 'GPW_b1', 'VIIRS_b1', 'gdp_b1', 'deforestation_b1', 
                     'wildfire_b1', 'GSN_b1', 'GSN_b2', 'GSN_b3', 'GSN_b4', 'GSN_b5']
SMOOTH_SIZES = [4, 16, 64]


def reconstruct_grid(df: pd.DataFrame, value_col: str) -> Tuple[np.ndarray, int, int, int, int]:
    """Reconstruct 2D grid from panel data."""
    min_row, max_row = df['row'].min(), df['row'].max()
    min_col, max_col = df['col'].min(), df['col'].max()
    
    grid = np.full((max_row - min_row + 1, max_col - min_col + 1), np.nan, dtype=np.float32)
    grid[(df['row'] - min_row).values, (df['col'] - min_col).values] = df[value_col].values
    
    return grid, min_row, max_row, min_col, max_col


def grid_to_dataframe(grid: np.ndarray, df: pd.DataFrame, min_row: int, min_col: int) -> pd.Series:
    """Convert grid back to DataFrame format."""
    values = grid[(df['row'] - min_row).values, (df['col'] - min_col).values]
    return pd.Series(values, index=df.index)


def compute_distance_features(df: pd.DataFrame) -> pd.DataFrame:
    """Compute distance to WDPA and decay features."""
    logger.info("  Computing distance features...")
    
    wdpa_grid, min_row, max_row, min_col, max_col = reconstruct_grid(df, 'WDPA_b1')
    wdpa_binary = np.where(np.isnan(wdpa_grid), 0, wdpa_grid > 0).astype(np.uint8)
    del wdpa_grid
    
    distance_grid = distance_transform_edt(1 - wdpa_binary) * 1000  # pixels to meters
    del wdpa_binary
    
    df['dist_wdpa'] = grid_to_dataframe(distance_grid, df, min_row, min_col)
    df['dist_wdpa_decay'] = np.exp(-df['dist_wdpa'] / DISTANCE_DECAY_SCALE)
    del distance_grid
    
    return df


def compute_smooth_feature(df: pd.DataFrame, variable: str, window_size: int) -> pd.Series:
    """Compute smoothed feature with given window size."""
    grid, min_row, max_row, min_col, max_col = reconstruct_grid(df, variable)
    valid_mask = ~np.isnan(grid)
    grid_filled = np.where(np.isnan(grid), 0, grid)
    del grid
    
    smoothed = uniform_filter(grid_filled, size=window_size, mode='constant')
    del grid_filled
    
    count = uniform_filter(valid_mask.astype(np.float32), size=window_size, mode='constant')
    del valid_mask
    
    smoothed = np.where(count > 0, smoothed, np.nan)
    del count
    
    result = grid_to_dataframe(smoothed, df, min_row, min_col)
    del smoothed
    
    return result


def compute_all_smooth_features(df: pd.DataFrame) -> pd.DataFrame:
    """Compute all smoothed features."""
    logger.info("  Computing smoothed features...")
    
    for variable in SMOOTH_VARIABLES:
        if variable not in df.columns:
            continue
        for window_size in SMOOTH_SIZES:
            df[f"{variable}_smooth{window_size}"] = compute_smooth_feature(df, variable, window_size)
            gc.collect()
    
    return df


def process_year(input_path: Path, year: int, temp_dir: Path) -> tuple[Path | None, dict]:
    """Process a single year and return temp file path + metrics."""
    start_time = time.time()
    logger.info(f"\nProcessing year {year}...")
    
    df_year = pd.read_parquet(input_path, filters=[('year', '==', year)], engine='pyarrow')
    n_rows = len(df_year)
    logger.info(f"  Loaded {n_rows:,} rows")
    
    if n_rows == 0:
        return None, {}
    
    df_year = compute_distance_features(df_year)
    df_year = compute_all_smooth_features(df_year)
    
    temp_file = temp_dir / f"year_{year}_processed.parquet"
    df_year.to_parquet(temp_file, engine='pyarrow', compression='snappy', index=False)
    
    del df_year
    gc.collect()
    
    elapsed = time.time() - start_time
    logger.info(f"  ✓ Year {year} complete in {elapsed:.1f}s")
    
    return temp_file, {"year": year, "rows": n_rows, "time_seconds": elapsed}


def main() -> None:
    """Main processing function."""
    repo_root = Path(__file__).resolve().parents[3]
    input_path = repo_root / "data/ml/merged_panel_2000_2024.parquet"
    output_path = repo_root / "data/ml/merged_panel_final.parquet"
    temp_dir = repo_root / "data/ml/.temp_spatial_features"
    
    # Initialize W&B
    wandb.init(
        project="thesis-spatial-features",
        name="spatial_features_preprocessing",
        config={
            "distance_decay_scale": DISTANCE_DECAY_SCALE,
            "smooth_variables": SMOOTH_VARIABLES,
            "smooth_sizes": SMOOTH_SIZES,
        }
    )
    
    logger.info("="*80)
    logger.info("SPATIAL FEATURES PREPROCESSING")
    logger.info("="*80)
    logger.info(f"Input:  {input_path}")
    logger.info(f"Output: {output_path}")
    
    if not input_path.exists():
        raise FileNotFoundError(f"Input file not found: {input_path}")
    
    temp_dir.mkdir(parents=True, exist_ok=True)
    
    # Get metadata
    logger.info("Reading metadata...")
    df_sample = pd.read_parquet(input_path, columns=['year'])
    years = sorted(df_sample['year'].unique())
    total_rows = len(df_sample)
    logger.info(f"Years: {years[0]}-{years[-1]} ({len(years)} years), Total rows: {total_rows:,}")
    del df_sample
    gc.collect()
    
    wandb.log({"total_years": len(years), "total_rows": total_rows})
    
    # Process year by year
    temp_files = []
    
    for year in years:
        try:
            temp_file, metrics = process_year(input_path, year, temp_dir)
            if temp_file is not None:
                temp_files.append(temp_file)
                wandb.log(metrics)
        except Exception as e:
            logger.error(f"Error processing year {year}: {e}")
            for tf in temp_files:
                if tf.exists():
                    tf.unlink()
            if temp_dir.exists() and not list(temp_dir.iterdir()):
                temp_dir.rmdir()
            wandb.finish(exit_code=1)
            raise
    
    # Combine all years using streaming
    logger.info("\n" + "="*80)
    logger.info(f"Combining {len(temp_files)} files (streaming)...")
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    writer = None
    total_rows_written = 0
    concat_start = time.time()
    
    for i, temp_file in enumerate(temp_files):
        logger.info(f"  [{i+1}/{len(temp_files)}] {temp_file.name}...")
        table = pq.read_table(temp_file)
        total_rows_written += len(table)
        
        if writer is None:
            logger.info(f"  Initializing output ({len(table.column_names)} columns)...")
            writer = pq.ParquetWriter(output_path, table.schema, compression='snappy')
        
        writer.write_table(table)
        del table
        gc.collect()
        
        wandb.log({"concatenation_progress": (i + 1) / len(temp_files)})
    
    if writer is None:
        raise RuntimeError("No data was written!")
    
    writer.close()
    concat_time = time.time() - concat_start
    logger.info(f"  ✓ Wrote {total_rows_written:,} rows in {concat_time:.1f}s")
    
    # Clean up temp files
    logger.info("\nCleaning up temp files...")
    for temp_file in temp_files:
        temp_file.unlink()
    if temp_dir.exists() and not list(temp_dir.iterdir()):
        temp_dir.rmdir()
    
    # Summary
    file_size_mb = output_path.stat().st_size / (1024**2)
    schema = pq.read_schema(output_path)
    new_cols = [c for c in schema.names if c.startswith('dist_') or '_smooth' in c]
    
    logger.info("\n" + "="*80)
    logger.info("SUMMARY")
    logger.info("="*80)
    logger.info(f"Total rows: {total_rows_written:,}")
    logger.info(f"New columns: {len(new_cols)}")
    logger.info(f"File size: {file_size_mb:.1f} MB")
    logger.info("✓ Processing complete!")
    
    wandb.log({
        "final_rows": total_rows_written,
        "new_columns": len(new_cols),
        "file_size_mb": file_size_mb,
        "concatenation_time_seconds": concat_time
    })
    wandb.finish()


if __name__ == "__main__":
    main()

