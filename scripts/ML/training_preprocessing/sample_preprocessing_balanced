#!/usr/bin/env python3

"""Temporal Transition Sampling Script (Optimized)

Purpose: Build a balanced dataset for predicting new WDPA establishment.
Input:  merged_panel_final.parquet (with transition_01 column pre-computed)
Process: Sample transitions on minimal columns, then read full features for sampled rows only.
Output: sample_training_balanced.parquet

Key optimization: Avoid heavy operations on full feature set (75GB+).
Instead, work with minimal columns (row, col, year, transition_01) for sampling,
then filter full features to only the sampled rows.
"""

from __future__ import annotations

import os
import sys
import time
from pathlib import Path

import duckdb
import wandb


# Configuration
TOTAL_SAMPLE_SIZE = 2_000_000
POSITIVE_LIMIT = 1_000_000
RANDOM_STATE = 42


def main() -> None:
    start_time = time.time()
    
    # Get W&B credentials from environment
    wandb_api_key = os.environ.get("WANDB_API_KEY")
    wandb_entity = os.environ.get("WANDB_ENTITY")
    
    if not wandb_api_key:
        print("Warning: WANDB_API_KEY not found in environment")
    if not wandb_entity:
        print("Warning: WANDB_ENTITY not found in environment")
    
    # Initialize W&B
    print("Initializing Weights & Biases...")
    wandb.init(
        project="ml-training-preprocessing",
        entity=wandb_entity,
        name=f"sample_preprocessing_{time.strftime('%Y%m%d_%H%M%S')}",
        config={
            "total_sample_size": TOTAL_SAMPLE_SIZE,
            "positive_limit": POSITIVE_LIMIT,
            "random_state": RANDOM_STATE,
        },
    )
    print("W&B connected")
    
    # Setup paths (outside try block for cleanup in finally)
    repo_root = Path(__file__).resolve().parents[3]
    input_path = repo_root / "data/ml/merged_panel_final.parquet"
    output_path = repo_root / "data/ml/sample_training_balanced.parquet"
    temp_dir = repo_root / "data/ml/temp"
    
    try:
        
        temp_dir.mkdir(parents=True, exist_ok=True)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        print(f"\nInput: {input_path}")
        print(f"Output: {output_path}")
        
        if not input_path.exists():
            raise FileNotFoundError(f"Input file not found: {input_path}")
        
        # Initialize DuckDB with memory-efficient settings
        con = duckdb.connect()
        
        # Euler cluster: 128GB nodes, use 48 cores and 100GB memory
        # Local: more conservative 4 cores and 16GB
        is_euler = bool(os.environ.get("SCRATCH"))
        num_threads = 48 if is_euler else 4
        memory_limit_gb = 100 if is_euler else 16
        
        con.execute(f"SET threads={num_threads}")
        con.execute("SET preserve_insertion_order=false")
        con.execute(f"SET memory_limit='{memory_limit_gb}GB'")
        
        # Use SCRATCH temp directory if on Euler, otherwise local temp dir
        temp_dir_env = os.environ.get("SCRATCH") or str(temp_dir)
        # Escape single quotes for SQL literal (handles paths like \"Master's Thesis\")
        temp_dir_sql = temp_dir_env.replace("'", "''")
        con.execute(f"SET temp_directory='{temp_dir_sql}/duckdb_temp'")
        
        if is_euler:
            con.execute("PRAGMA max_temp_directory_size='200GB'")
        
        print(f"DuckDB configured: {num_threads} threads, memory_limit={memory_limit_gb}GB")
        print(f"Running on: {'Euler cluster' if is_euler else 'local machine'}")
        
        wandb.log({
            "config/threads": num_threads, 
            "config/memory_limit_gb": memory_limit_gb,
            "config/is_euler": is_euler
        })
        
        # Escape paths for SQL
        escaped_in = str(input_path).replace("'", "''")
        escaped_out = str(output_path).replace("'", "''")
        
        # Step 1: Read minimal columns for transition analysis
        print("\nStep 1: Reading minimal columns (row, col, year, transition_01)...")
        print("   Avoiding full feature set (75GB+) for sampling decision")
        print("   Note: First year has transition_01=0 by design, will be filtered during sampling")
        step1_start = time.time()
        
        # Create temporary file with minimal columns only
        # No need to filter by year - transition_01 column already excludes first year's transitions
        temp_minimal = temp_dir / "temp_minimal_columns.parquet"
        escaped_minimal = str(temp_minimal).replace("'", "''")
        
        con.execute(f"""
            COPY (
                SELECT "row", "col", "year", "transition_01"
                FROM read_parquet('{escaped_in}')
            )
            TO '{escaped_minimal}'
            (FORMAT PARQUET, COMPRESSION ZSTD)
        """)
        
        step1_time = time.time() - step1_start
        print(f"Completed in {step1_time:.1f}s ({step1_time/60:.1f} min)")
        wandb.log({"timing/step1_seconds": step1_time, "timing/step1_minutes": step1_time/60})
        
        # Step 2: Count positives and negatives from minimal columns
        print("\nStep 2: Counting transitions from minimal columns...")
        step2_start = time.time()
        
        counts = con.execute(f"""
            SELECT 
                SUM("transition_01") AS positives,
                SUM(CASE WHEN "transition_01" = 0 THEN 1 ELSE 0 END) AS negatives,
                COUNT(*) AS total,
                MIN("year") AS year_min,
                MAX("year") AS year_max
            FROM read_parquet('{escaped_minimal}')
        """).fetchone()
        
        positives, negatives, total, year_min, year_max = counts
        
        print(f"   Total rows (with transition labels): {total:,}")
        print(f"   Positive transitions (0→1):          {positives:,}")
        print(f"   Negative transitions:                {negatives:,}")
        print(f"   Year range:                          {year_min}–{year_max}")
        
        step2_time = time.time() - step2_start
        wandb.log({
            "data/total_rows": total,
            "data/positives": positives,
            "data/negatives": negatives,
            "data/positive_ratio": positives / total if total > 0 else 0,
            "data/year_min": year_min,
            "data/year_max": year_max,
            "timing/step2_seconds": step2_time,
        })
        
        if positives == 0:
            raise ValueError("No positive transitions found! Cannot build training sample.")
        
        # Step 3: Determine sampling strategy
        print("\nStep 3: Determining sampling strategy...")
        
        sample_positives = min(POSITIVE_LIMIT, positives)
        sample_negatives = min(sample_positives, negatives)  # Match positives for balance
        total_sample = sample_positives + sample_negatives
        
        print(f"   Sampling {sample_positives:,} positives (of {positives:,} available)")
        print(f"   Sampling {sample_negatives:,} negatives (of {negatives:,} available)")
        print(f"   Total sample size: {total_sample:,}")
        print(f"   Class balance: {sample_positives/total_sample*100:.1f}% positive")
        
        wandb.log({
            "sampling/positives": sample_positives,
            "sampling/negatives": sample_negatives,
            "sampling/total": total_sample,
            "sampling/balance": sample_positives / total_sample if total_sample > 0 else 0,
        })
        
        # Step 4: Sample on minimal columns to get (row, col, year) keys
        print("\nStep 4: Sampling on minimal columns to identify rows to keep...")
        step4_start = time.time()
        
        # Create temporary files for sampled keys
        temp_sampled_keys = temp_dir / "temp_sampled_keys.parquet"
        escaped_keys = str(temp_sampled_keys).replace("'", "''")
        
        # Sample positives and negatives, keep only keys (row, col, year)
        print(f"   Sampling {sample_positives:,} positives + {sample_negatives:,} negatives (keys only)...")
        con.execute(f"""
            COPY (
                SELECT "row", "col", "year"
                FROM (
                    SELECT "row", "col", "year"
                    FROM read_parquet('{escaped_minimal}')
                    WHERE "transition_01" = 1
                )
                USING SAMPLE reservoir({sample_positives} ROWS) REPEATABLE ({RANDOM_STATE})
                
                UNION ALL
                
                SELECT "row", "col", "year"
                FROM (
                    SELECT "row", "col", "year"
                    FROM read_parquet('{escaped_minimal}')
                    WHERE "transition_01" = 0
                )
                USING SAMPLE reservoir({sample_negatives} ROWS) REPEATABLE ({RANDOM_STATE})
            )
            TO '{escaped_keys}'
            (FORMAT PARQUET, COMPRESSION ZSTD)
        """)
        
        step4_time = time.time() - step4_start
        print(f"Sampled keys in {step4_time:.1f}s")
        wandb.log({"timing/step4_seconds": step4_time})
        
        # Step 5: Join sampled keys with full feature set
        print("\nStep 5: Reading full features for sampled rows only...")
        print(f"   This reads only ~{total_sample:,} rows from the full feature set")
        step5_start = time.time()
        
        # Join sampled keys with full dataset - DuckDB optimizes this as a semi-join
        con.execute(f"""
            COPY (
                SELECT full.*
                FROM read_parquet('{escaped_in}') AS full
                INNER JOIN read_parquet('{escaped_keys}') AS keys
                    ON full.row = keys.row 
                    AND full.col = keys.col 
                    AND full.year = keys.year
            )
            TO '{escaped_out}'
            (FORMAT PARQUET, COMPRESSION ZSTD)
        """)
        
        step5_time = time.time() - step5_start
        print(f"Written full features to {output_path} in {step5_time:.1f}s")
        wandb.log({"timing/step5_seconds": step5_time})
        
        # Clean up temporary files
        print("\nCleaning up temporary files...")
        temp_minimal.unlink(missing_ok=True)
        temp_sampled_keys.unlink(missing_ok=True)
        print("Temporary files removed")
        
        # Step 6: Verify output
        print("\nStep 6: Verifying output...")
        
        verify_counts = con.execute(f"""
            SELECT 
                SUM("transition_01") AS pos_written,
                SUM(CASE WHEN "transition_01" = 0 THEN 1 ELSE 0 END) AS neg_written,
                COUNT(*) AS total_written,
                COUNT(DISTINCT "year") AS n_years,
                MIN("year") AS min_year,
                MAX("year") AS max_year
            FROM read_parquet('{escaped_out}')
        """).fetchone()
        
        pos_written, neg_written, total_written, n_years, min_year, max_year = verify_counts
        file_size_mb = output_path.stat().st_size / (1024 * 1024)
        total_time = time.time() - start_time
        
        # Print summary
        print("\n" + "="*70)
        print("SAMPLING COMPLETE")
        print("="*70)
        print(f"  Total rows:       {total_written:,}")
        print(f"  Positives (0→1):  {pos_written:,} ({pos_written/total_written*100:.1f}%)")
        print(f"  Negatives:        {neg_written:,} ({neg_written/total_written*100:.1f}%)")
        print(f"  Years covered:    {n_years} years ({min_year}–{max_year})")
        print(f"  File size:        {file_size_mb:.1f} MB")
        print(f"  Total time:       {total_time:.1f}s ({total_time/60:.1f} min)")
        print("="*70)
        
        # Log final results to W&B
        wandb.log({
            "output/total_rows": total_written,
            "output/positives": pos_written,
            "output/negatives": neg_written,
            "output/balance": pos_written / total_written if total_written > 0 else 0,
            "output/n_years": n_years,
            "output/year_min": min_year,
            "output/year_max": max_year,
            "output/file_size_mb": file_size_mb,
            "timing/total_seconds": total_time,
            "timing/total_minutes": total_time / 60,
            "status": "success",
        })
        
        # Warn if imbalanced
        balance = pos_written / total_written if total_written > 0 else 0
        if balance < 0.4 or balance > 0.6:
            print(f"\nWarning: Class balance is {balance*100:.1f}% (target: 50%)")
            wandb.alert(
                title="Class Imbalance Detected",
                text=f"Class balance is {balance*100:.1f}% positive",
                level=wandb.AlertLevel.WARN
            )
        
        print("\nAll done!")
        
    except Exception as e:
        error_msg = f"{type(e).__name__}: {str(e)}"
        print(f"\nERROR: {error_msg}")
        wandb.log({
            "status": "failed",
            "error_type": type(e).__name__,
            "error_message": str(e),
        })
        raise
    
    finally:
        # Clean up temporary files in case of error
        try:
            cleanup_files = [
                temp_dir / "temp_minimal_columns.parquet",
                temp_dir / "temp_sampled_keys.parquet"
            ]
            cleaned = False
            for f in cleanup_files:
                if f.exists():
                    f.unlink()
                    cleaned = True
            if cleaned:
                print("Cleaned up temporary files")
        except:
            pass
        
        wandb.finish()
        print("W&B run finished")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\nScript failed: {e}", file=sys.stderr)
        sys.exit(1)
