#!/usr/bin/env python3
"""
Panel dataset diagnostic visualizations & Data Integrity Debugger.

Added Features:
- Temporal Feature Comparison: Subplots comparing geography of 2011 vs 2013 vs 2015.
- Robust Data Integrity Scan: Automated detection of row-count crashes.

Usage notes:
- Sampling method (DuckDB `USING SAMPLE`):
  - Default: bernoulli
  - Override: set `PANEL_VIS_SAMPLE_METHOD=system` (or any value not containing 'bern')
  - Example:
    - `PANEL_VIS_SAMPLE_METHOD=bernoulli python3 scripts/ML/visualisations/visualise_panel`
"""

import os
import socket
import getpass
import warnings
from datetime import datetime
from pathlib import Path
from typing import Optional, List, Tuple, Dict
import glob

import duckdb
import matplotlib
matplotlib.use("Agg")  # Headless-safe
import matplotlib.pyplot as plt
from matplotlib.colors import LinearSegmentedColormap, TwoSlopeNorm
from matplotlib.ticker import FuncFormatter
import numpy as np
import pandas as pd
import seaborn as sns

try:
    import wandb
except ImportError:
    wandb = None

warnings.filterwarnings('ignore')

# ==========================================================================================
# PATH SETUP
# ==========================================================================================
ROOT_DIR = Path(__file__).resolve().parents[3]
OUTPUT_DIR = ROOT_DIR / "outputs" / "Figures" / "panel_visualisation"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

SCRATCH = os.environ.get("SCRATCH")

def _sql_escape_string_literal(value: str) -> str:
    """Escape a string for use inside single-quoted SQL string literals."""
    return value.replace("'", "''")

def quote_ident(name: str) -> str:
    """DuckDB identifier quoting (double quotes)."""
    return '"' + name.replace('"', '""') + '"'

# Backwards-compatible alias (older code in this file used `q(...)`)
q = quote_ident

# SQL-expression overrides for specific features (used to sanitize known data issues in plots).
# Key: feature name as it appears in the parquet schema
# Value: SQL expression (unaliased) that evaluates to that feature.
FEATURE_SQL_OVERRIDES: Dict[str, str] = {}

def feature_sql(feature: str) -> str:
    """Return SQL expression for a feature, applying any configured override."""
    return FEATURE_SQL_OVERRIDES.get(feature, q(feature))

def _resolve_panel_path_spec_to_abs(spec: str) -> str:
    """
    Resolve PANEL_FILE spec to an absolute path/pattern relative to repo root.

    - If spec is absolute: keep as-is.
    - If spec is relative (incl. './...'): interpret relative to ROOT_DIR.
    """
    spec = spec.strip()
    if not spec:
        return spec
    p = Path(spec)
    if p.is_absolute():
        return str(p)
    # Interpret relative paths as relative to repo root for reproducibility
    return str((ROOT_DIR / p).resolve())

def resolve_panel_spec() -> str:
    """
    Decide what parquet path/pattern DuckDB should read.

    Supports:
    - A single file path
    - A directory (reads '*.parquet' within it)
    - A glob pattern (e.g. '**/*scored*.parquet')
    """
    env_spec = (os.environ.get("PANEL_FILE") or "").strip()
    if env_spec:
        abs_spec = _resolve_panel_path_spec_to_abs(env_spec)
        # If it's a directory, turn into a parquet glob
        if Path(abs_spec).exists() and Path(abs_spec).is_dir():
            abs_spec = str(Path(abs_spec) / "*.parquet")
        # If it looks like a glob, ensure it matches something (best-effort)
        if any(ch in abs_spec for ch in ["*", "?", "["]):
            matches = glob.glob(abs_spec, recursive=True)
            if len(matches) == 0:
                raise FileNotFoundError(f"PANEL_FILE glob matched 0 files: {abs_spec}")
        else:
            if not Path(abs_spec).exists():
                raise FileNotFoundError(f"PANEL_FILE not found: {abs_spec}")
        return abs_spec

    # Fallbacks (Euler scratch first, then repo)
    candidate_paths = [
        Path(SCRATCH) / "data" / "ml" / "merged_panel_final.parquet" if SCRATCH else None,
        Path(SCRATCH) / "data" / "ml" / "merged_panel_2000_2024.parquet" if SCRATCH else None,
        ROOT_DIR / "data" / "ml" / "merged_panel_final.parquet",
        ROOT_DIR / "data" / "ml" / "merged_panel_2000_2024.parquet",
    ]
    panel_file = next((p for p in candidate_paths if p and p.exists()), None)
    if not panel_file:
        raise FileNotFoundError(f"Parquet not found. Searched: {[str(p) for p in candidate_paths if p]}")
    return str(panel_file)

PANEL_SPEC = resolve_panel_spec()

# ==========================================================================================
# HELPERS
# ==========================================================================================
def resolve_sample_method() -> str:
    # Default to Bernoulli sampling (more stable for approximate sampling across partitions).
    raw = (os.environ.get("PANEL_VIS_SAMPLE_METHOD") or "bernoulli").lower()
    return "bernoulli" if "bern" in raw else "system"

def resolve_coord_cols(cols: List[str]) -> Tuple[str, str]:
    """
    Choose best-available coordinate columns for plotting.

    Preference order:
    - x/y
    - lon/lat
    - longitude/latitude
    - easting/northing
    - col/row (fallback to binned grid indices)
    """
    cset = set(cols)
    for x, y in [("x", "y"), ("lon", "lat"), ("longitude", "latitude"), ("easting", "northing"), ("col", "row")]:
        if x in cset and y in cset:
            return x, y
    # Last resort: keep current behavior (will raise a clearer error later)
    return "col", "row"

def _neutral_diverging_cmap() -> LinearSegmentedColormap:
    """Diverging cmap with an explicit neutral midpoint (light gray/white)."""
    return LinearSegmentedColormap.from_list(
        "coolwarm_neutral_mid",
        ["#3b4cc0", "#f2f2f2", "#b40426"],
        N=256,
    )

def _is_numeric_duckdb_type(type_str: str) -> bool:
    """Best-effort numeric type detection from DuckDB DESCRIBE column_type."""
    t = (type_str or "").upper()
    numeric_markers = [
        "TINYINT", "SMALLINT", "INTEGER", "INT", "BIGINT", "HUGEINT",
        "UTINYINT", "USMALLINT", "UINTEGER", "UBIGINT",
        "FLOAT", "DOUBLE", "REAL", "DECIMAL",
    ]
    return any(m in t for m in numeric_markers)

def panel_read_expr() -> str:
    """
    DuckDB relation expression for the panel parquet(s).

    Key behavior:
    - Always uses union_by_name=true to prevent schema-mismatch crashes when PANEL_FILE is a glob.
    """
    escaped = _sql_escape_string_literal(PANEL_SPEC)
    return f"read_parquet('{escaped}', union_by_name=true)"

def _parse_bool_env(name: str, default: bool = False) -> bool:
    raw = os.environ.get(name)
    if raw is None:
        return bool(default)
    return str(raw).strip().lower() in ("1", "true", "yes", "y", "on")

def _configure_dist_wdpa_zero_recode(con, cols: List[str], label_col: Optional[str]) -> None:
    """
    Sanity check for dist_wdpa==0 and optionally treat them as missing in plots,
    but ONLY under WDPA_prev==0 (risk set).

    Behavior (env-configurable):
    - PANEL_VIS_RECODE_DIST_WDPA_ZERO: 'auto' (default) | 1/true | 0/false
    - PANEL_VIS_DIST_WDPA_ZERO_ABS_THRESHOLD: default 1000
    - PANEL_VIS_DIST_WDPA_ZERO_RATIO_THRESHOLD: default 1e-4
    """
    if "dist_wdpa" not in cols or "WDPA_prev" not in cols:
        return

    # Counts
    n_prev0_total = con.execute(
        f"SELECT COUNT(*) FROM {panel_read_expr()} WHERE {q('WDPA_prev')} = 0"
    ).fetchone()[0]
    n_prev0_dist0 = con.execute(
        f"SELECT COUNT(*) FROM {panel_read_expr()} WHERE {q('WDPA_prev')} = 0 AND {q('dist_wdpa')} = 0"
    ).fetchone()[0]

    n_pos_dist0 = None
    if label_col and label_col in cols:
        n_pos_total = con.execute(
            f"SELECT COUNT(*) FROM {panel_read_expr()} WHERE {q(label_col)} = 1"
        ).fetchone()[0]
        n_pos_dist0 = con.execute(
            f"SELECT COUNT(*) FROM {panel_read_expr()} WHERE {q(label_col)} = 1 AND {q('dist_wdpa')} = 0"
        ).fetchone()[0]
    else:
        n_pos_total = None

    ratio_prev0 = float(n_prev0_dist0) / max(int(n_prev0_total), 1)
    if n_pos_total is not None:
        ratio_pos = float(n_pos_dist0) / max(int(n_pos_total), 1)
    else:
        ratio_pos = None

    print("\nWDPA distance sanity checks:")
    print(f"  WDPA_prev==0 AND dist_wdpa==0: {n_prev0_dist0:,} / {n_prev0_total:,} ({ratio_prev0:.6%})")
    if n_pos_total is not None and n_pos_dist0 is not None:
        print(f"  transition_01==1 AND dist_wdpa==0: {n_pos_dist0:,} / {n_pos_total:,} ({ratio_pos:.6%})")

    # Decide recode
    mode = (os.environ.get("PANEL_VIS_RECODE_DIST_WDPA_ZERO") or "auto").strip().lower()
    abs_thresh = int(os.environ.get("PANEL_VIS_DIST_WDPA_ZERO_ABS_THRESHOLD", "1000"))
    ratio_thresh = float(os.environ.get("PANEL_VIS_DIST_WDPA_ZERO_RATIO_THRESHOLD", "1e-4"))

    if mode in ("1", "true", "yes", "y", "on"):
        enable = True
        reason = "forced on by PANEL_VIS_RECODE_DIST_WDPA_ZERO"
    elif mode in ("0", "false", "no", "n", "off"):
        enable = False
        reason = "forced off by PANEL_VIS_RECODE_DIST_WDPA_ZERO"
    else:
        enable = (n_prev0_dist0 >= abs_thresh) or (ratio_prev0 >= ratio_thresh)
        reason = f"auto (abs>={abs_thresh} or ratio>={ratio_thresh})"

    if enable:
        # Treat dist_wdpa==0 as missing ONLY when WDPA_prev==0.
        # This avoids turning legitimate zero distances into missing in other parts of the data.
        FEATURE_SQL_OVERRIDES["dist_wdpa"] = (
            f"CASE WHEN {q('WDPA_prev')} = 0 THEN NULLIF({q('dist_wdpa')}, 0) ELSE {q('dist_wdpa')} END"
        )
        print(f"  -> Applying plot-only recode for dist_wdpa zeros under WDPA_prev==0 ({reason}).")
    else:
        print(f"  -> Not recoding dist_wdpa zeros ({reason}).")

def wb_log(data: dict):
    if wandb and wandb.run: wandb.log(data)

def wb_log_image(key: str, path: Path, caption: str):
    if wandb and wandb.run:
        try:
            wandb.log({key: wandb.Image(str(path), caption=caption)})
        except Exception as e:
            print(f"W&B Image Log Error: {e}")

def wb_log_artifact_pngs(output_dir: Path, artifact_name: Optional[str] = None):
    """Log all PNGs in output_dir as a W&B Artifact for versioned lineage."""
    if not (wandb and wandb.run):
        return
    pngs = sorted(output_dir.glob("*.png"))
    if not pngs:
        return
    name = artifact_name or f"panel_visualisation_pngs_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    try:
        art = wandb.Artifact(name=name, type="panel_visualisation")
        for p in pngs:
            art.add_file(str(p))
        wandb.log_artifact(art)
    except Exception as e:
        print(f"W&B Artifact Log Error: {e}")

# ==========================================================================================
# DUCKDB CONFIGURATION
# ==========================================================================================
def get_con():
    con = duckdb.connect()
    is_euler = bool(os.environ.get("SCRATCH"))
    cpus = int(os.environ.get("SLURM_CPUS_PER_TASK", "0"))
    threads = cpus if cpus > 0 else (48 if is_euler else 4)
    
    mem_mb = os.environ.get("SLURM_MEM_PER_CPU")
    limit_gb = (int(mem_mb) * cpus // 1024) if (mem_mb and cpus) else (128 if is_euler else 16)
    
    con.execute(f"SET threads={threads}")
    con.execute(f"SET memory_limit='{limit_gb}GB'")
    
    tmp = Path(os.environ.get("SCRATCH", ROOT_DIR)) / "duckdb_temp"
    tmp.mkdir(parents=True, exist_ok=True)
    tmp_sql = _sql_escape_string_literal(str(tmp))
    con.execute(f"SET temp_directory='{tmp_sql}'")
    return con

# ==========================================================================================
# TEMPORAL FEATURE COMPARISON (DEBUGGER)
# ==========================================================================================
def plot_temporal_feature_comparison(
    con,
    feature: str,
    years=[2011, 2013, 2015],
    show_density_diff: Optional[bool] = None,
):
    """
    Compare geography of a feature across years, with a strict global color scale.

    - Global color scale: 2nd/98th percentiles computed on the combined 3-year sample.
    - Optional density-difference panel (2011 vs 2013) to highlight spatial loss during collapse.
    """
    print(f"Creating Temporal Comparison for {feature}...")
    sample_pct = float(os.environ.get("PANEL_VIS_TEMPORAL_SAMPLE_PCT", "2.0"))
    method = resolve_sample_method()
    show_density_diff = bool(int(os.environ.get("PANEL_VIS_SHOW_DENSITY_DIFF", "1"))) if show_density_diff is None else show_density_diff
    
    # 0. Pick coordinates (avoid row/col if x/y or lon/lat exist)
    schema = con.execute(f"DESCRIBE SELECT * FROM {panel_read_expr()} LIMIT 0").df()
    cols = schema["column_name"].tolist()
    cx, cy = resolve_coord_cols(cols)

    # 1. Strict global color scale from combined 3-year sample (2nd/98th percentiles)
    year_list = ",".join(str(int(y)) for y in years)
    feat_expr = feature_sql(feature)
    clip_query = f"""
        SELECT
            approx_quantile(val, 0.02) AS vmin,
            approx_quantile(val, 0.98) AS vmax
        FROM (
            SELECT {feat_expr} AS val
            FROM {panel_read_expr()}
            WHERE year IN ({year_list}) AND {feat_expr} IS NOT NULL
            USING SAMPLE {sample_pct}% ({method})
        ) t
    """
    vmin, vmax = con.execute(clip_query).fetchone()

    ncols = 4 if show_density_diff else 3
    fig, axes = plt.subplots(1, ncols, figsize=(6.5 * ncols, 6), sharex=True, sharey=True)
    if ncols == 1:
        axes = [axes]
    
    year_dfs: Dict[int, pd.DataFrame] = {}
    for i, year in enumerate(years):
        df = con.execute(f"""
            SELECT {q(cy)} AS y, {q(cx)} AS x, {feat_expr} AS val
            FROM {panel_read_expr()} 
            WHERE year = {int(year)} AND {feat_expr} IS NOT NULL
            USING SAMPLE {sample_pct}% ({method}) 
            LIMIT 300000
        """).df()
        year_dfs[int(year)] = df
        
        ax = axes[i]
        if not df.empty:
            sc = ax.scatter(df['x'], df['y'], c=df['val'], s=0.1,
                            cmap='viridis', vmin=vmin, vmax=vmax, rasterized=True)
            ax.set_title(f"Year {year} (n={len(df):,})", fontsize=12)
        else:
            ax.set_title(f"Year {year} (EMPTY)", color='red')
            
        ax.set_aspect('equal')

    # 2. Optional density difference (2011 vs 2013)
    if show_density_diff:
        ax = axes[-1]
        if len(years) > 1:
            y0, y1 = int(years[0]), int(years[1])
        else:
            y0 = y1 = int(years[0])
        # IMPORTANT: density diff should reflect the risk set footprint (not just non-null feature rows)
        dens_sample_pct = float(os.environ.get("PANEL_VIS_DENSITY_SAMPLE_PCT", str(sample_pct)))
        a = con.execute(
            f"""
            SELECT {q(cy)} AS y, {q(cx)} AS x
            FROM {panel_read_expr()}
            WHERE year = {y0}
            USING SAMPLE {dens_sample_pct}% ({method})
            LIMIT 500000
            """
        ).df()
        b = con.execute(
            f"""
            SELECT {q(cy)} AS y, {q(cx)} AS x
            FROM {panel_read_expr()}
            WHERE year = {y1}
            USING SAMPLE {dens_sample_pct}% ({method})
            LIMIT 500000
            """
        ).df()
        if a.empty or b.empty:
            ax.set_title(f"Density diff ({y0} vs {y1}) (EMPTY)", color="red")
        else:
            x_all = pd.concat([a["x"], b["x"]], ignore_index=True)
            y_all = pd.concat([a["y"], b["y"]], ignore_index=True)
            xmin, xmax = float(x_all.min()), float(x_all.max())
            ymin, ymax = float(y_all.min()), float(y_all.max())

            bins = int(os.environ.get("PANEL_VIS_DENSITY_BINS", "260"))
            ha, xe, ye = np.histogram2d(a["x"].to_numpy(), a["y"].to_numpy(), bins=bins, range=[[xmin, xmax], [ymin, ymax]])
            hb, _, _ = np.histogram2d(b["x"].to_numpy(), b["y"].to_numpy(), bins=[xe, ye])

            da = ha / max(ha.sum(), 1.0)
            db = hb / max(hb.sum(), 1.0)
            diff = (da - db).T  # transpose for imshow orientation (y as rows)

            vmax_d = float(np.nanpercentile(np.abs(diff), 99))
            norm = TwoSlopeNorm(vmin=-vmax_d, vcenter=0.0, vmax=vmax_d)
            im = ax.imshow(
                diff,
                origin="lower",
                extent=[xmin, xmax, ymin, ymax],
                cmap=_neutral_diverging_cmap(),
                norm=norm,
                interpolation="nearest",
                rasterized=True,
            )
            ax.set_title(f"Density diff: {y0} - {y1}", fontsize=12)
            cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
            cbar.set_label("Δ density (normalized)")
        ax.set_aspect("equal")

    plt.suptitle(
        f"Temporal Integrity Check: {feature} ({' vs '.join(map(str, years))})",
        fontsize=15,
        fontweight='bold',
    )
    fig.subplots_adjust(right=0.92)
    cbar_ax = fig.add_axes([0.94, 0.15, 0.015, 0.7])
    fig.colorbar(plt.cm.ScalarMappable(norm=plt.Normalize(vmin, vmax), cmap='viridis'), cax=cbar_ax, label=feature)
    
    out_path = OUTPUT_DIR / f"integrity_temporal_comparison_{feature}.png"
    plt.savefig(out_path, dpi=300, bbox_inches='tight')
    plt.close()
    wb_log_image(f"plots/temporal_comp_{feature}", out_path, f"Temporal comparison for {feature}")

# ==========================================================================================
# STANDARD PLOTS (OPTIMIZED)
# ==========================================================================================
def plot_time_series(con, label_col):
    print("Creating time series...")
    query = f"""
        SELECT year, COUNT(*) AS rows, 
        {f"SUM(CASE WHEN {q(label_col)} = 1 THEN 1 ELSE 0 END)" if label_col else "0"} AS pos
        FROM {panel_read_expr()} GROUP BY year ORDER BY year
    """
    df = con.execute(query).df()
    fig, ax1 = plt.subplots(figsize=(12, 6))
    ax1.plot(df['year'], df['rows'], color='tab:blue', marker='o', label='Total Rows')
    ax1.set_ylabel('Risk Set Size', color='tab:blue')
    if label_col:
        ax2 = ax1.twinx()
        ax2.bar(df['year'], df['pos'], color='tab:orange', alpha=0.4, label='Positives')
        ax2.set_ylabel('Positives', color='tab:orange')
    plt.title("Risk set size by year (shrinks as pixels become protected)")
    out = OUTPUT_DIR / "time_series.png"
    plt.savefig(out, dpi=300); plt.close()
    return out

def plot_spatial_map(con, label_col):
    if not label_col: return None
    print("Creating spatial map...")
    method = resolve_sample_method()
    schema = con.execute(f"DESCRIBE SELECT * FROM {panel_read_expr()} LIMIT 0").df()
    cols = schema["column_name"].tolist()
    cx, cy = resolve_coord_cols(cols)

    year_ref = int(os.environ.get("PANEL_VIS_MAP_YEAR", "2024"))
    pos_year = int(os.environ.get("PANEL_VIS_POS_YEAR", str(year_ref)))

    # Pull background from the full risk set to show geographic footprint clearly
    # IMPORTANT: pin to a single reference year to avoid multi-year duplicates inflating footprint.
    bg = con.execute(
        f"""
        SELECT DISTINCT x, y
        FROM (
            SELECT {q(cx)} AS x, {q(cy)} AS y
            FROM {panel_read_expr()}
            WHERE year = {year_ref}
            USING SAMPLE 2% ({method})
            LIMIT 800000
        ) t
        """
    ).df()
    # Overlay positives (if any)
    pos = con.execute(
        f"""
        SELECT DISTINCT x, y
        FROM (
            SELECT {q(cx)} AS x, {q(cy)} AS y
            FROM {panel_read_expr()}
            WHERE year = {pos_year} AND {q(label_col)} = 1
            USING SAMPLE 10% ({method})
            LIMIT 300000
        ) t
        """
    ).df()
    
    fig, ax = plt.subplots(figsize=(12, 10))
    ax.scatter(bg['x'], bg['y'], s=0.05, c='lightgray', alpha=0.15, rasterized=True)
    if not pos.empty:
        ax.scatter(pos['x'], pos['y'], s=0.5, c='red', alpha=0.5, rasterized=True)
    ax.set_aspect('equal')
    if pos_year == year_ref:
        ax.set_title(f"Spatial distribution of positives ({label_col}=1), year={year_ref}")
    else:
        ax.set_title(f"Spatial distribution of positives ({label_col}=1), bg_year={year_ref}, pos_year={pos_year}")
    out = OUTPUT_DIR / "spatial_map.png"
    plt.savefig(out, dpi=300); plt.close()
    return out

# ==========================================================================================
# CLASS SEPARATION + FEATURE SELECTION
# ==========================================================================================
def _is_near_flat(var_samp: float, q02: float, q98: float) -> bool:
    """Exclude exactly-flat or effectively-flat features (e.g., var_samp≈0, tiny percentile spread)."""
    if q02 is None or q98 is None or np.isnan(q02) or np.isnan(q98):
        return True
    if var_samp is not None and (not np.isnan(var_samp)) and var_samp <= 0:
        return True
    spread = float(q98) - float(q02)
    # Scale-aware threshold: require some meaningful spread relative to magnitude
    scale = max(abs(float(q02)), abs(float(q98)), 1.0)
    return spread <= (1e-6 * scale)

def _select_top_variance_features(
    con,
    numeric_cols: List[str],
    max_features: int = 4,
    sample_pct: float = 2.0,
    method: Optional[str] = None,
) -> List[str]:
    """
    Pick top-variance features, strictly skipping flat/near-flat ones.
    Returns up to max_features.
    """
    if not numeric_cols:
        return []
    method = method or resolve_sample_method()
    sample_pct = float(sample_pct)

    # IMPORTANT: compute all aggregates in a single scan (avoid per-column scans).
    # NOTE: DuckDB can throw OutOfRange for VARSAMP on extreme-value columns; for robustness
    # we rank by robust percentile spread instead of variance.
    select_exprs = []
    for c in numeric_cols:
        base_expr = feature_sql(c)
        val = f"CAST(({base_expr}) AS DOUBLE)"
        # Filter NaN/Inf and extreme magnitudes to keep quantiles stable
        val_clean = f"CASE WHEN {val} IS NOT NULL AND {val} = {val} AND abs({val}) < 1e308 THEN {val} ELSE NULL END"
        select_exprs.append(f"approx_quantile({val_clean}, 0.02) AS {q(c + '__q02')}")
        select_exprs.append(f"approx_quantile({val_clean}, 0.98) AS {q(c + '__q98')}")

    query = f"""
        SELECT
            {", ".join(select_exprs)}
        FROM {panel_read_expr()}
        USING SAMPLE {sample_pct}% ({method})
    """
    row = con.execute(query).fetchone()
    if row is None:
        return []
    colnames = [d[0] for d in con.description]
    stats_map = dict(zip(colnames, row))

    records = []
    for c in numeric_cols:
        q02 = stats_map.get(c + "__q02")
        q98 = stats_map.get(c + "__q98")
        spread = (float(q98) - float(q02)) if (q02 is not None and q98 is not None and not np.isnan(q02) and not np.isnan(q98)) else np.nan
        records.append({"feature": c, "q02": q02, "q98": q98, "spread_02_98": spread})
    stats = pd.DataFrame.from_records(records)
    stats["is_flat"] = stats.apply(lambda r: _is_near_flat(None, r["q02"], r["q98"]), axis=1)
    stats = stats[~stats["is_flat"]].copy()
    stats = stats.sort_values("spread_02_98", ascending=False)
    return stats["feature"].head(max_features).tolist()

def _auto_scale_transform(values: np.ndarray, skew_thresh: float = 2.0):
    """
    Decide scaling mode for plotting feature distributions.
    Returns (mode, forward, inverse, formatter) where mode in {'linear','log1p','symlog'}.
    """
    s = pd.Series(values).dropna()
    if s.empty:
        return "linear", (lambda x: x), (lambda x: x), None
    skew = float(s.skew())
    if abs(skew) < float(skew_thresh):
        return "linear", (lambda x: x), (lambda x: x), None

    vmin = float(s.min())
    if vmin >= 0:
        fwd = np.log1p
        inv = np.expm1
        fmt = FuncFormatter(lambda t, pos: f"{inv(t):.3g}")
        return "log1p", fwd, inv, fmt
    # Mixed-sign: prefer symlog on axis (keeps units)
    return "symlog", (lambda x: x), (lambda x: x), None

def plot_class_separation(con, label_col: str, numeric_cols: List[str]):
    if not label_col:
        return None
    print("Creating class separation plot...")
    method = resolve_sample_method()
    sample_pct = float(os.environ.get("PANEL_VIS_CLASSSEP_SAMPLE_PCT", "1.0"))
    skew_thresh = float(os.environ.get("PANEL_VIS_SKEW_THRESH", "2.0"))
    scale_mode = (os.environ.get("PANEL_VIS_SCALE_MODE") or "auto").lower()

    feats = _select_top_variance_features(con, numeric_cols, max_features=4, sample_pct=sample_pct, method=method)
    if not feats:
        print("Class separation: no non-flat numeric features found; skipping.")
        return None

    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    axes = axes.ravel()
    for i, feat in enumerate(feats):
        ax = axes[i]
        feat_expr = feature_sql(feat)
        df = con.execute(
            f"""
            SELECT {q(label_col)} AS label, {feat_expr} AS val
            FROM {panel_read_expr()}
            WHERE {q(label_col)} IS NOT NULL AND {feat_expr} IS NOT NULL
            USING SAMPLE {sample_pct}% ({method})
            LIMIT 350000
            """
        ).df()
        if df.empty:
            ax.set_title(f"{feat} (EMPTY)", color="red")
            continue

        vals = df["val"].to_numpy(dtype=float, copy=False)
        mode, fwd, inv, fmt = _auto_scale_transform(vals, skew_thresh=skew_thresh) if scale_mode == "auto" else ("linear", (lambda x: x), (lambda x: x), None)
        df["val_plot"] = fwd(df["val"].astype(float))

        sns.histplot(
            data=df,
            x="val_plot",
            hue="label",
            stat="density",
            common_norm=False,
            element="step",
            fill=False,
            bins=60,
            ax=ax,
        )
        title_suffix = f" ({mode})" if mode != "linear" else ""
        ax.set_title(f"{feat}{title_suffix}")
        ax.set_ylabel("Density")

        # Keep readable axis labels in original units (especially for log1p)
        ax.set_xlabel(feat)
        if mode == "log1p" and fmt is not None:
            ax.xaxis.set_major_formatter(fmt)
            ax.set_xlabel(f"{feat} (ticks in original units)")
        elif mode == "symlog":
            # symlog keeps original units; only tweak scaling if needed
            linthresh = float(os.environ.get("PANEL_VIS_SYMLOG_LINTHRESH", "1.0"))
            ax.set_xscale("symlog", linthresh=linthresh)

    # Hide unused panels if we got <4 features
    for j in range(len(feats), 4):
        axes[j].axis("off")

    plt.suptitle("Class Separation (top non-flat variance features)", fontsize=16, fontweight="bold")
    out = OUTPUT_DIR / "class_separation.png"
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.savefig(out, dpi=300)
    plt.close()
    wb_log_image("plots/class_separation", out, "Class separation distributions (top variance non-flat features)")
    return out

# ==========================================================================================
# CORRELATION MATRIX
# ==========================================================================================
def _cluster_order_features(corr: pd.DataFrame) -> List[str]:
    """Return feature order using hierarchical clustering on 1-|corr| distance."""
    try:
        from scipy.cluster.hierarchy import linkage, leaves_list
        from scipy.spatial.distance import squareform
    except Exception:
        return list(corr.columns)

    c = corr.fillna(0.0).to_numpy()
    dist = 1.0 - np.abs(c)
    np.fill_diagonal(dist, 0.0)
    d = squareform(dist, checks=False)
    Z = linkage(d, method="average")
    order = leaves_list(Z)
    return list(corr.columns[order])

def plot_correlation_matrix(con, numeric_cols: List[str], max_features: int = 60):
    print("Creating correlation matrix...")
    method = resolve_sample_method()
    sample_pct = float(os.environ.get("PANEL_VIS_CORR_SAMPLE_PCT", "0.6"))
    max_features = int(os.environ.get("PANEL_VIS_CORR_MAX_FEATURES", str(max_features)))

    # Downselect to informative, non-flat features for readability
    feats = _select_top_variance_features(con, numeric_cols, max_features=max_features, sample_pct=max(sample_pct, 0.5), method=method)
    if len(feats) < 3:
        print("Correlation matrix: insufficient non-flat features found; skipping.")
        return None

    sel = ", ".join(f"{feature_sql(c)} AS {q(c)}" for c in feats)
    df = con.execute(
        f"""
        SELECT {sel}
        FROM {panel_read_expr()}
        USING SAMPLE {sample_pct}% ({method})
        LIMIT 250000
        """
    ).df()
    if df.empty:
        return None

    corr = df.corr(numeric_only=True).astype(float)
    order = _cluster_order_features(corr)
    corr = corr.loc[order, order]

    fig, ax = plt.subplots(figsize=(13, 11))
    cmap = _neutral_diverging_cmap()
    sns.heatmap(
        corr,
        cmap=cmap,
        center=0.0,
        vmin=-1.0,
        vmax=1.0,
        square=True,
        linewidths=0.0,
        cbar_kws={"label": "Correlation (Pearson, centered at 0)"},
        ax=ax,
    )
    ax.set_title("Correlation matrix (cluster-reordered)", fontsize=14, fontweight="bold")
    out = OUTPUT_DIR / "correlation_matrix.png"
    plt.tight_layout()
    plt.savefig(out, dpi=300)
    plt.close()
    wb_log_image("plots/correlation_matrix", out, "Cluster-reordered correlation matrix (neutral midpoint at 0)")
    return out

# ==========================================================================================
# MAIN EXECUTION
# ==========================================================================================
def main():
    print(f"Starting diagnostics for: {PANEL_SPEC}")
    print(f"Absolute Output: {OUTPUT_DIR.resolve()}")
    print(f"Sampling method (PANEL_VIS_SAMPLE_METHOD): {resolve_sample_method()}")
    
    con = get_con()
    
    # 1. Resolve Label
    schema = con.execute(f"DESCRIBE SELECT * FROM {panel_read_expr()} LIMIT 0").df()
    cols = schema['column_name'].tolist()
    label_col = os.environ.get("LABEL_COL") or ("transition_01" if "transition_01" in cols else None)

    # 1b. dist_wdpa sanity checks + plot-only recode (optional/auto)
    _configure_dist_wdpa_zero_recode(con, cols, label_col)

    # 2. Start W&B
    use_wandb = bool(int(os.environ.get("PANEL_VIS_WANDB", "0")))
    if wandb and use_wandb:
        try:
            wandb.init(project="panel_visualisation", name=f"diag_{datetime.now().strftime('%m%d_%H%M')}")
        except Exception as e:
            print(f"W&B init failed; continuing without W&B. Error: {e}")
            # Keep wandb imported, but no active run means wb_log_* become no-ops.

    # 3. Numeric columns for feature-focused plots
    coord_x, coord_y = resolve_coord_cols(cols)
    exclude = {'year', coord_x, coord_y, 'row', 'col', 'x', 'y', label_col}
    numeric_cols = [
        r["column_name"]
        for _, r in schema.iterrows()
        if (r["column_name"] not in exclude) and _is_numeric_duckdb_type(str(r.get("column_type", "")))
    ]

    # 4. Class separation + correlation structure
    plot_class_separation(con, label_col, numeric_cols)
    plot_correlation_matrix(con, numeric_cols)

    # 5. Temporal integrity comparison
    if numeric_cols:
        if "elevation_b1" in numeric_cols:
            target_feat = "elevation_b1"
        else:
            picked = _select_top_variance_features(con, numeric_cols, max_features=1, sample_pct=0.5, method=resolve_sample_method())
            target_feat = picked[0] if picked else numeric_cols[0]
        plot_temporal_feature_comparison(con, target_feat)

    # 6. Standard plots
    plot_time_series(con, label_col)
    plot_spatial_map(con, label_col)

    # 7. Log outputs as W&B artifacts for lineage
    wb_log_artifact_pngs(OUTPUT_DIR)
    
    print("="*50)
    print("DIAGNOSTICS COMPLETE")
    print("="*50)
    if wandb and wandb.run:
        wandb.finish()

if __name__ == "__main__":
    main()