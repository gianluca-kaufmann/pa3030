#!/usr/bin/env python3
"""
Panel dataset diagnostic visualizations & Data Integrity Debugger.

Added Features:
- Temporal Feature Comparison: Subplots comparing geography of 2011 vs 2013 vs 2015.
- Robust Data Integrity Scan: Automated detection of row-count crashes.
"""

import os
import socket
import getpass
import warnings
from datetime import datetime
from pathlib import Path
from typing import Optional, List
import glob

import duckdb
import matplotlib
matplotlib.use("Agg")  # Headless-safe
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

try:
    import wandb
except ImportError:
    wandb = None

warnings.filterwarnings('ignore')

# ==========================================================================================
# PATH SETUP
# ==========================================================================================
ROOT_DIR = Path(__file__).resolve().parents[3]
OUTPUT_DIR = ROOT_DIR / "outputs" / "Figures" / "panel_visualisation"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

SCRATCH = os.environ.get("SCRATCH")

def _sql_escape_string_literal(value: str) -> str:
    """Escape a string for use inside single-quoted SQL string literals."""
    return value.replace("'", "''")

def _resolve_panel_path_spec_to_abs(spec: str) -> str:
    """
    Resolve PANEL_FILE spec to an absolute path/pattern relative to repo root.

    - If spec is absolute: keep as-is.
    - If spec is relative (incl. './...'): interpret relative to ROOT_DIR.
    """
    spec = spec.strip()
    if not spec:
        return spec
    p = Path(spec)
    if p.is_absolute():
        return str(p)
    # Interpret relative paths as relative to repo root for reproducibility
    return str((ROOT_DIR / p).resolve())

def resolve_panel_spec() -> str:
    """
    Decide what parquet path/pattern DuckDB should read.

    Supports:
    - A single file path
    - A directory (reads '*.parquet' within it)
    - A glob pattern (e.g. '**/*scored*.parquet')
    """
    env_spec = (os.environ.get("PANEL_FILE") or "").strip()
    if env_spec:
        abs_spec = _resolve_panel_path_spec_to_abs(env_spec)
        # If it's a directory, turn into a parquet glob
        if Path(abs_spec).exists() and Path(abs_spec).is_dir():
            abs_spec = str(Path(abs_spec) / "*.parquet")
        # If it looks like a glob, ensure it matches something (best-effort)
        if any(ch in abs_spec for ch in ["*", "?", "["]):
            matches = glob.glob(abs_spec, recursive=True)
            if len(matches) == 0:
                raise FileNotFoundError(f"PANEL_FILE glob matched 0 files: {abs_spec}")
        else:
            if not Path(abs_spec).exists():
                raise FileNotFoundError(f"PANEL_FILE not found: {abs_spec}")
        return abs_spec

    # Fallbacks (Euler scratch first, then repo)
    candidate_paths = [
        Path(SCRATCH) / "data" / "ml" / "merged_panel_final.parquet" if SCRATCH else None,
        Path(SCRATCH) / "data" / "ml" / "merged_panel_2000_2024.parquet" if SCRATCH else None,
        ROOT_DIR / "data" / "ml" / "merged_panel_final.parquet",
        ROOT_DIR / "data" / "ml" / "merged_panel_2000_2024.parquet",
    ]
    panel_file = next((p for p in candidate_paths if p and p.exists()), None)
    if not panel_file:
        raise FileNotFoundError(f"Parquet not found. Searched: {[str(p) for p in candidate_paths if p]}")
    return str(panel_file)

PANEL_SPEC = resolve_panel_spec()

# ==========================================================================================
# HELPERS
# ==========================================================================================
def q(name: str) -> str:
    """Standard DuckDB identifier quoting."""
    # Escape embedded quotes per SQL identifier rules: " -> ""
    return '"' + name.replace('"', '""') + '"'

def resolve_sample_method() -> str:
    raw = (os.environ.get("PANEL_VIS_SAMPLE_METHOD") or "system").lower()
    return "bernoulli" if "bern" in raw else "system"

def panel_read_expr() -> str:
    """
    DuckDB relation expression for the panel parquet(s).

    Key behavior:
    - Always uses union_by_name=true to prevent schema-mismatch crashes when PANEL_FILE is a glob.
    """
    escaped = _sql_escape_string_literal(PANEL_SPEC)
    return f"read_parquet('{escaped}', union_by_name=true)"

def wb_log(data: dict):
    if wandb and wandb.run: wandb.log(data)

def wb_log_image(key: str, path: Path, caption: str):
    if wandb and wandb.run:
        try:
            wandb.log({key: wandb.Image(str(path), caption=caption)})
        except Exception as e:
            print(f"W&B Image Log Error: {e}")

# ==========================================================================================
# DUCKDB CONFIGURATION
# ==========================================================================================
def get_con():
    con = duckdb.connect()
    is_euler = bool(os.environ.get("SCRATCH"))
    cpus = int(os.environ.get("SLURM_CPUS_PER_TASK", "0"))
    threads = cpus if cpus > 0 else (48 if is_euler else 4)
    
    mem_mb = os.environ.get("SLURM_MEM_PER_CPU")
    limit_gb = (int(mem_mb) * cpus // 1024) if (mem_mb and cpus) else (128 if is_euler else 16)
    
    con.execute(f"SET threads={threads}")
    con.execute(f"SET memory_limit='{limit_gb}GB'")
    
    tmp = Path(os.environ.get("SCRATCH", ROOT_DIR)) / "duckdb_temp"
    tmp.mkdir(parents=True, exist_ok=True)
    con.execute(f"SET temp_directory='{tmp}'")
    return con

# ==========================================================================================
# TEMPORAL FEATURE COMPARISON (DEBUGGER)
# ==========================================================================================
def plot_temporal_feature_comparison(con, feature: str, years=[2011, 2013, 2015]):
    """Compare geography of a feature across 2011 (good), 2013 (crash), and 2015 (good)."""
    print(f"Creating Temporal Comparison for {feature}...")
    sample_pct = 2.0
    method = resolve_sample_method()
    
    # 1. Get global scale for comparability
    scale_query = f"""
    SELECT min({q(feature)}) as vmin, max({q(feature)}) as vmax 
    FROM {panel_read_expr()} 
    USING SAMPLE 1% ({method})
    """
    stats = con.execute(scale_query).fetchone()
    # Apply 1/99 clipping logic
    clip_query = f"SELECT approx_quantile({q(feature)}, 0.01), approx_quantile({q(feature)}, 0.99) FROM {panel_read_expr()}"
    vmin, vmax = con.execute(clip_query).fetchone()

    fig, axes = plt.subplots(1, 3, figsize=(20, 6), sharex=True, sharey=True)
    
    for i, year in enumerate(years):
        df = con.execute(f"""
            SELECT row, col, {q(feature)} as val 
            FROM {panel_read_expr()} 
            WHERE year = {year} AND {q(feature)} IS NOT NULL
            USING SAMPLE {sample_pct}% ({method}) 
            LIMIT 300000
        """).df()
        
        ax = axes[i]
        if not df.empty:
            sc = ax.scatter(df['col'], df['row'], c=df['val'], s=0.1, 
                            cmap='viridis', vmin=vmin, vmax=vmax, rasterized=True)
            ax.set_title(f"Year {year} (n={len(df):,})", fontsize=12)
        else:
            ax.set_title(f"Year {year} (EMPTY)", color='red')
            
        ax.set_aspect('equal')

    plt.suptitle(f"Temporal Integrity Check: {feature} (2011 vs 2013 vs 2015)", fontsize=15, fontweight='bold')
    fig.subplots_adjust(right=0.92)
    cbar_ax = fig.add_axes([0.94, 0.15, 0.015, 0.7])
    fig.colorbar(plt.cm.ScalarMappable(norm=plt.Normalize(vmin, vmax), cmap='viridis'), cax=cbar_ax, label=feature)
    
    out_path = OUTPUT_DIR / f"integrity_temporal_comparison_{feature}.png"
    plt.savefig(out_path, dpi=300, bbox_inches='tight')
    plt.close()
    wb_log_image(f"plots/temporal_comp_{feature}", out_path, f"Temporal comparison for {feature}")

# ==========================================================================================
# STANDARD PLOTS (OPTIMIZED)
# ==========================================================================================
def plot_time_series(con, label_col):
    print("Creating time series...")
    query = f"""
        SELECT year, COUNT(*) AS rows, 
        {f"SUM(CASE WHEN {q(label_col)} = 1 THEN 1 ELSE 0 END)" if label_col else "0"} AS pos
        FROM {panel_read_expr()} GROUP BY year ORDER BY year
    """
    df = con.execute(query).df()
    fig, ax1 = plt.subplots(figsize=(12, 6))
    ax1.plot(df['year'], df['rows'], color='tab:blue', marker='o', label='Total Rows')
    ax1.set_ylabel('Risk Set Size', color='tab:blue')
    if label_col:
        ax2 = ax1.twinx()
        ax2.bar(df['year'], df['pos'], color='tab:orange', alpha=0.4, label='Positives')
        ax2.set_ylabel('Positives', color='tab:orange')
    plt.title(f"Temporal Distribution (Highlighting 2012-2014 collapse)")
    out = OUTPUT_DIR / "time_series.png"
    plt.savefig(out, dpi=300); plt.close()
    return out

def plot_spatial_map(con, label_col):
    if not label_col: return None
    print("Creating spatial map...")
    # Pull bg and pos separately to ensure context
    bg = con.execute(f"SELECT row, col FROM {panel_read_expr()} USING SAMPLE 0.5% LIMIT 200000").df()
    pos = con.execute(f"SELECT row, col FROM {panel_read_expr()} WHERE {q(label_col)}=1 USING SAMPLE 5%").df()
    
    fig, ax = plt.subplots(figsize=(12, 10))
    ax.scatter(bg['col'], bg['row'], s=0.05, c='lightgray', alpha=0.1, rasterized=True)
    ax.scatter(pos['col'], pos['row'], s=0.5, c='red', alpha=0.5, rasterized=True)
    ax.set_aspect('equal')
    ax.set_title(f"Spatial Distribution of Positives ({label_col}=1)")
    out = OUTPUT_DIR / "spatial_map.png"
    plt.savefig(out, dpi=300); plt.close()
    return out

# ==========================================================================================
# MAIN EXECUTION
# ==========================================================================================
def main():
    print(f"Starting diagnostics for: {PANEL_SPEC}")
    print(f"Absolute Output: {OUTPUT_DIR.resolve()}")
    
    con = get_con()
    
    # 1. Resolve Label
    schema = con.execute(f"DESCRIBE SELECT * FROM {panel_read_expr()} LIMIT 0").df()
    cols = schema['column_name'].tolist()
    label_col = os.environ.get("LABEL_COL") or ("transition_01" if "transition_01" in cols else None)

    # 2. Start W&B
    if wandb:
        wandb.init(project="panel_visualisation", name=f"diag_{datetime.now().strftime('%m%d_%H%M')}")

    # 3. RUN NEW TEMPORAL COMPARISONS
    # Find feature with high variance to use for mapping
    numeric_cols = [c for c in cols if c not in ['year', 'x', 'y', 'row', 'col', label_col]]
    if numeric_cols:
        target_feat = "elevation_b1" if "elevation_b1" in numeric_cols else numeric_cols[0]
        plot_temporal_feature_comparison(con, target_feat)

    # 4. RUN STANDARD PLOTS
    plot_time_series(con, label_col)
    plot_spatial_map(con, label_col)
    
    print("="*50)
    print("DIAGNOSTICS COMPLETE")
    print("="*50)
    if wandb: wandb.finish()

if __name__ == "__main__":
    main()