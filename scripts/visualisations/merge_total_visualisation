#!/usr/bin/env python3
"""
Generate 2D maps from the merged panel Parquet file without loading everything into memory.

This script:
1. Reads the parquet file in chunks (row groups) to avoid loading everything
2. Automatically finds representative tiles for each requested year
3. Visualizes predefined band groups across multiple years
4. Reshapes the data into 2D grids based on row/col coordinates
5. Saves figures per year showing the requested bands

Usage:
python scripts/visualisations/merge_total_visualisation
"""

import numpy as np
import pyarrow.parquet as pq
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from pathlib import Path
import warnings
from typing import Optional, List, Dict, Tuple
import pandas as pd

warnings.filterwarnings('ignore')

# Paths
ROOT_DIR = Path(__file__).resolve().parents[2]
DATA_ML = ROOT_DIR / "data" / "ml"
OUTPUT_RESULTS = ROOT_DIR / "outputs" / "Results"
OUTPUT_DIR = ROOT_DIR / "outputs" / "Figures" / "merge_total_vis"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Hardcoded parameters for random sample check
TILE_SIZE = 10000  # Size of square tile to visualize

# Yearly visualization requests (year → bands)
YEARLY_VIS_REQUESTS = [
    {'year': 2024, 'bands': ['WDPA', 'NDVI']},
    {'year': 2022, 'bands': ['road infrastructure', 'HNTL']},
    {'year': 2018, 'bands': ['GPW', 'deforestation']},
    {'year': 2016, 'bands': ['wildfires', 'gdp']},
    {'year': 2012, 'bands': ['landcover', 'elevation']},
    {'year': 2008, 'bands': [f'GSN_b{i}' for i in range(1, 6)]},
    {'year': 2004, 'bands': ['Oil and Gas', 'Powerplants']},
    {'year': 2000, 'bands': [f'WorldClim_b{i}' for i in range(1, 20)]},
    {'year': 2000, 'bands': ['HNTL', 'gdp']},  # sanity-check request (expected NaNs)
]
BAND_ALIASES = {
}

# Custom palettes
LANDCOVER_COLORMAP = ListedColormap([
    '#2f4858', '#33658a', '#55a6d9', '#86bbd8', '#f6ae2d',
    '#f26419', '#a4243b', '#4f772d', '#90be6d', '#e3d26f',
    '#bc6c25', '#8ecae6', '#ffb703', '#fb8500', '#6a994e'
])

# Mapping from requested band names to possible column name patterns
BAND_ALIASES = {
    'night_lights': ['HNTL', 'hntl'],
    'HNTL': ['HNTL', 'hntl'],
    'GSN_1': ['GSN_b1', 'GSN', 'gsn_b1', 'gsn'],  # Try numbered version first
    'road infrastructure': ['road_infrastructure', 'road_infrastructure_b1'],
    'wildfires': ['wildfire', 'wildfires'],
    'Oil and Gas': ['oil_gas', 'oil_gas_b1'],
    'Powerplants': ['powerplants', 'powerplants_b1'],
    'GPW': ['gpw', 'GPW'],
    'WorldClim': ['WorldClim', 'worldclim'],
}


def find_parquet_file() -> Path:
    """Find the merged panel parquet file."""
    for location in [DATA_ML, OUTPUT_RESULTS]:
        if location.exists():
            files = list(location.glob("merged_panel_*.parquet"))
            if files:
                return files[0]
    raise FileNotFoundError("No merged panel parquet file found")


def discover_available_years(pfile: pq.ParquetFile) -> List[int]:
    """Discover which years are available in the parquet file."""
    print("Discovering available years...")
    years_set = set()
    num_row_groups = pfile.metadata.num_row_groups
    
    # Sample row groups to find years (don't need to read all)
    sample_indices = list(range(0, num_row_groups, max(1, num_row_groups // 20)))  # Sample ~20 row groups
    sample_indices.append(num_row_groups - 1)  # Always check last one
    
    for rg_idx in sample_indices:
        try:
            table = pfile.read_row_group(rg_idx, columns=['year'])
            df = table.to_pandas()
            years_set.update(df['year'].unique())
        except Exception:
            continue
    
    years = sorted([int(y) for y in years_set if pd.notna(y)])
    print(f"  Found years: {years}")
    return years


def find_valid_tile_location(
    pfile: pq.ParquetFile,
    year: int,
    tile_size: int,
    max_samples: int = 50
) -> Tuple[int, int]:
    """
    Find a valid tile location (row_start, col_start) that has data for the given year.
    
    Returns:
        (row_start, col_start) tuple
    """
    print(f"Finding valid tile location for year {year}...")
    columns_to_read = ['year', 'row', 'col']
    num_row_groups = pfile.metadata.num_row_groups
    
    # Sample row groups to find data
    sample_indices = list(range(0, num_row_groups, max(1, num_row_groups // max_samples)))
    
    all_rows = []
    all_cols = []
    
    for rg_idx in sample_indices:
        try:
            table = pfile.read_row_group(rg_idx, columns=columns_to_read)
            df = table.to_pandas()
            
            # Filter by year
            year_data = df[df['year'] == year]
            
            if len(year_data) > 0:
                all_rows.extend(year_data['row'].tolist())
                all_cols.extend(year_data['col'].tolist())
        except Exception:
            continue
    
    if not all_rows:
        raise ValueError(f"No data found for year {year} in sampled row groups")
    
    # Find a good starting point (use median to get a central location)
    row_start = int(np.median(all_rows)) - tile_size // 2
    col_start = int(np.median(all_cols)) - tile_size // 2
    
    # Ensure non-negative
    row_start = max(0, row_start)
    col_start = max(0, col_start)
    
    print(f"  Found data at rows [{min(all_rows)}:{max(all_rows)}], cols [{min(all_cols)}:{max(all_cols)}]")
    print(f"  Selected tile location: row_start={row_start}, col_start={col_start}")
    
    return row_start, col_start


def get_available_bands(pfile: pq.ParquetFile) -> List[str]:
    """Get list of available band columns from the parquet file."""
    schema = pfile.schema
    columns = [schema[i].name for i in range(len(schema))]
    # Filter out metadata columns
    metadata_cols = {'year', 'x', 'y', 'row', 'col'}
    band_cols = [c for c in columns if c not in metadata_cols]
    return band_cols


def find_band_column(band_name: str, available_bands: List[str]) -> Optional[str]:
    """Find the actual column name for a requested band."""
    # First, try direct matching (exact, case-insensitive, partial)
    if band_name in available_bands:
        return band_name
    
    for col in available_bands:
        if col.lower() == band_name.lower():
            return col
    
    # Try partial match - band name at start of column name
    # (e.g., 'NDVI' matches 'NDVI_b1', 'landcover' matches 'landcover_b1')
    for col in available_bands:
        col_lower = col.lower()
        name_lower = band_name.lower()
        if col_lower.startswith(name_lower) or name_lower in col_lower:
            return col
    
        # Check aliases (e.g., 'night_lights' -> 'HNTL', 'GSN_1' -> 'GSN')
    if band_name in BAND_ALIASES:
        for alias_pattern in BAND_ALIASES[band_name]:
            # Try exact match with alias
            if alias_pattern in available_bands:
                return alias_pattern
            # Try case-insensitive exact match
            for col in available_bands:
                if col.lower() == alias_pattern.lower():
                    return col
            # Try partial match - alias pattern at start of column name
            # (e.g., 'HNTL' matches 'HNTL_b1', 'HNTL_b2', etc.)
            for col in available_bands:
                col_lower = col.lower()
                pattern_lower = alias_pattern.lower()
                if col_lower.startswith(pattern_lower) or pattern_lower in col_lower:
                    return col
    
    return None


def read_tile_from_parquet(
    pfile: pq.ParquetFile,
    row_start: int,
    col_start: int,
    tile_size: int,
    year: int,
    band_columns: List[str]
) -> Tuple[np.ndarray, Dict[str, np.ndarray], Dict[str, int]]:
    """
    Read a tile from the parquet file without loading everything into memory.
    
    Returns:
        - grid_shape: (height, width) of the tile
        - band_data: Dictionary mapping band names to 2D arrays
        - stats: Dictionary with row/col ranges and data statistics
    """
    row_end = row_start + tile_size
    col_end = col_start + tile_size
    
    print(f"Reading tile: rows [{row_start}:{row_end}], cols [{col_start}:{col_end}], year {year}")
    
    # Columns to read
    columns_to_read = ['year', 'row', 'col', 'x', 'y'] + band_columns
    
    # Read row groups in chunks and filter
    all_data = []
    num_row_groups = pfile.metadata.num_row_groups
    
    print(f"Scanning {num_row_groups} row groups...")
    rows_read = 0
    
    for rg_idx in range(num_row_groups):
        # Read row group
        table = pfile.read_row_group(rg_idx, columns=columns_to_read)
        
        # Convert to pandas for filtering (more efficient than Arrow for this)
        df = table.to_pandas()
        
        # Filter by year, row, and col
        mask = (
            (df['year'] == year) &
            (df['row'] >= row_start) &
            (df['row'] < row_end) &
            (df['col'] >= col_start) &
            (df['col'] < col_end)
        )
        
        filtered_df = df[mask]
        
        if len(filtered_df) > 0:
            all_data.append(filtered_df)
            rows_read += len(filtered_df)
        
        # Progress update every 100 row groups
        if (rg_idx + 1) % 100 == 0:
            print(f"  Processed {rg_idx + 1}/{num_row_groups} row groups, found {rows_read:,} matching rows")
    
    if not all_data:
        raise ValueError(f"No data found for year {year}, rows [{row_start}:{row_end}], cols [{col_start}:{col_end}]")
    
    # Combine all filtered data
    import pandas as pd
    df_tile = pd.concat(all_data, ignore_index=True)
    print(f"Total matching rows: {len(df_tile):,}")
    
    # Get actual row/col ranges in the data
    actual_row_min = df_tile['row'].min()
    actual_row_max = df_tile['row'].max()
    actual_col_min = df_tile['col'].min()
    actual_col_max = df_tile['col'].max()
    
    # Calculate grid dimensions
    row_range = int(actual_row_max - actual_row_min + 1)
    col_range = int(actual_col_max - actual_col_min + 1)
    
    print(f"Actual data range: rows [{actual_row_min}:{actual_row_max}], cols [{actual_col_min}:{actual_col_max}]")
    print(f"Grid dimensions: {row_range} x {col_range}")
    
    # Create 2D arrays for each band
    band_data = {}
    
    # Create coordinate mapping
    row_indices = df_tile['row'].values - actual_row_min
    col_indices = df_tile['col'].values - actual_col_min
    
    for band_col in band_columns:
        if band_col not in df_tile.columns:
            print(f"  Warning: Column '{band_col}' not found, skipping")
            continue
        
        # Initialize 2D array with NaN
        band_array = np.full((row_range, col_range), np.nan, dtype=np.float32)
        
        # Fill array using row/col indices
        values = df_tile[band_col].values.astype(np.float32)
        band_array[row_indices, col_indices] = values
        
        band_data[band_col] = band_array
        
        # Print statistics
        valid_count = np.sum(~np.isnan(band_array))
        valid_pct = 100 * valid_count / band_array.size
        mean_val = np.nanmean(band_array)
        print(f"  {band_col}: {valid_count:,} valid pixels ({valid_pct:.1f}%), mean={mean_val:.3f}")
    
    stats = {
        'row_min': int(actual_row_min),
        'row_max': int(actual_row_max),
        'col_min': int(actual_col_min),
        'col_max': int(actual_col_max),
        'height': row_range,
        'width': col_range,
        'year': year,
        'x_min': float(df_tile['x'].min()),
        'x_max': float(df_tile['x'].max()),
        'y_min': float(df_tile['y'].min()),
        'y_max': float(df_tile['y'].max()),
    }
    
    return (row_range, col_range), band_data, stats


def create_band_visualization(
    band_data: Dict[str, np.ndarray],
    stats: Dict[str, int],
    band_mapping: Dict[str, str],
    output_dir: Path
):
    """Create 2D map visualizations for requested bands.
    
    Args:
        band_data: Dictionary mapping column names to 2D arrays
        stats: Statistics dictionary
        band_mapping: Dictionary mapping requested band names to actual column names
        output_dir: Output directory for saving figures
    """
    requested_bands = list(band_mapping.keys())
    print(f"\nCreating visualizations for {len(requested_bands)} bands...")
    
    # Determine grid layout
    n_bands = len(requested_bands)
    n_cols = min(3, n_bands)
    n_rows = (n_bands + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 6 * n_rows))
    fig.suptitle(
        f'2D Maps - Year {stats["year"]} | Tile: rows [{stats["row_min"]}:{stats["row_max"]}], '
        f'cols [{stats["col_min"]}:{stats["col_max"]}]\n'
        f'Coordinates: ({stats["x_min"]:.2f}, {stats["y_min"]:.2f}) to ({stats["x_max"]:.2f}, {stats["y_max"]:.2f})',
        fontsize=12, fontweight='bold'
    )
    
    if n_bands == 1:
        axes = [axes]
    elif n_rows == 1:
        axes = axes if isinstance(axes, np.ndarray) else [axes]
    else:
        axes = axes.flatten()
    
    for idx, band_name in enumerate(requested_bands):
        ax = axes[idx]
        
        # Get the actual column name from mapping
        band_col = band_mapping.get(band_name)
        
        if band_col is None or band_col not in band_data:
            ax.text(0.5, 0.5, f'Band "{band_name}"\nnot found', 
                   ha='center', va='center', transform=ax.transAxes)
            ax.set_title(band_name, fontweight='bold')
            continue
        
        data = band_data[band_col]
        
        # Determine colormap and value range based on band type
        if 'NDVI' in band_col.upper() or 'ndvi' in band_col.lower():
            cmap = 'RdYlGn'
            # Check if data is in scaled format (MODIS NDVI: -2000 to 10000)
            # or standard format (-1 to 1)
            valid_data = data[~np.isnan(data)]
            if len(valid_data) > 0:
                data_max = np.nanmax(data)
                data_min = np.nanmin(data)
                # If values are large (likely scaled MODIS format), use data range
                if data_max > 10:
                    # MODIS NDVI format: use actual data range or percentiles
                    vmin = np.nanpercentile(data, 1)
                    vmax = np.nanpercentile(data, 99)
                    label = 'NDVI (scaled)'
                else:
                    # Standard NDVI format: -0.2 to 1.0
                    vmin, vmax = -0.2, 1.0
                    label = 'NDVI'
            else:
                vmin, vmax = -0.2, 1.0
                label = 'NDVI'
        elif 'elevation' in band_col.lower():
            cmap = 'terrain'
            vmin, vmax = np.nanpercentile(data, 1), np.nanpercentile(data, 99)
            label = 'Elevation (m)'
        elif 'hntl' in band_col.lower() or 'night' in band_col.lower():
            cmap = 'hot'
            vmin, vmax = 0, np.nanpercentile(data, 95)
            label = 'Night Lights'
        elif 'landcover' in band_col.lower():
            cmap = LANDCOVER_COLORMAP
            vmin, vmax = None, None
            label = 'Land Cover'
        elif 'gsn' in band_col.lower():
            cmap = 'viridis'
            vmin, vmax = np.nanpercentile(data, 1), np.nanpercentile(data, 99)
            label = 'GSN'
        elif 'wdpa' in band_col.lower():
            cmap = 'YlOrRd'
            vmin, vmax = 0, 1
            label = 'WDPA (Protected Areas)'
        else:
            cmap = 'viridis'
            vmin, vmax = np.nanpercentile(data, 1), np.nanpercentile(data, 99)
            label = band_col
        
        # Create visualization
        im = ax.imshow(data, cmap=cmap, aspect='auto', vmin=vmin, vmax=vmax, interpolation='nearest')
        ax.set_title(f'{band_name}\n({band_col})', fontweight='bold', fontsize=10)
        ax.set_xlabel('Column')
        ax.set_ylabel('Row')
        
        # Add colorbar
        cbar = plt.colorbar(im, ax=ax, shrink=0.8)
        cbar.set_label(label, rotation=270, labelpad=15)
        
        # Add statistics text
        valid_data = data[~np.isnan(data)]
        if len(valid_data) > 0:
            mean_val = np.nanmean(data)
            median_val = np.nanmedian(data)
            stats_text = f'Mean: {mean_val:.3f}\nMedian: {median_val:.3f}\nValid: {len(valid_data):,}'
            ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,
                   fontsize=8, verticalalignment='top',
                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))
    
    # Hide unused subplots
    for idx in range(n_bands, len(axes)):
        axes[idx].axis('off')
    
    plt.tight_layout()
    
    # Save figure
    output_file = output_dir / f"tile_maps_year{stats['year']}_r{stats['row_min']}_c{stats['col_min']}.png"
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"Saved: {output_file}")
    plt.close()


def main():
    """Main function - hardcoded parameters for random sample check."""
    print("=" * 80)
    print("2D MAP GENERATION FROM PARQUET FILE (Multi-Year Overview)")
    print("=" * 80)
    print(f"Tile size: {TILE_SIZE} × {TILE_SIZE} pixels")
    print("Yearly requests:")
    for req in YEARLY_VIS_REQUESTS:
        print(f"  - {req['year']}: {', '.join(req['bands'])}")
    print()
    
    # Find parquet file
    try:
        parquet_file = find_parquet_file()
        print(f"Found parquet file: {parquet_file}")
        file_size_gb = parquet_file.stat().st_size / (1024**3)
        print(f"File size: {file_size_gb:.2f} GB")
    except FileNotFoundError as e:
        print(f"ERROR: {e}")
        return
    
    # Open parquet file
    pfile = pq.ParquetFile(parquet_file)
    print(f"Row groups: {pfile.metadata.num_row_groups:,}")
    print(f"Total rows: {pfile.metadata.num_rows:,}")
    
    # Get available bands
    available_bands = get_available_bands(pfile)
    print(f"\nAvailable bands ({len(available_bands)}): {', '.join(available_bands[:10])}...")

    # Discover available years
    available_years = discover_available_years(pfile)
    if not available_years:
        print("ERROR: No years found in parquet file!")
        return
    
    tile_cache: Dict[int, Tuple[int, int]] = {}
    
    for request in YEARLY_VIS_REQUESTS:
        year = request['year']
        requested_bands = request['bands']
        
        print("\n" + "-" * 80)
        print(f"Processing year {year} with bands: {', '.join(requested_bands)}")
        print("-" * 80)
        
        if year not in available_years:
            print(f"  Skipping year {year}: not present in parquet file (available: {available_years})")
            continue
        
        # Find tile location (cache per year to avoid repeated scans)
        if year in tile_cache:
            row_start, col_start = tile_cache[year]
        else:
            try:
                row_start, col_start = find_valid_tile_location(pfile, year, TILE_SIZE)
                tile_cache[year] = (row_start, col_start)
            except ValueError as e:
                print(f"  Unable to find tile for year {year}: {e}")
                continue
        
        # Determine band columns
        band_columns = []
        band_mapping = {}
        for requested_band in requested_bands:
            found = find_band_column(requested_band, available_bands)
            if found:
                if found not in band_columns:
                    band_columns.append(found)
                band_mapping[requested_band] = found
                print(f"  '{requested_band}' -> '{found}'")
            else:
                print(f"  Warning: '{requested_band}' not found in available bands")
        
        if not band_columns:
            print(f"  Skipping year {year}: no matching bands found.")
            continue
        
        # Read tile data for this year
        try:
            _, band_data, stats = read_tile_from_parquet(
                pfile, row_start, col_start, TILE_SIZE, year, band_columns
            )
        except Exception as e:
            print(f"  ERROR reading tile for year {year}: {e}")
            import traceback
            traceback.print_exc()
            continue
        
        # Create visualization
        try:
            create_band_visualization(band_data, stats, band_mapping, OUTPUT_DIR)
        except Exception as e:
            print(f"  ERROR creating visualization for year {year}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    print("\n" + "=" * 80)
    print("MULTI-YEAR VISUALIZATION COMPLETE")
    print("=" * 80)
    print(f"Output directory: {OUTPUT_DIR}")


if __name__ == "__main__":
    main()

