#!/usr/bin/env python3
"""
Generate 2D maps from the merged panel Parquet file without loading everything into memory.

This script:
1. Reads the parquet file in chunks (row groups) to avoid loading everything
2. Filters by a hardcoded 500x500 pixel tile (random sample check)
3. Filters by a hardcoded year
4. Reshapes the data into a 2D grid based on row/col coordinates
5. Creates visualizations for selected bands (NDVI, landcover, elevation, VIIRS, GSN_1, WDPA)

Usage:
    python scripts/visualisations/merge_total_visualisation
"""

import numpy as np
import pyarrow.parquet as pq
import matplotlib.pyplot as plt
from pathlib import Path
import warnings
from typing import Optional, List, Dict, Tuple
import pandas as pd

warnings.filterwarnings('ignore')

# Paths
ROOT_DIR = Path(__file__).resolve().parents[2]
DATA_ML = ROOT_DIR / "data" / "ml"
OUTPUT_RESULTS = ROOT_DIR / "outputs" / "Results"
OUTPUT_DIR = ROOT_DIR / "outputs" / "Figures" / "merge_total_vis"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Hardcoded parameters for random sample check
TILE_SIZE = 10000
YEAR = None  # Will be auto-detected if None
ROW_START = None  # Will be auto-detected if None
COL_START = None  # Will be auto-detected if None

# Bands to visualize
REQUESTED_BANDS = ['NDVI', 'landcover', 'elevation', 'night_lights', 'GSN_1', 'WDPA']

# Mapping from requested band names to possible column name patterns
BAND_ALIASES = {
    'night_lights': ['VIIRS', 'viirs'],
    'GSN_1': ['GSN_b1', 'GSN', 'gsn_b1', 'gsn'],  # Try numbered version first
}


def find_parquet_file() -> Path:
    """Find the merged panel parquet file."""
    for location in [DATA_ML, OUTPUT_RESULTS]:
        if location.exists():
            files = list(location.glob("merged_panel_*.parquet"))
            if files:
                return files[0]
    raise FileNotFoundError("No merged panel parquet file found")


def discover_available_years(pfile: pq.ParquetFile) -> List[int]:
    """Discover which years are available in the parquet file."""
    print("Discovering available years...")
    years_set = set()
    num_row_groups = pfile.metadata.num_row_groups
    
    # Sample row groups to find years (don't need to read all)
    sample_indices = list(range(0, num_row_groups, max(1, num_row_groups // 20)))  # Sample ~20 row groups
    sample_indices.append(num_row_groups - 1)  # Always check last one
    
    for rg_idx in sample_indices:
        try:
            table = pfile.read_row_group(rg_idx, columns=['year'])
            df = table.to_pandas()
            years_set.update(df['year'].unique())
        except Exception:
            continue
    
    years = sorted([int(y) for y in years_set if pd.notna(y)])
    print(f"  Found years: {years}")
    return years


def find_valid_tile_location(
    pfile: pq.ParquetFile,
    year: int,
    tile_size: int,
    max_samples: int = 50
) -> Tuple[int, int]:
    """
    Find a valid tile location (row_start, col_start) that has data for the given year.
    
    Returns:
        (row_start, col_start) tuple
    """
    print(f"Finding valid tile location for year {year}...")
    columns_to_read = ['year', 'row', 'col']
    num_row_groups = pfile.metadata.num_row_groups
    
    # Sample row groups to find data
    sample_indices = list(range(0, num_row_groups, max(1, num_row_groups // max_samples)))
    
    all_rows = []
    all_cols = []
    
    for rg_idx in sample_indices:
        try:
            table = pfile.read_row_group(rg_idx, columns=columns_to_read)
            df = table.to_pandas()
            
            # Filter by year
            year_data = df[df['year'] == year]
            
            if len(year_data) > 0:
                all_rows.extend(year_data['row'].tolist())
                all_cols.extend(year_data['col'].tolist())
        except Exception:
            continue
    
    if not all_rows:
        raise ValueError(f"No data found for year {year} in sampled row groups")
    
    # Find a good starting point (use median to get a central location)
    row_start = int(np.median(all_rows)) - tile_size // 2
    col_start = int(np.median(all_cols)) - tile_size // 2
    
    # Ensure non-negative
    row_start = max(0, row_start)
    col_start = max(0, col_start)
    
    print(f"  Found data at rows [{min(all_rows)}:{max(all_rows)}], cols [{min(all_cols)}:{max(all_cols)}]")
    print(f"  Selected tile location: row_start={row_start}, col_start={col_start}")
    
    return row_start, col_start


def get_available_bands(pfile: pq.ParquetFile) -> List[str]:
    """Get list of available band columns from the parquet file."""
    schema = pfile.schema
    columns = [schema[i].name for i in range(len(schema))]
    # Filter out metadata columns
    metadata_cols = {'year', 'x', 'y', 'row', 'col'}
    band_cols = [c for c in columns if c not in metadata_cols]
    return band_cols


def find_band_column(band_name: str, available_bands: List[str]) -> Optional[str]:
    """Find the actual column name for a requested band."""
    # First, try direct matching (exact, case-insensitive, partial)
    if band_name in available_bands:
        return band_name
    
    for col in available_bands:
        if col.lower() == band_name.lower():
            return col
    
    # Try partial match - band name at start of column name
    # (e.g., 'NDVI' matches 'NDVI_b1', 'landcover' matches 'landcover_b1')
    for col in available_bands:
        col_lower = col.lower()
        name_lower = band_name.lower()
        if col_lower.startswith(name_lower) or name_lower in col_lower:
            return col
    
    # Check aliases (e.g., 'night_lights' -> 'VIIRS', 'GSN_1' -> 'GSN')
    if band_name in BAND_ALIASES:
        for alias_pattern in BAND_ALIASES[band_name]:
            # Try exact match with alias
            if alias_pattern in available_bands:
                return alias_pattern
            # Try case-insensitive exact match
            for col in available_bands:
                if col.lower() == alias_pattern.lower():
                    return col
            # Try partial match - alias pattern at start of column name
            # (e.g., 'VIIRS' matches 'VIIRS_b1', 'VIIRS_b2', etc.)
            for col in available_bands:
                col_lower = col.lower()
                pattern_lower = alias_pattern.lower()
                if col_lower.startswith(pattern_lower) or pattern_lower in col_lower:
                    return col
    
    return None


def read_tile_from_parquet(
    pfile: pq.ParquetFile,
    row_start: int,
    col_start: int,
    tile_size: int,
    year: int,
    band_columns: List[str]
) -> Tuple[np.ndarray, Dict[str, np.ndarray], Dict[str, int]]:
    """
    Read a tile from the parquet file without loading everything into memory.
    
    Returns:
        - grid_shape: (height, width) of the tile
        - band_data: Dictionary mapping band names to 2D arrays
        - stats: Dictionary with row/col ranges and data statistics
    """
    row_end = row_start + tile_size
    col_end = col_start + tile_size
    
    print(f"Reading tile: rows [{row_start}:{row_end}], cols [{col_start}:{col_end}], year {year}")
    
    # Columns to read
    columns_to_read = ['year', 'row', 'col', 'x', 'y'] + band_columns
    
    # Read row groups in chunks and filter
    all_data = []
    num_row_groups = pfile.metadata.num_row_groups
    
    print(f"Scanning {num_row_groups} row groups...")
    rows_read = 0
    
    for rg_idx in range(num_row_groups):
        # Read row group
        table = pfile.read_row_group(rg_idx, columns=columns_to_read)
        
        # Convert to pandas for filtering (more efficient than Arrow for this)
        df = table.to_pandas()
        
        # Filter by year, row, and col
        mask = (
            (df['year'] == year) &
            (df['row'] >= row_start) &
            (df['row'] < row_end) &
            (df['col'] >= col_start) &
            (df['col'] < col_end)
        )
        
        filtered_df = df[mask]
        
        if len(filtered_df) > 0:
            all_data.append(filtered_df)
            rows_read += len(filtered_df)
        
        # Progress update every 100 row groups
        if (rg_idx + 1) % 100 == 0:
            print(f"  Processed {rg_idx + 1}/{num_row_groups} row groups, found {rows_read:,} matching rows")
    
    if not all_data:
        raise ValueError(f"No data found for year {year}, rows [{row_start}:{row_end}], cols [{col_start}:{col_end}]")
    
    # Combine all filtered data
    import pandas as pd
    df_tile = pd.concat(all_data, ignore_index=True)
    print(f"Total matching rows: {len(df_tile):,}")
    
    # Get actual row/col ranges in the data
    actual_row_min = df_tile['row'].min()
    actual_row_max = df_tile['row'].max()
    actual_col_min = df_tile['col'].min()
    actual_col_max = df_tile['col'].max()
    
    # Calculate grid dimensions
    row_range = int(actual_row_max - actual_row_min + 1)
    col_range = int(actual_col_max - actual_col_min + 1)
    
    print(f"Actual data range: rows [{actual_row_min}:{actual_row_max}], cols [{actual_col_min}:{actual_col_max}]")
    print(f"Grid dimensions: {row_range} x {col_range}")
    
    # Create 2D arrays for each band
    band_data = {}
    
    # Create coordinate mapping
    row_indices = df_tile['row'].values - actual_row_min
    col_indices = df_tile['col'].values - actual_col_min
    
    for band_col in band_columns:
        if band_col not in df_tile.columns:
            print(f"  Warning: Column '{band_col}' not found, skipping")
            continue
        
        # Initialize 2D array with NaN
        band_array = np.full((row_range, col_range), np.nan, dtype=np.float32)
        
        # Fill array using row/col indices
        values = df_tile[band_col].values.astype(np.float32)
        band_array[row_indices, col_indices] = values
        
        band_data[band_col] = band_array
        
        # Print statistics
        valid_count = np.sum(~np.isnan(band_array))
        valid_pct = 100 * valid_count / band_array.size
        mean_val = np.nanmean(band_array)
        print(f"  {band_col}: {valid_count:,} valid pixels ({valid_pct:.1f}%), mean={mean_val:.3f}")
    
    stats = {
        'row_min': int(actual_row_min),
        'row_max': int(actual_row_max),
        'col_min': int(actual_col_min),
        'col_max': int(actual_col_max),
        'height': row_range,
        'width': col_range,
        'year': year,
        'x_min': float(df_tile['x'].min()),
        'x_max': float(df_tile['x'].max()),
        'y_min': float(df_tile['y'].min()),
        'y_max': float(df_tile['y'].max()),
    }
    
    return (row_range, col_range), band_data, stats


def create_band_visualization(
    band_data: Dict[str, np.ndarray],
    stats: Dict[str, int],
    band_mapping: Dict[str, str],
    output_dir: Path
):
    """Create 2D map visualizations for requested bands.
    
    Args:
        band_data: Dictionary mapping column names to 2D arrays
        stats: Statistics dictionary
        band_mapping: Dictionary mapping requested band names to actual column names
        output_dir: Output directory for saving figures
    """
    requested_bands = list(band_mapping.keys())
    print(f"\nCreating visualizations for {len(requested_bands)} bands...")
    
    # Determine grid layout
    n_bands = len(requested_bands)
    n_cols = min(3, n_bands)
    n_rows = (n_bands + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 6 * n_rows))
    fig.suptitle(
        f'2D Maps - Year {stats["year"]} | Tile: rows [{stats["row_min"]}:{stats["row_max"]}], '
        f'cols [{stats["col_min"]}:{stats["col_max"]}]\n'
        f'Coordinates: ({stats["x_min"]:.2f}, {stats["y_min"]:.2f}) to ({stats["x_max"]:.2f}, {stats["y_max"]:.2f})',
        fontsize=12, fontweight='bold'
    )
    
    if n_bands == 1:
        axes = [axes]
    elif n_rows == 1:
        axes = axes if isinstance(axes, np.ndarray) else [axes]
    else:
        axes = axes.flatten()
    
    for idx, band_name in enumerate(requested_bands):
        ax = axes[idx]
        
        # Get the actual column name from mapping
        band_col = band_mapping.get(band_name)
        
        if band_col is None or band_col not in band_data:
            ax.text(0.5, 0.5, f'Band "{band_name}"\nnot found', 
                   ha='center', va='center', transform=ax.transAxes)
            ax.set_title(band_name, fontweight='bold')
            continue
        
        data = band_data[band_col]
        
        # Determine colormap and value range based on band type
        if 'NDVI' in band_col.upper() or 'ndvi' in band_col.lower():
            cmap = 'RdYlGn'
            # Check if data is in scaled format (MODIS NDVI: -2000 to 10000)
            # or standard format (-1 to 1)
            valid_data = data[~np.isnan(data)]
            if len(valid_data) > 0:
                data_max = np.nanmax(data)
                data_min = np.nanmin(data)
                # If values are large (likely scaled MODIS format), use data range
                if data_max > 10:
                    # MODIS NDVI format: use actual data range or percentiles
                    vmin = np.nanpercentile(data, 1)
                    vmax = np.nanpercentile(data, 99)
                    label = 'NDVI (scaled)'
                else:
                    # Standard NDVI format: -0.2 to 1.0
                    vmin, vmax = -0.2, 1.0
                    label = 'NDVI'
            else:
                vmin, vmax = -0.2, 1.0
                label = 'NDVI'
        elif 'elevation' in band_col.lower():
            cmap = 'terrain'
            vmin, vmax = np.nanpercentile(data, 1), np.nanpercentile(data, 99)
            label = 'Elevation (m)'
        elif 'viirs' in band_col.lower() or 'night' in band_col.lower():
            cmap = 'hot'
            vmin, vmax = 0, np.nanpercentile(data, 95)
            label = 'Night Lights'
        elif 'landcover' in band_col.lower():
            cmap = 'tab20'
            vmin, vmax = None, None
            label = 'Land Cover'
        elif 'gsn' in band_col.lower():
            cmap = 'viridis'
            vmin, vmax = np.nanpercentile(data, 1), np.nanpercentile(data, 99)
            label = 'GSN'
        elif 'wdpa' in band_col.lower():
            cmap = 'YlOrRd'
            vmin, vmax = 0, 1
            label = 'WDPA (Protected Areas)'
        else:
            cmap = 'viridis'
            vmin, vmax = np.nanpercentile(data, 1), np.nanpercentile(data, 99)
            label = band_col
        
        # Create visualization
        im = ax.imshow(data, cmap=cmap, aspect='auto', vmin=vmin, vmax=vmax, interpolation='nearest')
        ax.set_title(f'{band_name}\n({band_col})', fontweight='bold', fontsize=10)
        ax.set_xlabel('Column')
        ax.set_ylabel('Row')
        
        # Add colorbar
        cbar = plt.colorbar(im, ax=ax, shrink=0.8)
        cbar.set_label(label, rotation=270, labelpad=15)
        
        # Add statistics text
        valid_data = data[~np.isnan(data)]
        if len(valid_data) > 0:
            mean_val = np.nanmean(data)
            median_val = np.nanmedian(data)
            stats_text = f'Mean: {mean_val:.3f}\nMedian: {median_val:.3f}\nValid: {len(valid_data):,}'
            ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,
                   fontsize=8, verticalalignment='top',
                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))
    
    # Hide unused subplots
    for idx in range(n_bands, len(axes)):
        axes[idx].axis('off')
    
    plt.tight_layout()
    
    # Save figure
    output_file = output_dir / f"tile_maps_year{stats['year']}_r{stats['row_min']}_c{stats['col_min']}.png"
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"Saved: {output_file}")
    plt.close()


def main():
    """Main function - hardcoded parameters for random sample check."""
    print("=" * 80)
    print("2D MAP GENERATION FROM PARQUET FILE (Random Sample Check)")
    print("=" * 80)
    print(f"Year: {YEAR if YEAR is not None else 'auto-detect'}")
    print(f"Tile: rows [{ROW_START if ROW_START is not None else 'auto-detect'}:{TILE_SIZE}], "
          f"cols [{COL_START if COL_START is not None else 'auto-detect'}:{TILE_SIZE}]")
    print(f"Requested bands: {', '.join(REQUESTED_BANDS)}")
    print()
    
    # Find parquet file
    try:
        parquet_file = find_parquet_file()
        print(f"Found parquet file: {parquet_file}")
        file_size_gb = parquet_file.stat().st_size / (1024**3)
        print(f"File size: {file_size_gb:.2f} GB")
    except FileNotFoundError as e:
        print(f"ERROR: {e}")
        return
    
    # Open parquet file
    pfile = pq.ParquetFile(parquet_file)
    print(f"Row groups: {pfile.metadata.num_row_groups:,}")
    print(f"Total rows: {pfile.metadata.num_rows:,}")
    
    # Discover available years
    available_years = discover_available_years(pfile)
    if not available_years:
        print("ERROR: No years found in parquet file!")
        return
    
    # Determine year to use
    if YEAR is None:
        # Use the most recent year
        selected_year = available_years[-1]
        print(f"\nAuto-selected year: {selected_year} (most recent available)")
    else:
        if YEAR not in available_years:
            print(f"ERROR: Year {YEAR} not found in parquet file!")
            print(f"Available years: {available_years}")
            print(f"Using most recent year instead: {available_years[-1]}")
            selected_year = available_years[-1]
        else:
            selected_year = YEAR
    
    # Find valid tile location if not specified
    if ROW_START is None or COL_START is None:
        try:
            row_start, col_start = find_valid_tile_location(pfile, selected_year, TILE_SIZE)
        except ValueError as e:
            print(f"ERROR: {e}")
            return
    else:
        row_start = ROW_START
        col_start = COL_START
    
    # Get available bands
    available_bands = get_available_bands(pfile)
    print(f"\nAvailable bands ({len(available_bands)}): {', '.join(available_bands[:10])}...")
    
    # Find matching band columns and create mapping
    band_columns = []
    band_mapping = {}  # Maps requested band name to actual column name
    for requested_band in REQUESTED_BANDS:
        found = find_band_column(requested_band, available_bands)
        if found:
            band_columns.append(found)
            band_mapping[requested_band] = found
            print(f"  '{requested_band}' -> '{found}'")
        else:
            print(f"  Warning: '{requested_band}' not found in available bands")
    
    if not band_columns:
        print("ERROR: No matching bands found!")
        return
    
    # Read tile data
    try:
        grid_shape, band_data, stats = read_tile_from_parquet(
            pfile, row_start, col_start, TILE_SIZE, selected_year, band_columns
        )
    except Exception as e:
        print(f"ERROR reading tile: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # Create visualizations
    try:
        create_band_visualization(band_data, stats, band_mapping, OUTPUT_DIR)
    except Exception as e:
        print(f"ERROR creating visualizations: {e}")
        import traceback
        traceback.print_exc()
        return
    
    print("\n" + "=" * 80)
    print("VISUALIZATION COMPLETE")
    print("=" * 80)
    print(f"Output directory: {OUTPUT_DIR}")


if __name__ == "__main__":
    main()

