{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33a0578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import pyramid_gaussian, resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc5569a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35320feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. SYNTHETIC DATA GENERATION ---\n",
    "# We will create a 100x100 grid to represent our world.\n",
    "# Each pixel is a \"grid cell\" as described in your paper.\n",
    "MAP_SIZE = (100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "298379bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_data(size):\n",
    "    \"\"\"\n",
    "    Generates synthetic geospatial maps for our features and target.\n",
    "    This simulates the data you would source from GEE, WDPA, etc.\n",
    "    \"\"\"\n",
    "    print(f\"Creating synthetic {size} maps...\")\n",
    "    \n",
    "    # --- Target Variable (y) ---\n",
    "    # This is our \"World Database on Protected Areas (WDPA)\" layer\n",
    "    # 0 = Not Protected, 1 = Protected\n",
    "    existing_pas = np.zeros(size)\n",
    "    # Create a 30x30 \"National Park\" in a high-value area\n",
    "    existing_pas[10:40, 10:40] = 1\n",
    "    # Create a smaller 15x15 \"Reserve\"\n",
    "    existing_pas[70:85, 10:25] = 1\n",
    "\n",
    "    # --- Feature Variables (X) ---\n",
    "    \n",
    "    # Feature 1: Environmental Value (like NDVI)\n",
    "    # Hypothesis: PAs are MORE likely in high-value areas.\n",
    "    env_value_map = np.zeros(size)\n",
    "    # The \"National Park\" area is a high-value hotspot\n",
    "    env_value_map[5:45, 5:45] = 0.9\n",
    "    # The \"Reserve\" area is also high-value\n",
    "    env_value_map[65:90, 5:30] = 0.8\n",
    "    # Add another \"unprotected\" hotspot (where transition risk should be high)\n",
    "    env_value_map[20:40, 70:90] = 0.85\n",
    "    \n",
    "    # Feature 2: Economic Activity (like GDP / Night Lights)\n",
    "    # Hypothesis: PAs are LESS likely in high-GDP areas (high opportunity cost).\n",
    "    gdp_map = np.zeros(size)\n",
    "    # Create a 25x25 \"high-GDP city\" where protection is unlikely\n",
    "    gdp_map[50:75, 60:85] = 1.0 \n",
    "    # Add some noise\n",
    "    gdp_map += np.random.rand(*size) * 0.1\n",
    "    \n",
    "    # Feature 3: Population Density\n",
    "    # Hypothesis: PAs are LESS likely in high-population areas.\n",
    "    pop_map = np.zeros(size)\n",
    "    # Population is high near the city\n",
    "    pop_map[45:80, 55:90] = 0.8\n",
    "    # Add some \"villages\"\n",
    "    pop_map[15:25, 50:60] = 0.3\n",
    "    pop_map += np.random.rand(*size) * 0.05\n",
    "    \n",
    "    # Return a dictionary of feature maps and the single target map\n",
    "    feature_maps = {\n",
    "        'env_value': env_value_map,\n",
    "        'gdp': gdp_map,\n",
    "        'population': pop_map\n",
    "    }\n",
    "    \n",
    "    return feature_maps, existing_pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99326642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. PYRAMID FEATURE ENGINEERING ---\n",
    "\n",
    "def build_pyramid_features(feature_maps, target_map):\n",
    "    \"\"\"\n",
    "    Takes the raw maps and engineers multi-scale features using a\n",
    "    Gaussian Pyramid. This is the core of the \"pyramid representation\".\n",
    "    \"\"\"\n",
    "    print(\"Engineering pyramid features...\")\n",
    "    \n",
    "    # This will hold all our features, one row per pixel\n",
    "    features_list = []\n",
    "    \n",
    "    # Flatten the target map to be our 'y' variable\n",
    "    # .ravel() turns a 2D map into a 1D list\n",
    "    target_series = pd.Series(target_map.ravel(), name=\"is_protected\")\n",
    "    \n",
    "    for name, map_data in feature_maps.items():\n",
    "        # ---\n",
    "        # Add the original map (Scale 0)\n",
    "        # This is the pixel's own value\n",
    "        # ---\n",
    "        features_list.append(\n",
    "            pd.Series(map_data.ravel(), name=f\"{name}_scale_0\")\n",
    "        )\n",
    "        \n",
    "        # ---\n",
    "        # Build a Gaussian Pyramid\n",
    "        # This creates blurred, downsampled versions of the map.\n",
    "        # We use `downscale=4` for more distinct scales.\n",
    "        # max_layer=3 gives us 3 pyramid levels (plus scale 0).\n",
    "        # ---\n",
    "        pyramid = list(pyramid_gaussian(map_data, downscale=4, max_layer=3))\n",
    "        \n",
    "        # pyramid[0] is the original 100x100\n",
    "        # pyramid[1] is 25x25 (blurred average of 4x4 blocks)\n",
    "        # pyramid[2] is 7x7 (blurred average of 16x16 blocks)\n",
    "        # pyramid[3] is 2x2 (blurred average of 64x64 blocks)\n",
    "\n",
    "        # Loop through the smaller pyramid layers (starting from layer 1)\n",
    "        for i, layer in enumerate(pyramid[1:], 1):\n",
    "            \n",
    "            # ---\n",
    "            # Now we resize the small, blurred layer back up to the\n",
    "            # original 100x100 size.\n",
    "            # ---\n",
    "            # This means every pixel now has new features:\n",
    "            # - Its original value (scale 0)\n",
    "            # - The average value of its 4x4 neighborhood (scale 1)\n",
    "            # - The average value of its 16x16 neighborhood (scale 2)\n",
    "            # - etc.\n",
    "            scaled_layer = resize(\n",
    "                layer, \n",
    "                MAP_SIZE, \n",
    "                anti_aliasing=True, \n",
    "                preserve_range=True\n",
    "            )\n",
    "            \n",
    "            # Add this new multi-scale feature to our list\n",
    "            features_list.append(\n",
    "                pd.Series(scaled_layer.ravel(), name=f\"{name}_scale_{i}\")\n",
    "            )\n",
    "            \n",
    "    # Combine all feature lists into a single DataFrame\n",
    "    X = pd.concat(features_list, axis=1)\n",
    "    y = target_series\n",
    "    \n",
    "    return X, y\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
