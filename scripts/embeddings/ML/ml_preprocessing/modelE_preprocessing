#!/usr/bin/env python3

"""
Temporal split script for modelE training/validation/test splits (embeddings dataset)
with lookahead window target.

Rules
-----
Dynamic splits based on WINDOW parameter and right-censoring:
  - Filter to year <= max_year - WINDOW (right-censoring)
  - Splits computed from censored dataset (e.g., WINDOW=2: train 2018-2020, val 2021, test 2022)

Lookahead Window Target:
  - Compute first_transition_year per (row, col) as MIN(year) WHERE transition_01=1
  - Add transition_01_win{WINDOW}: for each row at year t, set to 1 if first_transition_year exists 
    AND t < first_transition_year <= t+WINDOW, else 0
  - Filter to year <= max_year-WINDOW to handle right-censoring

Future Scoring (Optional):
  - Export years 2023-2024 (or max_year-WINDOW+1 to max_year) with no labels for inference only

Notes
-----
- Simple temporal split with no sampling
- All splits use the full dataset for their respective time periods
- Dataset contains data from 2018-2024 only
- Right-censoring ensures we can evaluate 5-year lookahead window
"""

from __future__ import annotations

import json
import math
import os
import sys
import time
from pathlib import Path

import duckdb
import wandb

# Configuration
RANDOM_STATE = 42
# Window configuration
WINDOW = int(os.environ.get("WINDOW", "2"))  # Lookahead window in years (2 or 3)
EXPORT_FUTURE_SCORING = os.environ.get("EXPORT_FUTURE_SCORING", "false").lower() in ("true", "1", "yes")
WANDB_PROJECT = "ml-training-preprocessing"

# Split configuration will be computed dynamically based on WINDOW and max_year
# For example, with WINDOW=2 and max_year=2024:
#   cutoff_year = 2024 - 2 = 2022
#   train: 2018-2020 (3 years)
#   val: 2021 (1 year)
#   test: 2022 (1 year)
#   future_scoring: 2023-2024 (if enabled)


def resolve_input() -> Path:
    """Locate embeddings_transition_panel_2018-2024.parquet (prefer $SCRATCH if present)."""
    repo_root = Path(__file__).resolve().parents[4]
    scratch_root = Path(os.environ["SCRATCH"]) if os.environ.get("SCRATCH") else None

    candidates = []
    if scratch_root is not None:
        candidates.append(scratch_root / "data/ml/embeddings_transition_panel_2018-2024.parquet")
    candidates.append(repo_root / "data/ml/embeddings_transition_panel_2018-2024.parquet")

    for cand in candidates:
        if cand.exists():
            return cand

    raise FileNotFoundError("embeddings_transition_panel_2018-2024.parquet not found in expected locations")


def configure_duckdb(con: duckdb.DuckDBPyConnection) -> None:
    """Apply sensible local/Euler defaults."""
    is_euler = bool(os.environ.get("SCRATCH"))
    slurm_cpus = int(os.environ.get("SLURM_CPUS_PER_TASK", "0"))
    num_threads = slurm_cpus if slurm_cpus > 0 else (48 if is_euler else 4)

    # Match DuckDB memory to Slurm allocation when present
    slurm_mem_per_cpu_mb = os.environ.get("SLURM_MEM_PER_CPU")
    if slurm_mem_per_cpu_mb and slurm_cpus:
        total_mem_mb = int(slurm_mem_per_cpu_mb) * slurm_cpus
        memory_limit_gb = max(1, total_mem_mb // 1024)
    else:
        memory_limit_gb = 128 if is_euler else 16

    con.execute(f"SET threads={num_threads}")
    con.execute("SET preserve_insertion_order=false")
    con.execute(f"SET memory_limit='{memory_limit_gb}GB'")

    temp_dir = os.environ.get("SCRATCH") or (Path(__file__).parent / "temp")
    # Ensure temp directory exists (DuckDB won't create parent directories)
    if isinstance(temp_dir, str):
        temp_dir = Path(temp_dir)
    temp_dir.mkdir(parents=True, exist_ok=True)
    # Create duckdb_temp subdirectory
    duckdb_temp_dir = temp_dir / "duckdb_temp"
    duckdb_temp_dir.mkdir(parents=True, exist_ok=True)
    temp_dir_sql = str(temp_dir).replace("'", "''")
    con.execute(f"SET temp_directory='{temp_dir_sql}/duckdb_temp'")

    if is_euler:
        con.execute("PRAGMA max_temp_directory_size='200GB'")

    print(f"DuckDB configured: {num_threads} threads, {memory_limit_gb}GB memory")
    print(f"Running on: {'Euler cluster' if is_euler else 'local machine'}")


def log_year_counts(con: duckdb.DuckDBPyConnection, parquet_path: Path, label: str, target_col: str = "transition_01", use_wandb: bool = False) -> None:
    escaped = str(parquet_path).replace("'", "''")
    rows = con.execute(
        f"""
        SELECT
            year,
            SUM({target_col}) AS positives,
            SUM(CASE WHEN {target_col} = 0 THEN 1 ELSE 0 END) AS negatives,
            COUNT(*) AS total,
            CASE WHEN COUNT(*) > 0 THEN SUM({target_col})::DOUBLE / COUNT(*) ELSE 0 END AS pos_ratio,
            CASE WHEN SUM({target_col}) > 0 
                 THEN SUM(CASE WHEN {target_col} = 0 THEN 1 ELSE 0 END)::DOUBLE / SUM({target_col})
                 ELSE 0 END AS neg_pos_ratio
        FROM read_parquet('{escaped}')
        GROUP BY year
        ORDER BY year
        """
    ).fetchall()

    print(f"\n{label} per-year counts (target: {target_col}):")
    for year, pos, neg, total, pos_ratio, neg_pos_ratio in rows:
        print(f"  {year}: total={total:,} pos={pos:,} neg={neg:,} | pos_ratio={pos_ratio:.5f} neg/pos={neg_pos_ratio:.1f}")

    if use_wandb:
        wandb.log({f"{label.lower().replace(' ', '_')}/per_year_{target_col}": [
            dict(year=y, positives=p, negatives=n, total=t, pos_ratio=r, neg_pos_ratio=npr) 
            for y, p, n, t, r, npr in rows
        ]})


def compute_dynamic_splits(max_year: int, window: int) -> tuple[tuple[int, int], tuple[int, int], tuple[int, int], tuple[int, int]]:
    """Compute train/val/test/future splits based on max_year and window.
    
    Returns:
        (train_years, val_years, test_years, future_years)
        future_years may be None if no future scoring years available
    """
    cutoff_year = max_year - window
    
    # Check available years range (assume min_year is 2018 for embeddings)
    min_year = 2018
    available_years = list(range(min_year, cutoff_year + 1))
    
    if len(available_years) < 4:
        raise ValueError(f"Not enough years available for splits. Available: {len(available_years)} years (need at least 4)")
    
    # Simple split strategy:
    # - Use roughly 60% for train, 20% for val, 20% for test
    # - Prefer more recent data for test
    n_available = len(available_years)
    n_test = max(1, n_available // 5)  # ~20% for test
    n_val = max(1, n_available // 5)   # ~20% for val
    n_train = n_available - n_val - n_test  # Remaining for train
    
    test_start = cutoff_year - n_test + 1
    val_start = test_start - n_val
    train_start = min_year
    
    train_years = (train_start, val_start - 1)
    val_years = (val_start, test_start - 1)
    test_years = (test_start, cutoff_year)
    
    # Future scoring years (if enabled and available)
    future_years = None
    if cutoff_year < max_year:
        future_years = (cutoff_year + 1, max_year)
    
    return train_years, val_years, test_years, future_years


def main() -> None:
    start_time = time.time()

    print(f"\nConfiguration:")
    print(f"  WINDOW: {WINDOW} years")
    print(f"  EXPORT_FUTURE_SCORING: {EXPORT_FUTURE_SCORING}")

    wandb_api_key = os.environ.get("WANDB_API_KEY")
    wandb_entity = os.environ.get("WANDB_ENTITY")
    if not wandb_api_key:
        print("Warning: WANDB_API_KEY not found in environment")
    if not wandb_entity:
        print("Warning: WANDB_ENTITY not found in environment (will use default)")
    
    use_wandb = False
    try:
        print("Initializing Weights & Biases...")
        wandb.init(
            project=WANDB_PROJECT,
            entity=wandb_entity,
            name=f"modelE_preprocessing_win{WINDOW}_{time.strftime('%Y%m%d_%H%M%S')}",
            config={
                "random_state": RANDOM_STATE,
                "window": WINDOW,
                "export_future_scoring": EXPORT_FUTURE_SCORING,
                "sampling_method": "temporal_split_full_win",
            },
        )
        use_wandb = True
        print("W&B connected\n")
    except Exception as err:
        print(f"W&B initialization failed: {err}\n")

    input_path = resolve_input()
    base_dir = input_path.parent
    
    # Output paths will be determined after computing splits
    target_col_name = f"transition_01_win{WINDOW}"
    
    print(f"\nInput: {input_path}")
    print(f"Target column: {target_col_name}")

    con = duckdb.connect()
    configure_duckdb(con)

    escaped_in = str(input_path).replace("'", "''")

    try:
        print("\n" + "="*70)
        print("STEP 1: Computing first_transition_year per (row, col)")
        print("="*70)
        
        # Step 1: Compute first_transition_year per (row, col)
        first_transition_sql = f"""
        CREATE OR REPLACE TEMP TABLE first_transitions AS
        SELECT
            row,
            col,
            MIN(year) AS first_transition_year
        FROM read_parquet('{escaped_in}')
        WHERE transition_01 = 1
        GROUP BY row, col
        """
        
        step1_start = time.time()
        con.execute(first_transition_sql)
        n_transitions = con.execute("SELECT COUNT(*) FROM first_transitions").fetchone()[0]
        print(f"  Found {n_transitions:,} (row, col) pairs with transitions")
        print(f"  Completed in {time.time() - step1_start:.1f}s")
        
        print("\n" + "="*70)
        print("STEP 2: Computing max_year and applying right-censoring")
        print("="*70)
        
        # Step 2: Get max_year from data
        max_year_result = con.execute(f"SELECT MAX(year) FROM read_parquet('{escaped_in}')").fetchone()
        max_year = max_year_result[0] if max_year_result else 2024
        cutoff_year = max_year - WINDOW
        
        print(f"  Max year in data: {max_year}")
        print(f"  Cutoff year (max_year - {WINDOW}): {cutoff_year}")
        print(f"  Filtering to year <= {cutoff_year}")
        
        # Compute dynamic splits
        train_years, val_years, test_years, future_years = compute_dynamic_splits(max_year, WINDOW)
        
        print(f"\n  Computed splits:")
        print(f"    TRAIN: {train_years[0]}-{train_years[1]}")
        print(f"    VAL: {val_years[0]}-{val_years[1]}")
        print(f"    TEST: {test_years[0]}-{test_years[1]}")
        if future_years:
            print(f"    FUTURE_SCORING: {future_years[0]}-{future_years[1]} (if enabled)")
        
        # Set output paths
        merged_win_out = base_dir / f"embeddings_transition_panel_2018-2024_win{WINDOW}.parquet"
        train_out = base_dir / f"train_embeddings_win{WINDOW}.parquet"
        val_out = base_dir / f"val_embeddings_win{WINDOW}.parquet"
        test_out = base_dir / f"test_embeddings_win{WINDOW}.parquet"
        future_out = base_dir / f"future_scoring_embeddings_win{WINDOW}.parquet" if future_years and EXPORT_FUTURE_SCORING else None
        
        escaped_merged_win = str(merged_win_out).replace("'", "''")
        escaped_train = str(train_out).replace("'", "''")
        escaped_val = str(val_out).replace("'", "''")
        escaped_test = str(test_out).replace("'", "''")
        escaped_future = str(future_out).replace("'", "''") if future_out else None
        
        print("\n" + "="*70)
        print("STEP 3: Creating merged panel with lookahead target")
        print("="*70)
        
        # Step 3: Create merged panel with transition_01_win{WINDOW}
        # For each row at year t, set transition_01_win{WINDOW} = 1 if first_transition_year exists 
        # AND t < first_transition_year <= t+WINDOW, else 0
        merged_win_sql = f"""
        COPY (
            SELECT
                d.*,
                CASE 
                    WHEN ft.first_transition_year IS NOT NULL 
                         AND d.year < ft.first_transition_year 
                         AND ft.first_transition_year <= d.year + {WINDOW}
                    THEN 1
                    ELSE 0
                END AS {target_col_name}
            FROM read_parquet('{escaped_in}') d
            LEFT JOIN first_transitions ft
                ON d.row = ft.row AND d.col = ft.col
            WHERE d.year <= {cutoff_year}
        )
        TO '{escaped_merged_win}' (FORMAT PARQUET, COMPRESSION ZSTD)
        """
        
        step3_start = time.time()
        con.execute(merged_win_sql)
        print(f"  Merged panel with win{WINDOW} target written in {time.time() - step3_start:.1f}s")
        
        # Log counts for merged win
        log_year_counts(con, merged_win_out, "MERGED_WIN", target_col_name, use_wandb)
        
        print("\n" + "="*70)
        print(f"COMPARISON: transition_01 vs {target_col_name}")
        print("="*70)
        
        # Compare total positives
        comparison_stats = con.execute(
            f"""
            SELECT
                SUM(transition_01) AS total_pos_original,
                SUM({target_col_name}) AS total_pos_win,
                COUNT(DISTINCT CASE WHEN transition_01 = 1 THEN (row, col) END) AS distinct_rc_original,
                COUNT(DISTINCT CASE WHEN {target_col_name} = 1 THEN (row, col) END) AS distinct_rc_win
            FROM read_parquet('{escaped_merged_win}')
            """
        ).fetchone()
        
        total_pos_original, total_pos_win, distinct_rc_original, distinct_rc_win = comparison_stats
        
        print(f"\nTotal positives:")
        print(f"  transition_01:        {total_pos_original:,}")
        print(f"  {target_col_name}:   {total_pos_win:,}")
        if total_pos_original > 0:
            ratio = total_pos_win / total_pos_original
            print(f"  Ratio (win/original): {ratio:.3f}x")
            if total_pos_win <= total_pos_original:
                print(f"  WARNING: win positives ({total_pos_win:,}) <= original ({total_pos_original:,})")
            else:
                print(f"  ✓ win has more positives (expected)")
        
        print(f"\nDistinct (row,col) with positives:")
        print(f"  transition_01:        {distinct_rc_original:,}")
        print(f"  {target_col_name}:   {distinct_rc_win:,}")
        if distinct_rc_original > 0:
            ratio_rc = distinct_rc_win / distinct_rc_original
            print(f"  Ratio (win/original): {ratio_rc:.3f}x")
            if distinct_rc_win <= distinct_rc_original:
                print(f"  WARNING: win distinct (row,col) ({distinct_rc_win:,}) <= original ({distinct_rc_original:,})")
            else:
                print(f"  ✓ win has more distinct (row,col) with positives (expected)")
        
        if use_wandb:
            wandb.log({
                "comparison/total_pos_original": int(total_pos_original),
                "comparison/total_pos_win": int(total_pos_win),
                "comparison/total_pos_ratio": float(total_pos_win / total_pos_original) if total_pos_original > 0 else 0.0,
                "comparison/distinct_rc_original": int(distinct_rc_original),
                "comparison/distinct_rc_win": int(distinct_rc_win),
                "comparison/distinct_rc_ratio": float(distinct_rc_win / distinct_rc_original) if distinct_rc_original > 0 else 0.0,
            })
        
        print("\n" + "="*70)
        print("STEP 4: Creating temporal splits")
        print("="*70)
        
        # TRAIN split
        train_sql = f"""
        COPY (
            SELECT
                *
            FROM read_parquet('{escaped_merged_win}')
            WHERE year BETWEEN {train_years[0]} AND {train_years[1]}
        )
        TO '{escaped_train}' (FORMAT PARQUET, COMPRESSION ZSTD)
        """

        print(f"\nCreating TRAIN split ({train_years[0]}–{train_years[1]}, full risk set)...")
        train_start = time.time()
        con.execute(train_sql)
        print(f"TRAIN written in {time.time() - train_start:.1f}s")

        # VAL split
        val_sql = f"""
        COPY (
            SELECT
                *
            FROM read_parquet('{escaped_merged_win}')
            WHERE year BETWEEN {val_years[0]} AND {val_years[1]}
        )
        TO '{escaped_val}' (FORMAT PARQUET, COMPRESSION ZSTD)
        """

        print(f"\nCreating VAL split ({val_years[0]}–{val_years[1]}, full risk set)...")
        val_start = time.time()
        con.execute(val_sql)
        print(f"VAL written in {time.time() - val_start:.1f}s")

        # TEST split
        test_sql = f"""
        COPY (
            SELECT
                *
            FROM read_parquet('{escaped_merged_win}')
            WHERE year BETWEEN {test_years[0]} AND {test_years[1]}
        )
        TO '{escaped_test}' (FORMAT PARQUET, COMPRESSION ZSTD)
        """

        print(f"\nCreating TEST split ({test_years[0]}–{test_years[1]}, full risk set, right-censored)...")
        test_start = time.time()
        con.execute(test_sql)
        print(f"TEST written in {time.time() - test_start:.1f}s")
        
        # Optional: Future scoring export (no labels, for inference only)
        if future_years and EXPORT_FUTURE_SCORING:
            print("\n" + "="*70)
            print("STEP 5: Exporting future scoring data (no labels)")
            print("="*70)
            
            # Export original data for future years (without win target, just features)
            future_sql = f"""
            COPY (
                SELECT
                    *
                FROM read_parquet('{escaped_in}')
                WHERE year BETWEEN {future_years[0]} AND {future_years[1]}
            )
            TO '{escaped_future}' (FORMAT PARQUET, COMPRESSION ZSTD)
            """
            
            print(f"\nExporting FUTURE_SCORING ({future_years[0]}–{future_years[1]}, no labels)...")
            future_start = time.time()
            con.execute(future_sql)
            print(f"FUTURE_SCORING written in {time.time() - future_start:.1f}s")
            
            future_rows = con.execute(
                f"SELECT COUNT(*) FROM read_parquet('{escaped_future}')"
            ).fetchone()[0]
            print(f"  Total rows: {future_rows:,}")

        # Log counts for both original and win targets
        print("\n" + "="*70)
        print("PER-YEAR STATISTICS")
        print("="*70)
        log_year_counts(con, train_out, "TRAIN", "transition_01", use_wandb)
        log_year_counts(con, train_out, "TRAIN", target_col_name, use_wandb)
        log_year_counts(con, val_out, "VAL", "transition_01", use_wandb)
        log_year_counts(con, val_out, "VAL", target_col_name, use_wandb)
        log_year_counts(con, test_out, "TEST", "transition_01", use_wandb)
        log_year_counts(con, test_out, "TEST", target_col_name, use_wandb)

        # Check total rows
        total_train_rows = con.execute(
            f"SELECT COUNT(*) FROM read_parquet('{escaped_train}')"
        ).fetchone()[0]
        
        total_val_rows = con.execute(
            f"SELECT COUNT(*) FROM read_parquet('{escaped_val}')"
        ).fetchone()[0]
        
        total_test_rows = con.execute(
            f"SELECT COUNT(*) FROM read_parquet('{escaped_test}')"
        ).fetchone()[0]
        
        print(f"\nTotal rows:")
        print(f"  TRAIN: {total_train_rows:,}")
        print(f"  VAL: {total_val_rows:,}")
        print(f"  TEST: {total_test_rows:,}")
        
        # Compute scale_pos_weight from TRAIN only (using win target): sqrt(n_neg / n_pos)
        train_stats = con.execute(
            f"""
            SELECT
                SUM({target_col_name}) AS total_pos,
                SUM(CASE WHEN {target_col_name} = 0 THEN 1 ELSE 0 END) AS total_neg
            FROM read_parquet('{escaped_train}')
            """
        ).fetchone()
        total_pos, total_neg = train_stats
        
        # Handle None values (when there are no rows)
        if total_pos is None:
            total_pos = 0
        if total_neg is None:
            total_neg = 0
            
        neg_pos_ratio = total_neg / total_pos if total_pos > 0 else 1.0
        scale_pos_weight = math.sqrt(neg_pos_ratio) if neg_pos_ratio > 0 else 1.0
        
        print(f"\nTRAIN class balance ({target_col_name}):")
        print(f"  Positives: {total_pos:,}")
        print(f"  Negatives: {total_neg:,}")
        print(f"  Ratio (neg/pos): {neg_pos_ratio:.2f}")
        print(f"  scale_pos_weight for LightGBM: {scale_pos_weight:.6f} (sqrt of ratio)")
        
        # Save scale_pos_weight to metadata file for use in training
        metadata_path = base_dir / f"train_embeddings_win{WINDOW}_metadata.json"
        metadata = {
            "scale_pos_weight": scale_pos_weight,
            "total_positives": int(total_pos),
            "total_negatives": int(total_neg),
            "neg_pos_ratio": float(neg_pos_ratio),
            "window": WINDOW,
            "target_column": target_col_name,
            "train_years": list(train_years),
            "val_years": list(val_years),
            "test_years": list(test_years),
            "cutoff_year": cutoff_year,
            "max_year": max_year,
            "computed_from": f"train_embeddings_win{WINDOW}.parquet"
        }
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        print(f"  Metadata saved to: {metadata_path}")

        if use_wandb:
            log_dict = {
                "train/rows": total_train_rows,
                "train/positives_win": total_pos,
                "train/negatives_win": total_neg,
                "train/scale_pos_weight": scale_pos_weight,
                "train/neg_pos_ratio": neg_pos_ratio,
                "val/rows": total_val_rows,
                "test/rows": total_test_rows,
                "window": WINDOW,
                "cutoff_year": cutoff_year,
                "max_year": max_year,
                "train_years": f"{train_years[0]}-{train_years[1]}",
                "val_years": f"{val_years[0]}-{val_years[1]}",
                "test_years": f"{test_years[0]}-{test_years[1]}",
                "timing/total_seconds": time.time() - start_time,
            }
            if future_years and EXPORT_FUTURE_SCORING:
                log_dict["future_scoring/rows"] = future_rows
                log_dict["future_scoring/years"] = f"{future_years[0]}-{future_years[1]}"
            wandb.log(log_dict)

        print("\nDone.")

    except Exception as e:
        error_msg = f"{type(e).__name__}: {e}"
        print(f"\nERROR: {error_msg}")
        if use_wandb:
            wandb.log({"status": "failed", "error": error_msg})
        raise
    finally:
        if use_wandb:
            wandb.finish()


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"\nScript failed: {e}", file=sys.stderr)
        sys.exit(1)

