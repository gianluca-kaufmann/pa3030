#!/usr/bin/env python3
"""
Goal:
    Compute comprehensive statistics for the merged panel dataset using streaming.
    This script handles large datasets (70GB+) efficiently without loading all data into RAM.

Input:
    - Parquet file: data/ml/merged_panel_final.parquet (processed incrementally).

Process:
    - Streams through row groups to avoid memory issues
    - Computes: dimensions, column types, unique counts, missing values,
      numeric statistics, uniqueness checks, static variable detection
    - Uses incremental algorithms (Welford's method for means/variance)

Output:
    - JSON file: outputs/Tables/merged_panel_statistics.json
    - Contains all computed statistics for later formatting
"""

import warnings
import gc
import json
import os
import numpy as np
import pandas as pd
import pyarrow.parquet as pq
import wandb
from pathlib import Path
from datetime import datetime
from typing import Dict, Set, Optional
from multiprocessing import Pool, cpu_count
from functools import partial

warnings.filterwarnings("ignore")

# Set up paths
ROOT_DIR = Path(__file__).resolve().parents[2]
DATA_DIR = ROOT_DIR / "data" / "ml"
OUTPUT_DIR = ROOT_DIR / "outputs" / "Tables"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Panel file
PANEL_FILE = DATA_DIR / "merged_panel_final.parquet"
OUTPUT_JSON = OUTPUT_DIR / "merged_panel_statistics.json"

# Configuration
BATCH_SIZE = int(os.environ.get('BATCH_SIZE', 75))  # Increased for better performance
CHECK_STATIC_VARS = os.environ.get('CHECK_STATIC_VARS', 'True').lower() in ('true', '1', 'yes')
NUM_WORKERS = int(os.environ.get('SLURM_CPUS_PER_TASK', cpu_count()))  # Use all CPUs


def convert_to_native(obj):
    """Convert numpy types to Python types for JSON."""
    if isinstance(obj, (np.integer, np.int32, np.int64)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float32, np.float64)):
        return None if (np.isnan(obj) or np.isinf(obj)) else float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {k: convert_to_native(v) for k, v in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [convert_to_native(item) for item in obj]
    elif isinstance(obj, set):
        return sorted([convert_to_native(item) for item in obj])
    return obj


def process_batch(batch_info, file_path, columns, check_static_vars):
    """Worker function to process a batch of row groups."""
    batch_start, batch_end = batch_info
    
    try:
        parquet_file = pq.ParquetFile(file_path)
    except Exception:
        return None
    
    # Read and combine row groups
    chunks = []
    for i in range(batch_start, batch_end):
        try:
            chunks.append(parquet_file.read_row_group(i).to_pandas())
        except Exception:
            continue
    
    if not chunks:
        return None
    
    df = pd.concat(chunks, ignore_index=True)
    del chunks
    
    # Initialize results
    result = {
        'n_rows': len(df),
        'year_counts': df['year'].value_counts().to_dict() if 'year' in df.columns else {},
        'years': set(df['year'].unique()) if 'year' in df.columns else set(),
        'pixels': set(zip(df['row'], df['col'])) if 'row' in df.columns and 'col' in df.columns else set(),
        'missing': {col: int(df[col].isnull().sum()) for col in columns if col in df.columns},
        'numeric_stats': {},
        'static_vars': {}
    }
    
    # Numeric statistics (Welford's algorithm)
    for col in columns:
        if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):
            vals = df[col].dropna()
            if len(vals) > 0:
                result['numeric_stats'][col] = {
                    'count': len(vals),
                    'mean': float(vals.mean()),
                    'M2': float(((vals - vals.mean()) ** 2).sum()),
                    'min': float(vals.min()),
                    'max': float(vals.max())
                }
    
    # Static variable detection
    if check_static_vars and 'row' in df.columns and 'col' in df.columns:
        meta = ['year', 'x', 'y', 'row', 'col']
        for col in [c for c in columns if c not in meta and c in df.columns]:
            try:
                has_variation = (df.groupby(['row', 'col'])[col].nunique() > 1).any()
                result['static_vars'][col] = has_variation
            except Exception:
                result['static_vars'][col] = True
    
    del df
    gc.collect()
    return result


def analyze_dataset_streaming(use_wandb: bool = True):
    """Analyze dataset using parallel processing."""
    print("\n" + "=" * 80)
    print("PARALLEL ANALYSIS - MERGED PANEL DATASET")
    print("=" * 80)
    
    # Read file metadata
    print(f"Opening {PANEL_FILE.name}...", flush=True)
    parquet_file = pq.ParquetFile(PANEL_FILE)
    metadata = parquet_file.metadata
    schema = parquet_file.schema
    
    num_row_groups = metadata.num_row_groups
    columns = [schema[i].name for i in range(len(schema))]
    column_types = {schema[i].name: str(schema[i].physical_type) for i in range(len(schema))}
    
    print(f"Workers: {NUM_WORKERS} | Row groups: {num_row_groups:,} | Columns: {len(columns)}", flush=True)
    
    # Create batches
    batches = [(i, min(i + BATCH_SIZE, num_row_groups)) 
               for i in range(0, num_row_groups, BATCH_SIZE)]
    print(f"Processing {len(batches)} batches...", flush=True)
    
    # Process in parallel
    worker_fn = partial(process_batch, file_path=str(PANEL_FILE), 
                       columns=columns, check_static_vars=CHECK_STATIC_VARS)
    
    with Pool(NUM_WORKERS) as pool:
        results = [r for r in pool.imap_unordered(worker_fn, batches) if r is not None]
    
    print(f"Completed {len(results)}/{len(batches)} batches. Aggregating...", flush=True)
    
    # Aggregate results
    total_rows = sum(r['n_rows'] for r in results)
    year_counts = {}
    all_years = set()
    pixels = set()
    missing = {col: 0 for col in columns}
    numeric_stats = {}
    static_vars = {col: False for col in columns if col not in ['year', 'x', 'y', 'row', 'col']} if CHECK_STATIC_VARS else {}
    
    for r in results:
        # Years
        for year, count in r['year_counts'].items():
            year_counts[int(year)] = year_counts.get(int(year), 0) + count
        all_years.update(r['years'])
        
        # Pixels
        pixels.update(r['pixels'])
        
        # Missing values
        for col, count in r['missing'].items():
            missing[col] += count
        
        # Numeric stats (Chan's algorithm for parallel variance)
        for col, stats in r['numeric_stats'].items():
            if col not in numeric_stats:
                numeric_stats[col] = stats.copy()
            else:
                n1, n2 = numeric_stats[col]['count'], stats['count']
                n = n1 + n2
                delta = stats['mean'] - numeric_stats[col]['mean']
                numeric_stats[col]['mean'] = (n1 * numeric_stats[col]['mean'] + n2 * stats['mean']) / n
                numeric_stats[col]['M2'] += stats['M2'] + (delta ** 2) * n1 * n2 / n
                numeric_stats[col]['min'] = min(numeric_stats[col]['min'], stats['min'])
                numeric_stats[col]['max'] = max(numeric_stats[col]['max'], stats['max'])
                numeric_stats[col]['count'] = n
        
        # Static variables
        if CHECK_STATIC_VARS:
            for col, has_var in r['static_vars'].items():
                if has_var:
                    static_vars[col] = True
    
    print(f"Processed {total_rows:,} rows", flush=True)
    
    # Compute final statistics
    final_numeric_stats = {}
    for col, stats in numeric_stats.items():
        variance = stats['M2'] / stats['count'] if stats['count'] > 1 else 0.0
        final_numeric_stats[col] = {
            'min': stats['min'], 'max': stats['max'],
            'mean': stats['mean'], 'std': np.sqrt(variance),
            'count': stats['count']
        }
    
    # Uniqueness check
    n_pixels = len(pixels)
    n_years = len(all_years)
    expected = n_pixels * n_years
    is_unique = (expected == total_rows)
    
    print(f"Pixels: {n_pixels:,} | Years: {n_years} | Expected rows: {expected:,} | Unique: {is_unique}", flush=True)
    
    # Static variables
    static_list = [col for col, has_var in static_vars.items() if not has_var] if CHECK_STATIC_VARS else []
    if CHECK_STATIC_VARS:
        print(f"Static variables: {len(static_list)}", flush=True)
    
    # Return results
    return {
        'metadata': {
            'file': str(PANEL_FILE.name),
            'generated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'total_rows': total_rows,
            'total_columns': len(columns),
            'num_row_groups': num_row_groups
        },
        'columns': {
            'names': columns,
            'types': column_types,
            'numeric_columns': list(numeric_stats.keys()),
            'num_numeric': len(numeric_stats)
        },
        'years': {
            'unique_years': sorted(all_years),
            'num_unique_years': len(all_years),
            'year_counts': {str(k): int(v) for k, v in year_counts.items()}
        },
        'pixels': {
            'unique_pixels': n_pixels,
            'pixel_identifier': '(row, col)'
        },
        'uniqueness': {
            'unique_pixels': n_pixels,
            'unique_years': n_years,
            'expected_rows': expected,
            'actual_rows': total_rows,
            'is_unique': is_unique,
            'difference': abs(expected - total_rows)
        },
        'missing_values': missing,
        'numeric_statistics': final_numeric_stats,
        'static_variables': {
            'enabled': CHECK_STATIC_VARS,
            'count': len(static_list),
            'variables': static_list
        }
    }


def main():
    """Main function."""
    print("\n" + "=" * 80)
    print("PANEL STATISTICS COMPUTATION")
    print("=" * 80)
    print(f"Workers: {NUM_WORKERS} | Batch size: {BATCH_SIZE} | Static vars: {CHECK_STATIC_VARS}")
    print()
    
    if not PANEL_FILE.exists():
        print(f"ERROR: File not found: {PANEL_FILE}")
        return
    
    # Initialize wandb
    run = wandb.init(
        project="masters-thesis-data-analysis",
        name=f"panel_stats_{datetime.now().strftime('%Y%m%d_%H%M')}",
        config={
            "batch_size": BATCH_SIZE,
            "num_workers": NUM_WORKERS,
            "check_static_vars": CHECK_STATIC_VARS
        }
    )
    
    try:
        # Run analysis
        start = datetime.now()
        results = analyze_dataset_streaming(use_wandb=False)
        elapsed = (datetime.now() - start).total_seconds() / 60
        
        # Save results
        with open(OUTPUT_JSON, 'w') as f:
            json.dump(convert_to_native(results), f, indent=2)
        
        print(f"\n✓ Completed in {elapsed:.1f} minutes")
        print(f"✓ Saved to: {OUTPUT_JSON}")
        print(f"  Rows: {results['metadata']['total_rows']:,}")
        print(f"  Years: {results['years']['num_unique_years']}")
        print(f"  Pixels: {results['pixels']['unique_pixels']:,}")
        print(f"  Static vars: {results['static_variables']['count']}")
        
        wandb.finish()
        
    except Exception as e:
        print(f"\nERROR: {e}")
        wandb.finish(exit_code=1)


if __name__ == "__main__":
    main()
