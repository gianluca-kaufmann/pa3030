#!/usr/bin/env python3
"""
Fast panel dataset overview - checks structure and completeness.
Runs in ~1-5 minutes by reading all metadata columns for exact structure stats,
then sampling for feature statistics (missing values, numeric ranges).
"""

import json
import os
import numpy as np
import pyarrow.parquet as pq
from pathlib import Path
from datetime import datetime
from collections import Counter

# Paths
ROOT_DIR = Path(__file__).resolve().parents[2]
SCRATCH_ROOT = Path(os.environ["SCRATCH"]) if "SCRATCH" in os.environ else None

# Prefer panel file on $SCRATCH (Euler) if available, otherwise fall back to repo path
candidate_panel_paths = []
if SCRATCH_ROOT is not None:
    candidate_panel_paths.append(SCRATCH_ROOT / "data" / "ml" / "merged_panel_final.parquet")
candidate_panel_paths.append(ROOT_DIR / "data" / "ml" / "merged_panel_final.parquet")

PANEL_FILE = candidate_panel_paths[-1]
for cand in candidate_panel_paths:
    if cand.exists():
        PANEL_FILE = cand
        break

OUTPUT_JSON = ROOT_DIR / "outputs" / "Tables" / "merged_panel_statistics.json"
OUTPUT_JSON.parent.mkdir(parents=True, exist_ok=True)

# How many row groups to sample for statistics (None = all, but slow)
SAMPLE_ROW_GROUPS = 20


def analyze_panel():
    """Analyze panel dataset structure and completeness."""
    print("=" * 60)
    print("PANEL DATASET OVERVIEW")
    print("=" * 60)
    
    if not PANEL_FILE.exists():
        print(f"ERROR: File not found: {PANEL_FILE}")
        return None
    
    # File info
    file_size_gb = PANEL_FILE.stat().st_size / (1024**3)
    print(f"\nFile: {PANEL_FILE.name} ({file_size_gb:.1f} GB)")
    
    # Open file and read metadata (instant, no data loading)
    print("Reading metadata...", flush=True)
    pf = pq.ParquetFile(PANEL_FILE)
    meta = pf.metadata
    schema = pf.schema
    
    num_row_groups = meta.num_row_groups
    total_rows = meta.num_rows
    columns = [schema[i].name for i in range(len(schema))]
    column_types = {schema[i].name: str(schema[i].physical_type) for i in range(len(schema))}
    
    print(f"Total rows: {total_rows:,}")
    print(f"Columns: {len(columns)}")
    print(f"Row groups: {num_row_groups:,}")
    
    # Identify column categories
    meta_cols = ['year', 'x', 'y', 'row', 'col']
    feature_cols = [c for c in columns if c not in meta_cols]
    
    print(f"\nMetadata columns: {meta_cols}")
    print(f"Feature columns: {len(feature_cols)}")
    
    # ===================================================================
    # STEP 1: Get EXACT structure statistics (year counts, unique pixels)
    # Read only metadata columns for all rows - fast since they're small!
    # ===================================================================
    print(f"\nReading metadata columns for exact structure statistics...", flush=True)
    meta_table = pf.read(columns=['year', 'row', 'col'])
    meta_df = meta_table.to_pandas()
    
    # Check for duplicates on ['year', 'row', 'col']
    print(f"Checking for duplicate (year, row, col) combinations...", flush=True)
    duplicates = meta_df.duplicated(subset=['year', 'row', 'col'], keep=False)
    n_duplicates = duplicates.sum()
    if n_duplicates > 0:
        print(f"⚠️  WARNING: Found {n_duplicates:,} duplicate (year, row, col) combinations!")
        duplicate_rows = meta_df[duplicates].sort_values(['year', 'row', 'col'])
        print(f"   First few duplicates:")
        for _, row in duplicate_rows.head(5).iterrows():
            print(f"     Year {row['year']}, row {row['row']}, col {row['col']}")
    else:
        print(f"✓ No duplicate (year, row, col) combinations found")
    
    # Exact year counts
    year_counts = meta_df['year'].value_counts().to_dict()
    unique_years = sorted(year_counts.keys())
    n_years = len(unique_years)
    rows_per_year = {int(y): int(c) for y, c in year_counts.items()}
    
    # Exact unique pixels
    print(f"Computing unique pixels...", flush=True)
    unique_pixels = meta_df[['row', 'col']].drop_duplicates()
    n_unique_pixels = len(unique_pixels)
    
    # Verify panel composition: check if set of (row, col) pairs is identical across all years
    print(f"Verifying panel composition across years...", flush=True)
    pixels_by_year = {}
    for year in unique_years:
        year_pixels = set(meta_df[meta_df['year'] == year][['row', 'col']].apply(tuple, axis=1))
        pixels_by_year[year] = year_pixels
    
    # Check if all years have the same set of pixels
    reference_year = unique_years[0]
    reference_pixels = pixels_by_year[reference_year]
    panel_is_balanced = True
    pixels_dropped = {}
    pixels_added = {}
    
    for year in unique_years[1:]:
        year_pixels = pixels_by_year[year]
        dropped = reference_pixels - year_pixels
        added = year_pixels - reference_pixels
        if dropped or added:
            panel_is_balanced = False
            if dropped:
                pixels_dropped[year] = len(dropped)
            if added:
                pixels_added[year] = len(added)
    
    if panel_is_balanced:
        print(f"✓ Panel composition is balanced: all {n_unique_pixels:,} pixels present in every year")
    else:
        print(f"⚠️  WARNING: Panel composition is NOT balanced across years!")
        for year in unique_years[1:]:
            if year in pixels_dropped or year in pixels_added:
                dropped_str = f", {pixels_dropped.get(year, 0):,} pixels dropped" if year in pixels_dropped else ""
                added_str = f", {pixels_added.get(year, 0):,} pixels added" if year in pixels_added else ""
                print(f"   Year {year}: {len(pixels_by_year[year]):,} pixels{dropped_str}{added_str}")
    
    # Check structure
    expected_rows = n_unique_pixels * n_years
    
    print(f"✓ Found {n_years} years, {n_unique_pixels:,} unique pixels, {total_rows:,} total rows")
    
    # Free memory
    del meta_df, meta_table, unique_pixels
    
    # ===================================================================
    # STEP 2: Sample row groups for feature statistics (missing values, numeric stats)
    # These can be estimates - don't need exact counts
    # ===================================================================
    n_sample = min(SAMPLE_ROW_GROUPS or num_row_groups, num_row_groups)
    sample_indices = np.linspace(0, num_row_groups - 1, n_sample, dtype=int)
    
    print(f"\nSampling {n_sample} row groups for feature statistics...", flush=True)
    
    sample_rows = 0
    missing_counts = {col: 0 for col in columns}
    # Accumulators for numeric statistics across ALL sampled row groups
    numeric_accumulators = {}
    
    for idx, rg_idx in enumerate(sample_indices):
        if (idx + 1) % 5 == 0:
            print(f"  Progress: {idx + 1}/{n_sample} row groups", flush=True)
        
        # Read single row group
        table = pf.read_row_group(rg_idx, columns=columns)
        df = table.to_pandas()
        sample_rows += len(df)
        
        # Missing values
        for col in columns:
            if col in df.columns:
                missing_counts[col] += df[col].isnull().sum()
        
        # Numeric stats: update accumulators for every sampled row group
        for col in columns:
            if col in df.columns and np.issubdtype(df[col].dtype, np.number):
                vals = df[col].dropna()
                if len(vals) == 0:
                    continue
                
                if col not in numeric_accumulators:
                    numeric_accumulators[col] = {
                        'count': 0,
                        'sum': 0.0,
                        'sum_sq': 0.0,
                        'min': float('inf'),
                        'max': float('-inf')
                    }
                acc = numeric_accumulators[col]
                acc['count'] += len(vals)
                acc['sum'] += float(vals.sum())
                acc['sum_sq'] += float((vals ** 2).sum())
                acc['min'] = min(acc['min'], float(vals.min()))
                acc['max'] = max(acc['max'], float(vals.max()))
        
        del df, table
    
    # Finalize numeric statistics from accumulators
    numeric_stats = {}
    for col, acc in numeric_accumulators.items():
        count = acc['count']
        if count == 0:
            continue
        mean = acc['sum'] / count
        # Population variance from sum of squares
        var = max(acc['sum_sq'] / count - mean**2, 0.0)
        std = var**0.5
        numeric_stats[col] = {
            'min': acc['min'],
            'max': acc['max'],
            'mean': mean,
            'std': std,
            'sample_size': count
        }

    # Check structure
    print(f"\n" + "=" * 60)
    print("STRUCTURE CHECK (EXACT)")
    print("=" * 60)
    
    print(f"\nYears: {n_years} ({min(unique_years)} - {max(unique_years)})")
    print(f"Unique pixels: {n_unique_pixels:,}")
    print(f"Expected rows (pixels × years): {expected_rows:,}")
    print(f"Actual rows: {total_rows:,}")
    
    # Check balance
    year_values = list(rows_per_year.values())
    is_balanced = (max(year_values) - min(year_values)) / max(year_values) < 0.01 if year_values else True
    print(f"Balanced across years: {'Yes' if is_balanced else 'No'}")
    
    # Show duplicate status
    if n_duplicates > 0:
        print(f"⚠️  Duplicates found: {n_duplicates:,} duplicate (year, row, col) combinations")
    else:
        print(f"✓ No duplicate (year, row, col) combinations")
    
    # Show panel composition status
    if panel_is_balanced:
        print(f"✓ Panel composition: All pixels present in every year")
    else:
        print(f"⚠️  Panel composition: Pixels vary across years (see details above)")
    
    # Show rows per year
    print(f"\nRows per year:")
    for year in sorted(rows_per_year.keys()):
        count = rows_per_year[year]
        pct = 100 * count / total_rows if total_rows > 0 else 0
        print(f"  {year}: {count:,} ({pct:.1f}%)")
    
    # Missing values summary
    print(f"\n" + "=" * 60)
    print(f"MISSING VALUES (ESTIMATED from {n_sample} row groups)")
    print("=" * 60)
    
    cols_with_missing = [(col, count) for col, count in missing_counts.items() if count > 0]
    if cols_with_missing:
        cols_with_missing.sort(key=lambda x: -x[1])
        print(f"\nColumns with missing values: {len(cols_with_missing)}")
        print(f"Sampled rows: {sample_rows:,} of {total_rows:,}")
        for col, count in cols_with_missing[:10]:
            pct = 100 * count / sample_rows if sample_rows > 0 else 0
            print(f"  {col}: {count:,} ({pct:.1f}% of sample)")
        if len(cols_with_missing) > 10:
            print(f"  ... and {len(cols_with_missing) - 10} more")
    else:
        print(f"\nNo missing values found in sample of {sample_rows:,} rows!")
    
    # Numeric columns summary
    print(f"\n" + "=" * 60)
    print("NUMERIC COLUMNS (ESTIMATED from sample)")
    print("=" * 60)
    print(f"\nTotal numeric columns: {len(numeric_stats)}")
    
    # Show a few examples
    for col in list(numeric_stats.keys())[:5]:
        stats = numeric_stats[col]
        print(f"  {col}: min={stats['min']:.2f}, max={stats['max']:.2f}, mean={stats['mean']:.2f}")
    if len(numeric_stats) > 5:
        print(f"  ... and {len(numeric_stats) - 5} more")
    
    # Compile results
    results = {
        'metadata': {
            'file': str(PANEL_FILE.name),
            'file_size_gb': round(file_size_gb, 2),
            'generated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'total_rows': total_rows,
            'total_columns': len(columns),
            'num_row_groups': num_row_groups,
            'sampled_row_groups': n_sample
        },
        'columns': {
            'all': columns,
            'metadata': meta_cols,
            'features': feature_cols,
            'types': column_types,
            'numeric_count': len(numeric_stats)
        },
        'structure': {
            'years': unique_years,
            'num_years': n_years,
            'unique_pixels': n_unique_pixels,
            'expected_rows': expected_rows,
            'actual_rows': total_rows,
            'is_balanced': is_balanced,
            'rows_per_year': rows_per_year,
            'has_duplicates': n_duplicates > 0,
            'num_duplicates': int(n_duplicates),
            'panel_composition_balanced': panel_is_balanced,
            'pixels_dropped_by_year': {int(k): int(v) for k, v in pixels_dropped.items()} if pixels_dropped else {},
            'pixels_added_by_year': {int(k): int(v) for k, v in pixels_added.items()} if pixels_added else {}
        },
        'missing_values': {
            'sample_rows': sample_rows,
            'columns_with_missing': len(cols_with_missing),
            'details': {col: int(count) for col, count in cols_with_missing}
        },
        'numeric_statistics': numeric_stats
    }
    
    return results


def main():
    start = datetime.now()
    
    results = analyze_panel()
    
    if results is None:
        return
    
    # Save results
    with open(OUTPUT_JSON, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    elapsed = (datetime.now() - start).total_seconds()
    
    print(f"\n" + "=" * 60)
    print("COMPLETE")
    print("=" * 60)
    print(f"Time: {elapsed:.1f} seconds")
    print(f"Output: {OUTPUT_JSON}")


if __name__ == "__main__":
    main()
