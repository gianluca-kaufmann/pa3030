#!/usr/bin/env python3
"""
Goal:
    Compute comprehensive statistics for the merged panel dataset using streaming.
    This script handles large datasets (71GB+) efficiently without loading all data into RAM.
    
    memory optimized for euler cluster (128GB RAM, slower CPUs)
    Designed to avoid OOM (Out of Memory) kills with aggressive memory management.

Input:
    - Parquet file: data/ml/merged_panel_final.parquet (processed incrementally).

Process:
    - Streams through row groups in conservative batches (100 at a time)
    - Computes: dimensions, column types, unique counts, missing values,
      numeric statistics, uniqueness checks, static variable detection (optional)
    - Uses incremental algorithms (Welford's method for means/variance)
    - MEMORY OPTIMIZATIONS:
      * Conservative batch sizes to avoid memory spikes
      * Immediate deletion of intermediate variables
      * Aggressive garbage collection every 1000 row groups
      * Efficient pixel tracking (stores rows/cols separately, not tuples)
      * Two-pass approach for unique pixel counting (memory-efficient)
      * Static variable detection DISABLED by default (very memory intensive)

Output:
    - JSON file: outputs/Tables/merged_panel_statistics.json
    - Contains all computed statistics for later formatting
    - Uploaded to Weights & Biases for monitoring
"""

import warnings
import gc
import json
import numpy as np
import pandas as pd
import pyarrow.parquet as pq
import wandb
from pathlib import Path
from datetime import datetime
from typing import Dict, Set, Optional

warnings.filterwarnings("ignore")

# Set up paths
ROOT_DIR = Path(__file__).resolve().parents[2]
DATA_DIR = ROOT_DIR / "data" / "ml"
OUTPUT_DIR = ROOT_DIR / "outputs" / "Tables"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Panel file
PANEL_FILE = DATA_DIR / "merged_panel_final.parquet"
OUTPUT_JSON = OUTPUT_DIR / "merged_panel_statistics.json"

# Configuration
BATCH_SIZE = 100  # Process 100 row groups at a time (balanced for Euler's 128GB RAM)
CHECK_STATIC_VARS = False  # DISABLED: Very memory intensive, enable only if needed
WANDB_LOG_FREQUENCY = 50  # Log to wandb every N row groups (reduce overhead)


def log_progress(message: str):
    """Simple logging function with immediate flush."""
    print(message, flush=True)


def convert_to_native(obj):
    """Recursively convert numpy types to native Python types for JSON serialization."""
    if isinstance(obj, (np.integer, np.int32, np.int64)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float32, np.float64)):
        if np.isnan(obj) or np.isinf(obj):
            return None
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {k: convert_to_native(v) for k, v in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [convert_to_native(item) for item in obj]
    elif isinstance(obj, set):
        return sorted([convert_to_native(item) for item in obj])
    else:
        return obj


def analyze_dataset_streaming(use_wandb: bool = True):
    """
    Analyze the full dataset by processing row groups incrementally.
    Computes all required statistics without loading entire dataset into memory.
    
    Args:
        use_wandb: Whether to log progress to Weights & Biases
    
    Returns:
        Dictionary with all computed statistics
    """
    log_progress("\n" + "=" * 80)
    log_progress("STREAMING ANALYSIS OF MERGED PANEL DATASET")
    log_progress("=" * 80)
    
    log_progress("Opening parquet file and reading metadata...")
    parquet_file = pq.ParquetFile(PANEL_FILE)
    metadata = parquet_file.metadata
    schema = parquet_file.schema
    
    num_row_groups = metadata.num_row_groups
    total_rows = metadata.num_rows
    
    log_progress(f"Processing all {num_row_groups:,} row groups ({total_rows:,} total rows)...")
    log_progress("This will process data incrementally to avoid memory issues.")
    
    # Get column names and types from schema
    columns = [schema[i].name for i in range(len(schema))]
    column_types = {schema[i].name: str(schema[i].physical_type) for i in range(len(schema))}
    
    log_progress(f"Schema: {len(columns)} columns")
    
    # Log initial metadata to wandb
    if use_wandb:
        wandb.log({
            "dataset/num_columns": len(columns),
            "dataset/num_row_groups": num_row_groups,
            "dataset/total_rows": total_rows
        })
    
    # Initialize aggregators
    year_counts = {}
    all_years = set()
    unique_rows = set()  # Track rows/cols separately instead of storing all (row,col) tuples
    unique_cols = set()
    missing_counts = {col: 0 for col in columns}
    total_processed = 0
    
    # For numeric statistics (incremental computation using Welford's algorithm)
    numeric_cols = []
    numeric_stats = {}  # Will track: count, mean, M2, min, max
    
    # For static variable detection
    # We'll track which columns have variation (True) vs are static (False)
    pixel_col_uniqueness = {}  # {col: bool} - True if column varies, False if static
    feature_cols = [c for c in columns if c not in ['year', 'x', 'y', 'row', 'col']]
    
    # Initialize all feature columns as potentially static (False)
    if CHECK_STATIC_VARS:
        for col in feature_cols:
            pixel_col_uniqueness[col] = False
    
    # Identify metadata columns
    metadata_cols = ['year', 'x', 'y', 'row', 'col']
    
    log_progress("\nStarting streaming computation...")
    
    # Process row groups in batches
    for batch_start in range(0, num_row_groups, BATCH_SIZE):
        batch_end = min(batch_start + BATCH_SIZE, num_row_groups)
        log_progress(f"  Processing row groups {batch_start+1}-{batch_end} ({batch_end}/{num_row_groups})...")
        
        # Read batch of row groups
        chunks = []
        for i in range(batch_start, batch_end):
            try:
                table = parquet_file.read_row_group(i)
                chunks.append(table.to_pandas())
                del table  # Free Arrow table immediately
            except Exception as e:
                log_progress(f"    Warning: Could not read row group {i}: {e}")
                continue
        
        if not chunks:
            continue
        
        # Combine batch - use concat with copy=False to reduce memory
        df_batch = pd.concat(chunks, ignore_index=True, copy=False)
        del chunks  # Free list immediately
        gc.collect()  # Force collection after freeing chunks
        
        batch_size = len(df_batch)
        total_processed += batch_size
        
        # Identify numeric columns (first batch only)
        if not numeric_cols:
            for col in columns:
                if col in df_batch.columns and pd.api.types.is_numeric_dtype(df_batch[col]):
                    numeric_cols.append(col)
                    numeric_stats[col] = {
                        'count': 0,
                        'mean': 0.0,
                        'M2': 0.0,
                        'min': float('inf'),
                        'max': float('-inf')
                    }
            log_progress(f"  Identified {len(numeric_cols)} numeric columns")
        
        # Update year counts (optimized)
        if 'year' in df_batch.columns:
            batch_year_counts = df_batch['year'].value_counts().to_dict()
            for year, count in batch_year_counts.items():
                year_int = int(year)
                year_counts[year_int] = year_counts.get(year_int, 0) + count
                all_years.add(year_int)
            del batch_year_counts  # Free immediately
        
        # Update pixel tracking (memory optimized - don't store all tuples)
        if 'row' in df_batch.columns and 'col' in df_batch.columns:
            unique_rows.update(df_batch['row'].unique())
            unique_cols.update(df_batch['col'].unique())
        
        # Update missing value counts
        for col in columns:
            if col in df_batch.columns:
                missing = df_batch[col].isnull().sum()
                missing_counts[col] += int(missing)
        
        # Update numeric statistics using Welford's algorithm (incremental mean/variance)
        for col in numeric_cols:
            if col in df_batch.columns:
                valid_data = df_batch[col].dropna()
                if len(valid_data) == 0:
                    continue
                    
                stats = numeric_stats[col]
                
                # Vectorized update for better performance
                vals = valid_data.values
                for val in vals:
                    stats['count'] += 1
                    delta = val - stats['mean']
                    stats['mean'] += delta / stats['count']
                    delta2 = val - stats['mean']
                    stats['M2'] += delta * delta2
                
                # Update min/max using numpy for speed
                batch_min = float(vals.min())
                batch_max = float(vals.max())
                stats['min'] = min(stats['min'], batch_min)
                stats['max'] = max(stats['max'], batch_max)
                
                # Clean up immediately
                del valid_data, vals
        
        # Track static variables (check if columns vary within pixels across years)
        if CHECK_STATIC_VARS and 'row' in df_batch.columns and 'col' in df_batch.columns and 'year' in df_batch.columns:
            # Only check columns that haven't been marked as varying yet
            cols_to_check = [col for col in feature_cols if col in df_batch.columns and col in pixel_col_uniqueness and not pixel_col_uniqueness[col]]
            
            if cols_to_check:
                try:
                    # More efficient: check all columns at once
                    grouped = df_batch.groupby(['row', 'col'])[cols_to_check].nunique()
                    for col in cols_to_check:
                        if (grouped[col] > 1).any():
                            pixel_col_uniqueness[col] = True
                except Exception:
                    # If grouping fails, mark all as varying (safer)
                    for col in cols_to_check:
                        pixel_col_uniqueness[col] = True
        
        # Clean up
        del df_batch
        gc.collect()
        
        # Log batch progress to wandb (reduce frequency for performance)
        if use_wandb and (batch_end % WANDB_LOG_FREQUENCY == 0 or batch_end == num_row_groups):
            progress_pct = (batch_end / num_row_groups) * 100
            wandb.log({
                "progress/row_groups_processed": batch_end,
                "progress/rows_processed": total_processed,
                "progress/percent_complete": progress_pct,
                "progress/unique_rows": len(unique_rows),
                "progress/unique_cols": len(unique_cols),
                "progress/unique_years": len(all_years)
            })
        
        # Progress update
        if (batch_end % 200 == 0) or (batch_end == num_row_groups):
            log_progress(f"  Progress: {batch_end}/{num_row_groups} row groups ({total_processed:,} rows processed)")
        
        # Force garbage collection every 1000 row groups to prevent memory buildup
        if batch_end % 1000 == 0:
            gc.collect()
    
    log_progress(f"\n  Completed! Processed {total_processed:,} rows from {num_row_groups} row groups")
    
    # Compute final numeric statistics
    log_progress("\nComputing final numeric statistics...")
    final_numeric_stats = {}
    wandb_numeric_stats = {}  # For wandb logging
    
    for col, stats in numeric_stats.items():
        if stats['count'] > 0:
            variance = stats['M2'] / stats['count'] if stats['count'] > 1 else 0.0
            std = np.sqrt(variance)
            final_numeric_stats[col] = {
                'min': stats['min'] if stats['min'] != float('inf') else None,
                'max': stats['max'] if stats['max'] != float('-inf') else None,
                'mean': stats['mean'],
                'std': std,
                'count': stats['count']
            }
            
            # Prepare for wandb logging (only a subset to avoid too many metrics)
            if use_wandb and col not in metadata_cols:
                # Log statistics for feature columns
                wandb_numeric_stats[f"stats/{col}/mean"] = stats['mean']
                wandb_numeric_stats[f"stats/{col}/std"] = std
                wandb_numeric_stats[f"stats/{col}/min"] = stats['min'] if stats['min'] != float('inf') else None
                wandb_numeric_stats[f"stats/{col}/max"] = stats['max'] if stats['max'] != float('-inf') else None
    
    # Log numeric statistics to wandb
    if use_wandb and wandb_numeric_stats:
        wandb.log(wandb_numeric_stats)
    
    # Count exact unique pixels (memory-efficient second pass)
    log_progress("\nCounting unique pixels (final pass)...")
    unique_pixel_set = set()
    
    for i in range(0, num_row_groups, BATCH_SIZE):
        batch_end_count = min(i + BATCH_SIZE, num_row_groups)
        chunks = []
        for j in range(i, batch_end_count):
            try:
                table = parquet_file.read_row_group(j, columns=['row', 'col'])
                chunks.append(table.to_pandas())
            except Exception:
                continue
        
        if chunks:
            df_temp = pd.concat(chunks, ignore_index=True)
            unique_pixel_set.update(set(zip(df_temp['row'].values, df_temp['col'].values)))
            del df_temp, chunks
            gc.collect()
        
        if (batch_end_count % 200 == 0) or (batch_end_count == num_row_groups):
            log_progress(f"  Progress: {batch_end_count}/{num_row_groups} row groups...")
    
    n_unique_pixels = len(unique_pixel_set)
    del unique_pixel_set
    gc.collect()
    
    # Compute and check uniqueness
    n_unique_years = len(all_years)
    expected_rows = n_unique_pixels * n_unique_years
    is_unique = (expected_rows == total_processed)
    
    log_progress(f"\n  Unique pixels: {n_unique_pixels:,}")
    log_progress(f"  Unique years: {n_unique_years}")
    log_progress(f"  Expected rows: {expected_rows:,}")
    log_progress(f"  Actual rows: {total_processed:,}")
    
    is_unique = (expected_rows == total_processed)
    uniqueness_check = {
        'unique_pixels': n_unique_pixels,
        'unique_years': n_unique_years,
        'expected_rows': expected_rows,
        'actual_rows': total_processed,
        'is_unique': is_unique,
        'difference': abs(expected_rows - total_processed)
    }
    
    # Log uniqueness to wandb
    if use_wandb:
        wandb.log({
            "uniqueness/unique_pixels": n_unique_pixels,
            "uniqueness/unique_years": n_unique_years,
            "uniqueness/expected_rows": expected_rows,
            "uniqueness/actual_rows": total_processed,
            "uniqueness/is_unique": is_unique,
            "uniqueness/difference": abs(expected_rows - total_processed)
        })
    
    if is_unique:
        log_progress("  All (row, col, year) combinations appear to be unique")
    else:
        log_progress(f"  WARNING: Row count mismatch! Difference: {abs(expected_rows - total_processed):,}")
    
    # Static variable detection
    static_vars = []
    if CHECK_STATIC_VARS:
        log_progress("\nDetecting static variables...")
        # Columns that remain False (never found variation) are static
        static_vars = [col for col, has_variation in pixel_col_uniqueness.items() if not has_variation]
        log_progress(f"  Found {len(static_vars)} static variables (constant across years)")
        if len(static_vars) > 0:
            log_progress(f"  First 10 static variables: {static_vars[:10]}")
        
        # Log to wandb
        if use_wandb:
            wandb.log({
                "static_variables/count": len(static_vars),
                "static_variables/fraction": len(static_vars) / len(feature_cols) if len(feature_cols) > 0 else 0
            })
    else:
        log_progress("\nStatic variable detection SKIPPED (set CHECK_STATIC_VARS=True to enable)")
    
    # Compile results
    results = {
        'metadata': {
            'file': str(PANEL_FILE.name),
            'generated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'total_rows': total_processed,
            'total_columns': len(columns),
            'num_row_groups': num_row_groups
        },
        'columns': {
            'names': columns,
            'types': column_types,
            'numeric_columns': numeric_cols,
            'num_numeric': len(numeric_cols)
        },
        'years': {
            'unique_years': sorted(all_years),
            'num_unique_years': len(all_years),
            'year_counts': {str(k): int(v) for k, v in year_counts.items()}
        },
        'pixels': {
            'unique_pixels': n_unique_pixels,
            'pixel_identifier': '(row, col)'
        },
        'uniqueness': uniqueness_check,
        'missing_values': {col: int(missing_counts[col]) for col in columns},
        'numeric_statistics': final_numeric_stats,
        'static_variables': {
            'enabled': CHECK_STATIC_VARS,
            'count': len(static_vars),
            'variables': static_vars
        }
    }
    
    # Log final summary statistics to wandb
    if use_wandb:
        # Year counts
        year_count_dict = {f"year_counts/{year}": count for year, count in year_counts.items()}
        wandb.log(year_count_dict)
        
        # Missing values summary
        total_missing = sum(missing_counts.values())
        cols_with_missing = sum(1 for v in missing_counts.values() if v > 0)
        wandb.log({
            "missing_values/total": total_missing,
            "missing_values/columns_affected": cols_with_missing,
            "missing_values/fraction": total_missing / (total_processed * len(columns)) if total_processed > 0 else 0
        })
        
        # Overall summary
        wandb.log({
            "summary/total_rows": total_processed,
            "summary/total_columns": len(columns),
            "summary/numeric_columns": len(numeric_cols),
            "summary/unique_years": len(all_years),
            "summary/unique_pixels": n_unique_pixels
        })
    
    return results


def main():
    """Main analysis function."""
    print("\n" + "=" * 80)
    print("MERGED PANEL DATASET - COMPREHENSIVE STATISTICS COMPUTATION")
    print("South America 1km Dataset (2000-2024)")
    print("=" * 80)
    print()
    
    if not PANEL_FILE.exists():
        print(f"ERROR: Panel file not found: {PANEL_FILE}")
        print("Please check that the file exists in data/ml/")
        return
    
    # Initialize Weights & Biases
    run = wandb.init(
        project="masters-thesis-data-analysis",
        name=f"panel_statistics_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        config={"dataset": str(PANEL_FILE.name), "batch_size": BATCH_SIZE, 
                "check_static_vars": CHECK_STATIC_VARS, 
                "file_size_gb": PANEL_FILE.stat().st_size / (1024**3)},
        tags=["statistics", "panel-analysis", "streaming"]
    )
    
    try:
        log_progress("Mode: FULL DATASET STREAMING ANALYSIS (Memory-Optimized for Euler)")
        log_progress("This will process all row groups incrementally with aggressive memory management.")
        log_progress("Estimated time: 20-40 minutes (with 128GB RAM)")
        log_progress(f"Batch size: {BATCH_SIZE} row groups (conservative to avoid OOM)")
        log_progress(f"Memory management: Aggressive GC, immediate cleanup, efficient data structures")
        log_progress(f"Static variable check: {'Enabled (MEMORY INTENSIVE!)' if CHECK_STATIC_VARS else 'Disabled (recommended)'}")
        log_progress(f"Input file: {PANEL_FILE}")
        log_progress(f"Output file: {OUTPUT_JSON}")
        log_progress(f"Weights & Biases: Enabled (run: {run.name})\n")
        
        # Run streaming analysis
        results = analyze_dataset_streaming(use_wandb=True)
        
        # Save to JSON
        log_progress("\n" + "=" * 80)
        log_progress("Saving results to JSON...")
        log_progress("=" * 80)
        
        with open(OUTPUT_JSON, 'w') as f:
            json.dump(convert_to_native(results), f, indent=2)
        
        log_progress(f"Results saved to: {OUTPUT_JSON}")
        
        # Print summary
        log_progress("\n" + "=" * 80)
        log_progress("COMPUTATION SUMMARY")
        log_progress("=" * 80)
        log_progress(f"Total rows: {results['metadata']['total_rows']:,}")
        log_progress(f"Total columns: {results['metadata']['total_columns']}")
        log_progress(f"Numeric columns: {results['columns']['num_numeric']}")
        log_progress(f"Unique years: {results['years']['num_unique_years']}")
        log_progress(f"Unique pixels: {results['pixels']['unique_pixels']:,}")
        log_progress(f"Columns with missing values: {sum(1 for v in results['missing_values'].values() if v > 0)}")
        log_progress(f"Static variables: {results['static_variables']['count']}")
        
        # Show rows per year
        log_progress("\nRows per year:")
        for year in sorted(results['years']['unique_years']):
            count = results['years']['year_counts'].get(str(year), 0)
            log_progress(f"  Year {year}: {count:,} rows")
        
        log_progress("\n" + "=" * 80)
        log_progress("COMPUTATION COMPLETE")
        log_progress("=" * 80)
        log_progress(f"\nNext step: Run 'inspect_merged_total' to generate human-readable report")
        
        # Upload to wandb as artifact
        artifact = wandb.Artifact(f"panel_statistics_{datetime.now().strftime('%Y%m%d_%H%M%S')}", 
                                  type="statistics")
        artifact.add_file(str(OUTPUT_JSON))
        run.log_artifact(artifact)
        log_progress(f"\nResults uploaded to Weights & Biases")
        
        wandb.finish()
        
    except FileNotFoundError as e:
        print(f"\nERROR: {e}")
        wandb.finish(exit_code=1)
        return
    except Exception as e:
        print(f"\nERROR: {e}")
        import traceback
        traceback.print_exc()
        wandb.finish(exit_code=1)
        return


if __name__ == "__main__":
    main()
