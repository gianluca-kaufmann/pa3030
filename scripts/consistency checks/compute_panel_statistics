#!/usr/bin/env python3
"""
Fast panel dataset overview - checks structure and completeness.
Runs in ~1-5 minutes by using metadata and sampling instead of full scans.
"""

import json
import numpy as np
import pyarrow.parquet as pq
from pathlib import Path
from datetime import datetime
from collections import Counter

# Paths
ROOT_DIR = Path(__file__).resolve().parents[2]
PANEL_FILE = ROOT_DIR / "data" / "ml" / "merged_panel_final.parquet"
OUTPUT_JSON = ROOT_DIR / "outputs" / "Tables" / "merged_panel_statistics.json"
OUTPUT_JSON.parent.mkdir(parents=True, exist_ok=True)

# How many row groups to sample for statistics (None = all, but slow)
SAMPLE_ROW_GROUPS = 20


def analyze_panel():
    """Analyze panel dataset structure and completeness."""
    print("=" * 60)
    print("PANEL DATASET OVERVIEW")
    print("=" * 60)
    
    if not PANEL_FILE.exists():
        print(f"ERROR: File not found: {PANEL_FILE}")
        return None
    
    # File info
    file_size_gb = PANEL_FILE.stat().st_size / (1024**3)
    print(f"\nFile: {PANEL_FILE.name} ({file_size_gb:.1f} GB)")
    
    # Open file and read metadata (instant, no data loading)
    print("Reading metadata...", flush=True)
    pf = pq.ParquetFile(PANEL_FILE)
    meta = pf.metadata
    schema = pf.schema
    
    num_row_groups = meta.num_row_groups
    total_rows = meta.num_rows
    columns = [schema[i].name for i in range(len(schema))]
    column_types = {schema[i].name: str(schema[i].physical_type) for i in range(len(schema))}
    
    print(f"Total rows: {total_rows:,}")
    print(f"Columns: {len(columns)}")
    print(f"Row groups: {num_row_groups:,}")
    
    # Identify column categories
    meta_cols = ['year', 'x', 'y', 'row', 'col']
    feature_cols = [c for c in columns if c not in meta_cols]
    
    print(f"\nMetadata columns: {meta_cols}")
    print(f"Feature columns: {len(feature_cols)}")
    
    # Sample row groups for statistics (much faster than full scan)
    n_sample = min(SAMPLE_ROW_GROUPS or num_row_groups, num_row_groups)
    sample_indices = np.linspace(0, num_row_groups - 1, n_sample, dtype=int)
    
    print(f"\nSampling {n_sample} row groups for statistics...", flush=True)
    
    # Collect stats from samples
    year_counts = Counter()
    sample_rows = 0
    missing_counts = {col: 0 for col in columns}
    numeric_stats = {}
    row_col_pairs = set()  # Only track from samples, not all data
    
    for idx, rg_idx in enumerate(sample_indices):
        if (idx + 1) % 5 == 0:
            print(f"  Progress: {idx + 1}/{n_sample} row groups", flush=True)
        
        # Read single row group
        table = pf.read_row_group(rg_idx, columns=columns)
        df = table.to_pandas()
        sample_rows += len(df)
        
        # Year distribution
        if 'year' in df.columns:
            year_counts.update(df['year'].value_counts().to_dict())
        
        # Track unique (row, col) in sample
        if 'row' in df.columns and 'col' in df.columns:
            pairs = list(zip(df['row'].values, df['col'].values))
            row_col_pairs.update(pairs[:10000])  # Limit to avoid memory issues
        
        # Missing values
        for col in columns:
            if col in df.columns:
                missing_counts[col] += df[col].isnull().sum()
        
        # Numeric stats (only for first batch to get types)
        if idx == 0:
            for col in columns:
                if col in df.columns and np.issubdtype(df[col].dtype, np.number):
                    vals = df[col].dropna()
                    if len(vals) > 0:
                        numeric_stats[col] = {
                            'min': float(vals.min()),
                            'max': float(vals.max()),
                            'mean': float(vals.mean()),
                            'std': float(vals.std()) if len(vals) > 1 else 0.0,
                            'sample_size': len(vals)
                        }
        
        del df, table
    
    # Estimate unique pixels from sample
    sample_unique_pixels = len(row_col_pairs)
    
    # Get unique years from counts
    unique_years = sorted(year_counts.keys())
    n_years = len(unique_years)
    
    # Estimate total unique pixels
    # If data is balanced: total_rows / n_years
    estimated_pixels = total_rows // n_years if n_years > 0 else 0
    
    # Check if rows per year are balanced
    rows_per_year = {int(y): int(c) for y, c in year_counts.items()}
    # Scale up from sample to full dataset
    scale_factor = total_rows / sample_rows if sample_rows > 0 else 1
    estimated_year_counts = {y: int(c * scale_factor) for y, c in rows_per_year.items()}
    
    # Check structure
    print(f"\n" + "=" * 60)
    print("STRUCTURE CHECK")
    print("=" * 60)
    
    print(f"\nYears: {n_years} ({min(unique_years)} - {max(unique_years)})")
    print(f"Estimated pixels: {estimated_pixels:,}")
    print(f"Expected rows (pixels Ã— years): {estimated_pixels * n_years:,}")
    print(f"Actual rows: {total_rows:,}")
    
    # Check balance
    year_values = list(estimated_year_counts.values())
    is_balanced = (max(year_values) - min(year_values)) / max(year_values) < 0.01 if year_values else True
    print(f"Balanced across years: {'Yes' if is_balanced else 'No'}")
    
    # Missing values summary
    print(f"\n" + "=" * 60)
    print("MISSING VALUES (from sample)")
    print("=" * 60)
    
    cols_with_missing = [(col, count) for col, count in missing_counts.items() if count > 0]
    if cols_with_missing:
        cols_with_missing.sort(key=lambda x: -x[1])
        print(f"\nColumns with missing values: {len(cols_with_missing)}")
        for col, count in cols_with_missing[:10]:
            pct = 100 * count / sample_rows if sample_rows > 0 else 0
            print(f"  {col}: {count:,} ({pct:.1f}%)")
        if len(cols_with_missing) > 10:
            print(f"  ... and {len(cols_with_missing) - 10} more")
    else:
        print("\nNo missing values found in sample!")
    
    # Numeric columns summary
    print(f"\n" + "=" * 60)
    print("NUMERIC COLUMNS")
    print("=" * 60)
    print(f"\nTotal numeric columns: {len(numeric_stats)}")
    
    # Show a few examples
    for col in list(numeric_stats.keys())[:5]:
        stats = numeric_stats[col]
        print(f"  {col}: min={stats['min']:.2f}, max={stats['max']:.2f}, mean={stats['mean']:.2f}")
    if len(numeric_stats) > 5:
        print(f"  ... and {len(numeric_stats) - 5} more")
    
    # Compile results
    results = {
        'metadata': {
            'file': str(PANEL_FILE.name),
            'file_size_gb': round(file_size_gb, 2),
            'generated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'total_rows': total_rows,
            'total_columns': len(columns),
            'num_row_groups': num_row_groups,
            'sampled_row_groups': n_sample
        },
        'columns': {
            'all': columns,
            'metadata': meta_cols,
            'features': feature_cols,
            'types': column_types,
            'numeric_count': len(numeric_stats)
        },
        'structure': {
            'years': unique_years,
            'num_years': n_years,
            'estimated_pixels': estimated_pixels,
            'expected_rows': estimated_pixels * n_years,
            'actual_rows': total_rows,
            'is_balanced': is_balanced,
            'rows_per_year_estimate': estimated_year_counts
        },
        'missing_values': {
            'sample_rows': sample_rows,
            'columns_with_missing': len(cols_with_missing),
            'details': {col: int(count) for col, count in cols_with_missing}
        },
        'numeric_statistics': numeric_stats
    }
    
    return results


def main():
    start = datetime.now()
    
    results = analyze_panel()
    
    if results is None:
        return
    
    # Save results
    with open(OUTPUT_JSON, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    elapsed = (datetime.now() - start).total_seconds()
    
    print(f"\n" + "=" * 60)
    print("COMPLETE")
    print("=" * 60)
    print(f"Time: {elapsed:.1f} seconds")
    print(f"Output: {OUTPUT_JSON}")


if __name__ == "__main__":
    main()
